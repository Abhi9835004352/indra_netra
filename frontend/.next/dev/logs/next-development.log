[00:00:04.569] Server  LOG      âœ“ Ready in 3s
[00:00:06.802] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:07.248] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:07.265] Server  LOG     ðŸ“¹ Frame received: 51322 bytes from camera cam_001
[00:00:07.271] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:07.306] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:07.320] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:07.325] Server  LOG     ðŸ“¹ Frame received: 13275 bytes from camera cam_002
[00:00:07.327] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:07.348] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:07.365] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:07.370] Server  LOG     ðŸ“¹ Frame received: 53162 bytes from camera cam_003
[00:00:07.375] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:07.402] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:08.775] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:09.001] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:09.175] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:04:39.963Z"}
[00:00:09.175] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:04:40.222Z"}
[00:00:09.314] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:10.163] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:04:40.537Z"}
[00:00:10.599] Server  LOG      â—‹ Compiling /admin/monitoring ...
[00:00:11.313] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:11.353] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:11.376] Server  LOG     ðŸ“¹ Frame received: 51322 bytes from camera cam_001
[00:00:11.378] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:11.466] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:11.478] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:11.486] Server  LOG     ðŸ“¹ Frame received: 51322 bytes from camera cam_001
[00:00:11.509] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:11.571] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:11.604] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:11.609] Server  LOG     ðŸ“¹ Frame received: 53162 bytes from camera cam_003
[00:00:11.611] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:11.916] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:11.959] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:11.991] Server  LOG     ðŸ“¹ Frame received: 13275 bytes from camera cam_002
[00:00:12.002] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:12.095] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:12.132] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:12.144] Server  LOG     ðŸ“¹ Frame received: 13275 bytes from camera cam_002
[00:00:12.172] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:13.966] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:14.042] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:14.214] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:14.294] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:14.333] Server  LOG     ðŸ“¹ Frame received: 51781 bytes from camera cam_001
[00:00:14.342] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:14.421] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:04:45.182Z"}
[00:00:14.426] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:04:45.220Z"}
[00:00:14.458] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:14.522] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:14.626] Server  LOG     ðŸ“¹ Frame received: 14936 bytes from camera cam_002
[00:00:14.669] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:14.874] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:15.021] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:15.156] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:15.216] Server  LOG     ðŸ“¹ Frame received: 52030 bytes from camera cam_003
[00:00:15.245] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:15.338] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:04:46.088Z"}
[00:00:15.912] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:16.021] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:16.160] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:16.223] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:16.238] Server  LOG     ðŸ“¹ Frame received: 50531 bytes from camera cam_001
[00:00:16.256] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:16.346] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:04:47.134Z"}
[00:00:16.346] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:04:47.228Z"}
[00:00:16.445] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:16.671] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:16.847] Server  LOG     ðŸ“¹ Frame received: 48966 bytes from camera cam_002
[00:00:16.875] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:19.913] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:20.726] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:20.757] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:20.762] Server  LOG     ðŸ“¹ Frame received: 52372 bytes from camera cam_003
[00:00:20.770] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:20.791] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:20.884] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:20.922] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:20.936] Server  LOG     ðŸ“¹ Frame received: 48649 bytes from camera cam_001
[00:00:20.951] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:21.042] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:21.092] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:21.126] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:21.141] Server  LOG     ðŸ“¹ Frame received: 49667 bytes from camera cam_002
[00:00:21.147] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:21.192] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:04:51.108Z"}
[00:00:21.192] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:04:51.932Z"}
[00:00:21.192] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:04:52.263Z"}
[00:00:21.423] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:21.534] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:21.574] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:21.595] Server  LOG     ðŸ“¹ Frame received: 52998 bytes from camera cam_003
[00:00:21.642] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:21.957] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:04:52.637Z"}
[00:00:21.992] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:22.054] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:22.085] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:22.092] Server  LOG     ðŸ“¹ Frame received: 47580 bytes from camera cam_001
[00:00:22.098] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:22.170] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:04:53.213Z"}
[00:00:23.184] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:24.906] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:24.970] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:25.033] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:25.065] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:25.082] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:25.191] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:25.217] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:25.223] Server  LOG     ðŸ“¹ Frame received: 49775 bytes from camera cam_002
[00:00:25.226] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:25.320] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:25.365] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:25.376] Server  LOG     ðŸ“¹ Frame received: 53435 bytes from camera cam_003
[00:00:25.394] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:25.478] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:25.579] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:25.603] Server  LOG     ðŸ“¹ Frame received: 47351 bytes from camera cam_001
[00:00:25.609] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:25.734] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:25.793] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:25.806] Server  LOG     ðŸ“¹ Frame received: 47083 bytes from camera cam_002
[00:00:25.824] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:25.932] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:26.034] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:26.055] Server  LOG     ðŸ“¹ Frame received: 55508 bytes from camera cam_003
[00:00:26.075] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:27.034] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:27.119] Browser INFO    %cDownload the React DevTools for a better development experience: https://react.dev/link/react-devtools font-weight:bold
[00:00:27.409] Browser LOG     Starting inference for videos: [1,2,3]
[00:00:27.409] Browser LOG     Starting inference for video: 1 Gate 1 Entrance
[00:00:27.409] Browser LOG     Starting inference for video: 2 Main Hall
[00:00:27.409] Browser LOG     Starting inference for video: 3 Parking Area
[00:00:27.409] Browser LOG     %c[Vercel Web Analytics]%c Debug mode is enabled by default in development. No requests will be sent to the server. color: rgb(120, 120, 120) color: inherit
[00:00:27.411] Browser LOG     %c[Vercel Web Analytics]%c Running queued event color: rgb(120, 120, 120) color: inherit pageview {"path":"/admin/monitoring","route":"/admin/monitoring"}
[00:00:27.412] Browser LOG     %c[Vercel Web Analytics]%c [pageview] http://localhost:3000/admin/monitoring color: rgb(120, 120, 120) color: inherit {"dp":"/admin/monitoring","o":"http://localhost:3000/admin/monitoring","sdkn":"@vercel/analytics/next","sdkv":"1.5.0","sv":"0.1.3","ts":1763751898534}
[00:00:27.908] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:27.969] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:27.980] Server  LOG     ðŸ“¹ Frame received: 53252 bytes from camera cam_001
[00:00:27.988] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:28.042] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:28.090] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:28.099] Server  LOG     ðŸ“¹ Frame received: 56165 bytes from camera cam_003
[00:00:28.112] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:28.151] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:28.172] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:28.381] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:28.413] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:28.418] Server  LOG     ðŸ“¹ Frame received: 52117 bytes from camera cam_001
[00:00:28.423] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:28.485] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:28.512] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:28.515] Server  LOG     ðŸ“¹ Frame received: 52812 bytes from camera cam_003
[00:00:28.520] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:28.561] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:28.599] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:28.644] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:28.897] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:28.936] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:28.943] Server  LOG     ðŸ“¹ Frame received: 52293 bytes from camera cam_001
[00:00:28.947] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:29.065] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:29.097] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:29.102] Server  LOG     ðŸ“¹ Frame received: 9571 bytes from camera cam_002
[00:00:29.108] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:30.322] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:30.402] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:30.426] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:30.438] Server  LOG     ðŸ“¹ Frame received: 54550 bytes from camera cam_003
[00:00:30.446] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:30.487] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:30.511] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:01.520Z"}
[00:00:30.543] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:30.590] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:30.612] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:30.625] Server  LOG     ðŸ“¹ Frame received: 50091 bytes from camera cam_001
[00:00:30.627] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:30.647] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:30.698] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:01.631Z"}
[00:00:30.698] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:01.754Z"}
[00:00:30.730] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:30.749] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:30.751] Server  LOG     ðŸ“¹ Frame received: 18810 bytes from camera cam_002
[00:00:30.760] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:30.820] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:30.842] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:30.849] Server  LOG     ðŸ“¹ Frame received: 52603 bytes from camera cam_003
[00:00:30.861] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:30.904] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:01.774Z"}
[00:00:30.925] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:31.034] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:31.063] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:31.077] Server  LOG     ðŸ“¹ Frame received: 51105 bytes from camera cam_001
[00:00:31.083] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:31.112] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:31.145] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:02.040Z"}
[00:00:31.197] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:31.240] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:31.248] Server  LOG     ðŸ“¹ Frame received: 62011 bytes from camera cam_002
[00:00:31.252] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:31.286] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:02.249Z"}
[00:00:32.211] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:32.317] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:32.339] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:32.349] Server  LOG     ðŸ“¹ Frame received: 56228 bytes from camera cam_003
[00:00:32.413] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:32.454] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:32.504] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:03.434Z"}
[00:00:32.576] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:32.619] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:32.637] Server  LOG     ðŸ“¹ Frame received: 46722 bytes from camera cam_001
[00:00:32.646] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:32.673] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:03.507Z"}
[00:00:32.694] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:32.828] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:03.912Z"}
[00:00:32.881] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:32.944] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:32.993] Server  LOG     ðŸ“¹ Frame received: 64104 bytes from camera cam_002
[00:00:33.030] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:33.132] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:33.311] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:04.107Z"}
[00:00:33.324] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:33.447] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:33.502] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:33.516] Server  LOG     ðŸ“¹ Frame received: 51912 bytes from camera cam_003
[00:00:33.528] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:33.555] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:04.530Z"}
[00:00:33.585] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:33.748] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:33.759] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:33.876] Server  LOG     ðŸ“¹ Frame received: 47228 bytes from camera cam_001
[00:00:33.894] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:33.902] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:04.749Z"}
[00:00:33.966] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:34.023] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:34.050] Server  LOG     ðŸ“¹ Frame received: 63392 bytes from camera cam_002
[00:00:34.081] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:35.347] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:35.388] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:35.417] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:35.437] Server  LOG     ðŸ“¹ Frame received: 55189 bytes from camera cam_003
[00:00:35.443] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:35.467] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:35.528] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:35.568] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:35.577] Server  LOG     ðŸ“¹ Frame received: 40705 bytes from camera cam_001
[00:00:35.596] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:35.639] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:35.724] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:35.743] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:35.772] Server  LOG     ðŸ“¹ Frame received: 49283 bytes from camera cam_003
[00:00:35.778] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:35.818] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:35.842] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:35.907] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:35.925] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:35.931] Server  LOG     ðŸ“¹ Frame received: 63574 bytes from camera cam_002
[00:00:35.938] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:35.964] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:35.979] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:36.004] Server  LOG     ðŸ“¹ Frame received: 44360 bytes from camera cam_001
[00:00:36.011] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:36.027] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:36.095] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:36.134] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:36.145] Server  LOG     ðŸ“¹ Frame received: 63320 bytes from camera cam_002
[00:00:36.151] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:36.657] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:36.693] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:36.708] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:36.725] Server  LOG     ðŸ“¹ Frame received: 55005 bytes from camera cam_003
[00:00:36.736] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:36.797] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:36.826] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:36.838] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:36.841] Server  LOG     ðŸ“¹ Frame received: 42609 bytes from camera cam_001
[00:00:36.845] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:37.146] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:37.206] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:37.231] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:37.238] Server  LOG     ðŸ“¹ Frame received: 49921 bytes from camera cam_003
[00:00:37.242] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:37.262] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:37.327] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:37.345] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:37.353] Server  LOG     ðŸ“¹ Frame received: 63596 bytes from camera cam_002
[00:00:37.362] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:37.405] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:37.425] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:37.480] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:37.520] Server  LOG     ðŸ“¹ Frame received: 47682 bytes from camera cam_001
[00:00:37.700] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:37.945] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:06.567Z"}
[00:00:37.946] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:06.664Z"}
[00:00:37.946] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:06.735Z"}
[00:00:37.946] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:06.943Z"}
[00:00:37.946] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:06.950Z"}
[00:00:37.946] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:07.141Z"}
[00:00:37.946] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:07.870Z"}
[00:00:37.946] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:08.004Z"}
[00:00:37.946] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:08.330Z"}
[00:00:37.946] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:08.423Z"}
[00:00:37.946] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:08.627Z"}
[00:00:38.348] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:38.492] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:38.527] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:38.540] Server  LOG     ðŸ“¹ Frame received: 63212 bytes from camera cam_002
[00:00:38.549] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:38.568] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:09.519Z"}
[00:00:39.336] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:39.361] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:39.376] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:39.395] Server  LOG     ðŸ“¹ Frame received: 48870 bytes from camera cam_003
[00:00:39.397] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:39.410] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:39.431] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:39.464] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:39.473] Server  LOG     ðŸ“¹ Frame received: 47243 bytes from camera cam_001
[00:00:39.481] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:39.793] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:39.846] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:39.863] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:39.869] Server  LOG     ðŸ“¹ Frame received: 62279 bytes from camera cam_002
[00:00:39.876] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:39.916] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:39.951] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:39.972] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:39.980] Server  LOG     ðŸ“¹ Frame received: 47889 bytes from camera cam_003
[00:00:39.992] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:40.185] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:40.223] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:40.240] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:40.252] Server  LOG     ðŸ“¹ Frame received: 50841 bytes from camera cam_001
[00:00:40.262] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:40.523] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:40.549] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:40.560] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:40.562] Server  LOG     ðŸ“¹ Frame received: 50587 bytes from camera cam_003
[00:00:40.568] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:40.820] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:40.840] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:40.852] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:40.859] Server  LOG     ðŸ“¹ Frame received: 61989 bytes from camera cam_002
[00:00:40.862] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:40.914] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:40.944] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:40.958] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:40.963] Server  LOG     ðŸ“¹ Frame received: 48692 bytes from camera cam_001
[00:00:40.964] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:41.206] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:41.321] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:41.345] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:41.362] Server  LOG     ðŸ“¹ Frame received: 58882 bytes from camera cam_002
[00:00:41.377] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:41.412] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:41.435] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:41.454] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:41.460] Server  LOG     ðŸ“¹ Frame received: 46558 bytes from camera cam_003
[00:00:41.467] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:41.787] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:41.813] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:41.839] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:41.853] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:41.858] Server  LOG     ðŸ“¹ Frame received: 51322 bytes from camera cam_001
[00:00:41.860] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:41.897] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:41.907] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:41.931] Server  LOG     ðŸ“¹ Frame received: 61161 bytes from camera cam_002
[00:00:41.936] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:42.158] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:42.180] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:42.189] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:42.194] Server  LOG     ðŸ“¹ Frame received: 53162 bytes from camera cam_003
[00:00:42.196] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:42.228] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:42.249] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:42.262] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:42.269] Server  LOG     ðŸ“¹ Frame received: 52098 bytes from camera cam_001
[00:00:42.273] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:42.678] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:42.703] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:42.718] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:42.722] Server  LOG     ðŸ“¹ Frame received: 60833 bytes from camera cam_002
[00:00:42.723] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:42.785] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:42.810] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:42.820] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:42.827] Server  LOG     ðŸ“¹ Frame received: 53150 bytes from camera cam_003
[00:00:42.833] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:43.203] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:43.229] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:43.452] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:43.738] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:43.873] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:43.953] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:44.411] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:44.421] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:44.423] Server  LOG     ðŸ“¹ Frame received: 51666 bytes from camera cam_001
[00:00:44.424] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:44.449] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:44.460] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:44.465] Server  LOG     ðŸ“¹ Frame received: 60470 bytes from camera cam_002
[00:00:44.466] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:45.115] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:45.196] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:45.359] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:45.372] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:45.374] Server  LOG     ðŸ“¹ Frame received: 53183 bytes from camera cam_003
[00:00:45.375] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:45.437] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:11.137Z"}
[00:00:45.437] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:10.551Z"}
[00:00:45.437] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:10.623Z"}
[00:00:45.437] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:11.006Z"}
[00:00:45.437] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:11.390Z"}
[00:00:45.437] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:11.739Z"}
[00:00:45.437] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:12.043Z"}
[00:00:45.437] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:12.121Z"}
[00:00:45.437] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:12.385Z"}
[00:00:45.437] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:12.617Z"}
[00:00:45.437] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:12.999Z"}
[00:00:45.437] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:13.032Z"}
[00:00:45.437] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:13.379Z"}
[00:00:45.437] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:13.419Z"}
[00:00:45.437] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:13.898Z"}
[00:00:45.437] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:14.002Z"}
[00:00:45.437] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:14.401Z"}
[00:00:45.437] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:14.435Z"}
[00:00:45.437] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:14.673Z"}
[00:00:45.437] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:14.961Z"}
[00:00:45.437] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:15.096Z"}
[00:00:45.437] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:15.177Z"}
[00:00:45.681] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:45.689] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:45.693] Server  LOG     ðŸ“¹ Frame received: 51666 bytes from camera cam_001
[00:00:45.694] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:45.708] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:45.720] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:45.723] Server  LOG     ðŸ“¹ Frame received: 60470 bytes from camera cam_002
[00:00:45.724] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:46.318] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:46.643] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:46.650] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:46.764] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:16.339Z"}
[00:00:46.764] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:16.418Z"}
[00:00:47.030] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:47.040] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:47.041] Server  LOG     ðŸ“¹ Frame received: 53183 bytes from camera cam_003
[00:00:47.042] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:47.102] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:47.111] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:47.116] Server  LOG     ðŸ“¹ Frame received: 60470 bytes from camera cam_002
[00:00:47.117] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:47.176] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:17.540Z"}
[00:00:47.176] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:17.863Z"}
[00:00:47.176] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:17.869Z"}
[00:00:47.186] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:47.201] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:47.206] Server  LOG     ðŸ“¹ Frame received: 51666 bytes from camera cam_001
[00:00:47.209] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:47.991] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:48.042] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:48.110] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:48.423] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:48.432] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:48.435] Server  LOG     ðŸ“¹ Frame received: 53183 bytes from camera cam_003
[00:00:48.437] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:48.459] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:48.471] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:48.473] Server  LOG     ðŸ“¹ Frame received: 51666 bytes from camera cam_001
[00:00:48.474] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:48.491] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:48.505] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:48.518] Server  LOG     ðŸ“¹ Frame received: 60470 bytes from camera cam_002
[00:00:48.519] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:48.546] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:48.569] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:48.577] Server  LOG     ðŸ“¹ Frame received: 53183 bytes from camera cam_003
[00:00:48.583] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:49.568] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:49.584] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:49.636] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:49.708] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:50.049] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:19.214Z"}
[00:00:50.049] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:19.266Z"}
[00:00:50.049] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:19.334Z"}
[00:00:51.055] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:51.063] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:51.065] Server  LOG     ðŸ“¹ Frame received: 51666 bytes from camera cam_001
[00:00:51.065] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:51.080] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:51.088] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:51.090] Server  LOG     ðŸ“¹ Frame received: 60470 bytes from camera cam_002
[00:00:51.091] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:51.341] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:51.353] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:51.355] Server  LOG     ðŸ“¹ Frame received: 53183 bytes from camera cam_003
[00:00:51.359] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:51.928] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:51.992] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:52.221] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:20.790Z"}
[00:00:52.221] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:20.808Z"}
[00:00:52.221] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:20.857Z"}
[00:00:52.221] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:20.932Z"}
[00:00:52.304] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:53.196] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:23.151Z"}
[00:00:53.196] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:23.214Z"}
[00:00:53.196] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:23.527Z"}
[00:00:53.254] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:53.263] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:53.265] Server  LOG     ðŸ“¹ Frame received: 51666 bytes from camera cam_001
[00:00:53.265] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:53.305] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:53.314] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:53.317] Server  LOG     ðŸ“¹ Frame received: 60470 bytes from camera cam_002
[00:00:53.318] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:53.402] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:53.413] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:53.416] Server  LOG     ðŸ“¹ Frame received: 53183 bytes from camera cam_003
[00:00:53.427] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:54.210] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:54.302] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:54.393] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:54.402] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:54.405] Server  LOG     ðŸ“¹ Frame received: 51666 bytes from camera cam_001
[00:00:54.408] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:54.977] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:55.408] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:55.417] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:55.419] Server  LOG     ðŸ“¹ Frame received: 60470 bytes from camera cam_002
[00:00:55.420] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:55.566] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:55.575] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:55.579] Server  LOG     ðŸ“¹ Frame received: 53183 bytes from camera cam_003
[00:00:55.580] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:55.616] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:55.626] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:55.629] Server  LOG     ðŸ“¹ Frame received: 51666 bytes from camera cam_001
[00:00:55.636] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:56.102] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:56.452] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:57.201] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:57.264] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:57.494] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:57.505] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:57.507] Server  LOG     ðŸ“¹ Frame received: 51666 bytes from camera cam_001
[00:00:57.508] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:57.526] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:57.535] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:57.541] Server  LOG     ðŸ“¹ Frame received: 60470 bytes from camera cam_002
[00:00:57.542] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:57.577] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:57.587] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:57.589] Server  LOG     ðŸ“¹ Frame received: 53183 bytes from camera cam_003
[00:00:57.590] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:59.104] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:59.117] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:59.197] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:00:59.283] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:25.413Z"}
[00:00:59.283] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:25.526Z"}
[00:00:59.283] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:26.193Z"}
[00:00:59.283] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:27.324Z"}
[00:00:59.283] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:27.672Z"}
[00:00:59.283] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:28.488Z"}
[00:00:59.283] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:28.425Z"}
[00:00:59.420] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:59.429] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:59.431] Server  LOG     ðŸ“¹ Frame received: 51666 bytes from camera cam_001
[00:00:59.434] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:59.452] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:59.470] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:59.473] Server  LOG     ðŸ“¹ Frame received: 60470 bytes from camera cam_002
[00:00:59.476] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:00:59.569] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:00:59.577] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:00:59.580] Server  LOG     ðŸ“¹ Frame received: 53183 bytes from camera cam_003
[00:00:59.582] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:00.868] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:00.948] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:01.223] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:01.317] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:30.328Z"}
[00:01:01.318] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:30.339Z"}
[00:01:01.318] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:30.421Z"}
[00:01:01.393] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:01.401] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:01.402] Server  LOG     ðŸ“¹ Frame received: 51666 bytes from camera cam_001
[00:01:01.403] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:01.418] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:01.427] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:01.429] Server  LOG     ðŸ“¹ Frame received: 60470 bytes from camera cam_002
[00:01:01.429] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:01.441] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:01.452] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:01.455] Server  LOG     ðŸ“¹ Frame received: 53183 bytes from camera cam_003
[00:01:01.457] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:02.796] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:02.844] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:02.898] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:32.088Z"}
[00:01:02.898] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:32.171Z"}
[00:01:02.898] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:32.447Z"}
[00:01:02.930] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:03.002] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:03.012] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:03.015] Server  LOG     ðŸ“¹ Frame received: 51666 bytes from camera cam_001
[00:01:03.016] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:03.032] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:03.042] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:03.045] Server  LOG     ðŸ“¹ Frame received: 60470 bytes from camera cam_002
[00:01:03.046] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:03.063] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:03.076] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:03.082] Server  LOG     ðŸ“¹ Frame received: 53183 bytes from camera cam_003
[00:01:03.087] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:04.484] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:04.528] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:04.586] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:04.879] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:04.890] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:04.892] Server  LOG     ðŸ“¹ Frame received: 51666 bytes from camera cam_001
[00:01:04.893] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:04.981] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:04.990] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:04.992] Server  LOG     ðŸ“¹ Frame received: 60470 bytes from camera cam_002
[00:01:04.993] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:05.092] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:05.100] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:05.103] Server  LOG     ðŸ“¹ Frame received: 53183 bytes from camera cam_003
[00:01:05.104] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:06.238] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:06.322] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:06.426] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:06.519] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:34.020Z"}
[00:01:06.519] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:34.066Z"}
[00:01:06.519] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:34.154Z"}
[00:01:06.519] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:35.708Z"}
[00:01:06.520] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:35.751Z"}
[00:01:06.520] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:35.810Z"}
[00:01:06.585] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:06.594] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:06.595] Server  LOG     ðŸ“¹ Frame received: 51666 bytes from camera cam_001
[00:01:06.596] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:06.625] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:06.638] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:06.640] Server  LOG     ðŸ“¹ Frame received: 60470 bytes from camera cam_002
[00:01:06.642] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:06.703] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:06.714] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:06.719] Server  LOG     ðŸ“¹ Frame received: 53183 bytes from camera cam_003
[00:01:06.721] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:08.091] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:08.145] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:08.244] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:08.250] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:37.462Z"}
[00:01:08.250] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:37.546Z"}
[00:01:08.250] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:37.650Z"}
[00:01:08.353] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:08.366] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:08.369] Server  LOG     ðŸ“¹ Frame received: 51666 bytes from camera cam_001
[00:01:08.373] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:08.388] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:08.398] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:08.400] Server  LOG     ðŸ“¹ Frame received: 60470 bytes from camera cam_002
[00:01:08.401] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:08.416] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:08.429] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:08.435] Server  LOG     ðŸ“¹ Frame received: 53183 bytes from camera cam_003
[00:01:08.442] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:09.671] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:09.703] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:09.750] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:09.882] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:09.893] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:09.896] Server  LOG     ðŸ“¹ Frame received: 51666 bytes from camera cam_001
[00:01:09.897] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:09.920] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:09.929] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:09.931] Server  LOG     ðŸ“¹ Frame received: 60470 bytes from camera cam_002
[00:01:09.934] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:09.946] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:09.972] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:09.977] Server  LOG     ðŸ“¹ Frame received: 53183 bytes from camera cam_003
[00:01:09.984] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:11.451] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:11.524] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:11.577] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:11.755] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:11.764] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:11.770] Server  LOG     ðŸ“¹ Frame received: 51666 bytes from camera cam_001
[00:01:11.771] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:11.906] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:11.918] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:11.923] Server  LOG     ðŸ“¹ Frame received: 60470 bytes from camera cam_002
[00:01:11.924] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:11.958] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:11.973] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:11.976] Server  LOG     ðŸ“¹ Frame received: 53183 bytes from camera cam_003
[00:01:11.978] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:13.437] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:13.511] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:13.591] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:13.693] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:13.706] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:13.709] Server  LOG     ðŸ“¹ Frame received: 51666 bytes from camera cam_001
[00:01:13.711] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:13.729] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:13.739] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:13.741] Server  LOG     ðŸ“¹ Frame received: 60470 bytes from camera cam_002
[00:01:13.742] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:13.755] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:13.764] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:13.769] Server  LOG     ðŸ“¹ Frame received: 53183 bytes from camera cam_003
[00:01:13.774] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:15.398] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:15.409] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:15.469] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:15.518] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:39.314Z"}
[00:01:15.519] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:39.369Z"}
[00:01:15.519] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:39.466Z"}
[00:01:15.519] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:40.894Z"}
[00:01:15.519] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:40.927Z"}
[00:01:15.519] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:40.974Z"}
[00:01:15.519] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:42.747Z"}
[00:01:15.519] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:42.671Z"}
[00:01:15.519] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:42.801Z"}
[00:01:15.519] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:44.661Z"}
[00:01:15.519] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:44.735Z"}
[00:01:15.519] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:44.816Z"}
[00:01:15.621] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:15.631] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:15.633] Server  LOG     ðŸ“¹ Frame received: 51666 bytes from camera cam_001
[00:01:15.637] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:15.668] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:15.678] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:15.680] Server  LOG     ðŸ“¹ Frame received: 60470 bytes from camera cam_002
[00:01:15.681] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:15.753] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:15.762] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:15.765] Server  LOG     ðŸ“¹ Frame received: 53183 bytes from camera cam_003
[00:01:15.769] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:17.199] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:17.289] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:17.333] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:17.375] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:46.621Z"}
[00:01:17.375] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:46.631Z"}
[00:01:17.375] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:46.693Z"}
[00:01:17.489] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:17.501] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:17.503] Server  LOG     ðŸ“¹ Frame received: 51666 bytes from camera cam_001
[00:01:17.504] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:17.520] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:17.529] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:17.531] Server  LOG     ðŸ“¹ Frame received: 60470 bytes from camera cam_002
[00:01:17.532] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:17.546] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:17.555] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:17.558] Server  LOG     ðŸ“¹ Frame received: 53183 bytes from camera cam_003
[00:01:17.559] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:18.969] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:19.019] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:19.089] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:19.186] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:48.422Z"}
[00:01:19.186] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:48.512Z"}
[00:01:19.186] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:48.556Z"}
[00:01:19.299] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:19.308] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:19.310] Server  LOG     ðŸ“¹ Frame received: 51666 bytes from camera cam_001
[00:01:19.311] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:19.326] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:19.336] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:19.338] Server  LOG     ðŸ“¹ Frame received: 60470 bytes from camera cam_002
[00:01:19.339] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:19.400] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:19.416] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:20.288] Server  LOG     ðŸ“¹ Frame received: 53183 bytes from camera cam_003
[00:01:20.289] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:20.883] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:20.959] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:21.140] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:50.190Z"}
[00:01:21.140] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:50.241Z"}
[00:01:21.140] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:50.311Z"}
[00:01:21.213] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:21.221] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:21.223] Server  LOG     ðŸ“¹ Frame received: 51666 bytes from camera cam_001
[00:01:21.224] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:21.621] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:21.771] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:21.781] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:21.784] Server  LOG     ðŸ“¹ Frame received: 60470 bytes from camera cam_002
[00:01:21.785] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:21.802] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:21.813] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:21.817] Server  LOG     ðŸ“¹ Frame received: 53183 bytes from camera cam_003
[00:01:21.818] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:22.098] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:52.107Z"}
[00:01:22.099] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:52.178Z"}
[00:01:22.266] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:52.844Z"}
[00:01:22.445] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:22.479] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:22.490] Server  LOG     ðŸ“¹ Frame received: 51666 bytes from camera cam_001
[00:01:22.493] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:22.539] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:22.564] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:22.574] Server  LOG     ðŸ“¹ Frame received: 53183 bytes from camera cam_003
[00:01:22.583] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:22.636] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:22.660] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:22.676] Server  LOG     ðŸ“¹ Frame received: 60470 bytes from camera cam_002
[00:01:22.681] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:24.442] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:24.570] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:24.601] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:24.616] Server  LOG     ðŸ“¹ Frame received: 51666 bytes from camera cam_001
[00:01:24.633] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:24.652] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:55.662Z"}
[00:01:25.590] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:25.658] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:25.696] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:25.717] Server  LOG     ðŸ“¹ Frame received: 53183 bytes from camera cam_003
[00:01:25.737] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:25.789] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:56.812Z"}
[00:01:25.913] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:26.039] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:57.089Z"}
[00:01:26.084] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:26.121] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:26.128] Server  LOG     ðŸ“¹ Frame received: 60470 bytes from camera cam_002
[00:01:26.139] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:26.187] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:26.243] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:26.291] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:26.307] Server  LOG     ðŸ“¹ Frame received: 51666 bytes from camera cam_001
[00:01:26.320] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:26.365] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:26.392] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:57.398Z"}
[00:01:26.469] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:26.511] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:26.526] Server  LOG     ðŸ“¹ Frame received: 60470 bytes from camera cam_002
[00:01:26.535] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:26.567] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:57.515Z"}
[00:01:26.583] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:26.699] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:26.742] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:26.749] Server  LOG     ðŸ“¹ Frame received: 53183 bytes from camera cam_003
[00:01:26.756] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:26.799] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:57.734Z"}
[00:01:28.125] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:28.219] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:28.240] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:28.265] Server  LOG     ðŸ“¹ Frame received: 48977 bytes from camera cam_001
[00:01:28.274] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:28.310] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:05:59.345Z"}
[00:01:29.347] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:29.432] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:29.470] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:29.484] Server  LOG     ðŸ“¹ Frame received: 55201 bytes from camera cam_002
[00:01:29.497] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:29.521] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:00.568Z"}
[00:01:29.541] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:29.622] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:29.728] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:29.760] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:29.791] Server  LOG     ðŸ“¹ Frame received: 48544 bytes from camera cam_003
[00:01:29.806] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:29.842] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:29.866] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:29.876] Server  LOG     ðŸ“¹ Frame received: 48302 bytes from camera cam_001
[00:01:29.888] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:29.916] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:00.727Z"}
[00:01:29.916] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:00.818Z"}
[00:01:29.926] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:29.994] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:30.075] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:30.108] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:30.118] Server  LOG     ðŸ“¹ Frame received: 54163 bytes from camera cam_002
[00:01:30.146] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:30.175] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:30.222] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:30.230] Server  LOG     ðŸ“¹ Frame received: 49271 bytes from camera cam_003
[00:01:30.236] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:30.275] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:01.013Z"}
[00:01:30.275] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:01.101Z"}
[00:01:31.533] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:31.610] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:32.061] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:02.755Z"}
[00:01:32.764] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:32.800] Browser INFO    %cDownload the React DevTools for a better development experience: https://react.dev/link/react-devtools font-weight:bold
[00:01:33.055] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:33.071] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:33.097] Browser LOG     Starting inference for videos: [1,2,3]
[00:01:33.097] Browser LOG     Starting inference for video: 1 Gate 1 Entrance
[00:01:33.097] Browser LOG     Starting inference for video: 2 Main Hall
[00:01:33.097] Browser LOG     Starting inference for video: 3 Parking Area
[00:01:33.097] Browser LOG     %c[Vercel Web Analytics]%c Debug mode is enabled by default in development. No requests will be sent to the server. color: rgb(120, 120, 120) color: inherit
[00:01:33.097] Browser LOG     %c[Vercel Web Analytics]%c Running queued event color: rgb(120, 120, 120) color: inherit pageview {"path":"/admin/monitoring","route":"/admin/monitoring"}
[00:01:33.097] Browser LOG     %c[Vercel Web Analytics]%c [pageview] http://localhost:3000/admin/monitoring color: rgb(120, 120, 120) color: inherit {"dp":"/admin/monitoring","o":"http://localhost:3000/admin/monitoring","sdkn":"@vercel/analytics/next","sdkv":"1.5.0","sv":"0.1.3","ts":1763751964211}
[00:01:33.666] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:33.695] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:33.702] Server  LOG     ðŸ“¹ Frame received: 51932 bytes from camera cam_001
[00:01:33.705] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:33.745] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:33.762] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:33.809] Server  LOG     ðŸ“¹ Frame received: 52030 bytes from camera cam_003
[00:01:33.819] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:33.883] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:33.969] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:34.021] Server  LOG     ðŸ“¹ Frame received: 9570 bytes from camera cam_002
[00:01:34.031] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:34.661] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:34.952] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:34.964] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:36.147] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:36.167] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:36.267] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:39.186] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:39.196] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:39.201] Server  LOG     ðŸ“¹ Frame received: 52362 bytes from camera cam_001
[00:01:39.201] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:39.386] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:39.396] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:39.399] Server  LOG     ðŸ“¹ Frame received: 9565 bytes from camera cam_002
[00:01:39.402] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:39.738] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:39.756] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:39.764] Server  LOG     ðŸ“¹ Frame received: 53150 bytes from camera cam_003
[00:01:39.765] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:41.157] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:41.198] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:41.271] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:07.371Z"}
[00:01:41.272] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:07.391Z"}
[00:01:41.272] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:07.490Z"}
[00:01:41.595] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:42.074] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:42.086] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:42.089] Server  LOG     ðŸ“¹ Frame received: 40735 bytes from camera cam_001
[00:01:42.090] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:42.396] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:12.379Z"}
[00:01:42.396] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:12.422Z"}
[00:01:42.396] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:12.817Z"}
[00:01:42.956] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:42.966] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:42.983] Server  LOG     ðŸ“¹ Frame received: 62676 bytes from camera cam_002
[00:01:42.984] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:43.866] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:44.952] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:45.152] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:45.165] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:45.258] Server  LOG     ðŸ“¹ Frame received: 50800 bytes from camera cam_003
[00:01:45.262] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:47.956] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:49.416] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:49.428] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:49.431] Server  LOG     ðŸ“¹ Frame received: 62286 bytes from camera cam_002
[00:01:49.434] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:49.560] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:49.568] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:49.574] Server  LOG     ðŸ“¹ Frame received: 50118 bytes from camera cam_003
[00:01:49.575] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:51.416] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:51.448] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:51.743] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:15.086Z"}
[00:01:51.743] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:16.175Z"}
[00:01:51.780] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:51.792] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:51.794] Server  LOG     ðŸ“¹ Frame received: 47881 bytes from camera cam_001
[00:01:51.795] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:51.810] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:51.819] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:51.822] Server  LOG     ðŸ“¹ Frame received: 59112 bytes from camera cam_002
[00:01:51.823] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:51.851] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:51.861] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:51.864] Server  LOG     ðŸ“¹ Frame received: 48384 bytes from camera cam_003
[00:01:51.865] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:51.874] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:51.883] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:51.887] Server  LOG     ðŸ“¹ Frame received: 48438 bytes from camera cam_001
[00:01:51.888] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:51.928] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:51.946] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:51.949] Server  LOG     ðŸ“¹ Frame received: 61193 bytes from camera cam_002
[00:01:51.950] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:51.997] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:52.011] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:52.017] Server  LOG     ðŸ“¹ Frame received: 47745 bytes from camera cam_003
[00:01:52.020] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:57.285] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:57.428] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:01:57.452] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:01:57.476] Server  LOG     ðŸ“¹ Frame received: 47351 bytes from camera cam_001
[00:01:57.483] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:01:57.542] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:57.705] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:57.728] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:57.744] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:57.855] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:01:59.453] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:02:00.264] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:19.170Z"}
[00:02:00.264] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:22.636Z"}
[00:02:00.264] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:22.672Z"}
[00:02:01.459] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:02:01.469] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:02:01.470] Server  LOG     ðŸ“¹ Frame received: 60483 bytes from camera cam_002
[00:02:01.471] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:02:01.489] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:02:01.499] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:02:01.502] Server  LOG     ðŸ“¹ Frame received: 45017 bytes from camera cam_001
[00:02:01.503] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:02:02.955] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:02:03.068] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:02:03.360] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:02:03.371] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:02:03.373] Server  LOG     ðŸ“¹ Frame received: 50958 bytes from camera cam_001
[00:02:03.374] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:02:03.393] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:02:03.402] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:02:03.405] Server  LOG     ðŸ“¹ Frame received: 60733 bytes from camera cam_002
[00:02:03.406] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:02:04.755] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:02:04.770] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:02:04.775] Server  LOG     ðŸ“¹ Frame received: 54550 bytes from camera cam_003
[00:02:04.777] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:02:04.901] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:02:04.927] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:02:05.903] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:02:05.916] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:02:05.919] Server  LOG     ðŸ“¹ Frame received: 45280 bytes from camera cam_001
[00:02:05.920] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:02:05.934] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:02:05.944] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:02:05.947] Server  LOG     ðŸ“¹ Frame received: 60733 bytes from camera cam_002
[00:02:05.949] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:02:06.227] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:02:06.984] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:02:06.993] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:02:06.996] Server  LOG     ðŸ“¹ Frame received: 47889 bytes from camera cam_003
[00:02:06.997] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:02:07.562] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:02:07.594] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:02:08.464] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:02:09.013] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:02:09.022] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:02:09.024] Server  LOG     ðŸ“¹ Frame received: 48585 bytes from camera cam_001
[00:02:09.024] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:02:10.424] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:29.360] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:28.498Z"}
[00:03:29.360] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:28.765Z"}
[00:03:29.360] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:28.934Z"}
[00:03:29.360] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:28.967Z"}
[00:03:29.360] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:29.077Z"}
[00:03:29.360] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:28.920Z"}
[00:03:29.360] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:30.676Z"}
[00:03:29.360] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:34.178Z"}
[00:03:29.360] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:34.292Z"}
[00:03:29.360] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:36.071Z"}
[00:03:29.361] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:36.150Z"}
[00:03:29.361] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:37.449Z"}
[00:03:29.361] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:38.783Z"}
[00:03:29.361] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:38.816Z"}
[00:03:29.361] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:39.685Z"}
[00:03:29.361] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:06:41.648Z"}
[00:03:29.805] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:29.834] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:29.845] Server  LOG     ðŸ“¹ Frame received: 49875 bytes from camera cam_002
[00:03:29.846] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:29.882] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:29.905] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:29.910] Server  LOG     ðŸ“¹ Frame received: 56165 bytes from camera cam_003
[00:03:29.916] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:29.963] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:30.002] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:30.004] Server  LOG     ðŸ“¹ Frame received: 51322 bytes from camera cam_001
[00:03:30.005] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:30.024] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:30.039] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:30.042] Server  LOG     ðŸ“¹ Frame received: 51322 bytes from camera cam_001
[00:03:30.046] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:30.069] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:30.084] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:30.088] Server  LOG     ðŸ“¹ Frame received: 49575 bytes from camera cam_002
[00:03:30.089] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:30.109] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:30.124] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:30.130] Server  LOG     ðŸ“¹ Frame received: 56165 bytes from camera cam_003
[00:03:30.131] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:32.504] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:32.541] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:32.574] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:32.595] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:32.600] Server  LOG     ðŸ“¹ Frame received: 52420 bytes from camera cam_001
[00:03:32.602] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:32.643] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:32.701] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:32.720] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:32.727] Server  LOG     ðŸ“¹ Frame received: 47908 bytes from camera cam_002
[00:03:32.732] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:32.755] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:32.796] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:32.854] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:32.882] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:32.910] Server  LOG     ðŸ“¹ Frame received: 53054 bytes from camera cam_003
[00:03:32.916] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:32.993] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:33.019] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:33.058] Server  LOG     ðŸ“¹ Frame received: 52714 bytes from camera cam_003
[00:03:33.062] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:33.095] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:33.144] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:33.191] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:33.201] Server  LOG     ðŸ“¹ Frame received: 51401 bytes from camera cam_001
[00:03:33.211] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:33.298] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:33.357] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:33.389] Server  LOG     ðŸ“¹ Frame received: 50165 bytes from camera cam_002
[00:03:33.400] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:35.153] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:35.253] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:35.378] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:35.399] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:35.436] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:35.505] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:36.357] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:03.717Z"}
[00:03:36.358] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:03.752Z"}
[00:03:36.358] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:03.819Z"}
[00:03:36.358] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:03.885Z"}
[00:03:36.358] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:04.003Z"}
[00:03:36.358] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:04.218Z"}
[00:03:36.358] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:06.377Z"}
[00:03:36.358] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:06.475Z"}
[00:03:36.358] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:06.602Z"}
[00:03:36.358] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:06.623Z"}
[00:03:36.358] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:06.660Z"}
[00:03:36.358] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:06.730Z"}
[00:03:36.562] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:36.603] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:36.618] Server  LOG     ðŸ“¹ Frame received: 50531 bytes from camera cam_001
[00:03:36.629] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:36.691] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:36.708] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:36.713] Server  LOG     ðŸ“¹ Frame received: 49051 bytes from camera cam_002
[00:03:36.714] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:36.749] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:36.769] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:36.782] Server  LOG     ðŸ“¹ Frame received: 47799 bytes from camera cam_001
[00:03:36.790] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:36.886] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:36.949] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:36.977] Server  LOG     ðŸ“¹ Frame received: 49039 bytes from camera cam_002
[00:03:37.000] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:37.061] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:37.157] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:37.236] Server  LOG     ðŸ“¹ Frame received: 53723 bytes from camera cam_003
[00:03:37.284] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:37.393] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:37.458] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:37.491] Server  LOG     ðŸ“¹ Frame received: 51968 bytes from camera cam_003
[00:03:37.548] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:39.650] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:39.670] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:39.680] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:39.683] Server  LOG     ðŸ“¹ Frame received: 43688 bytes from camera cam_001
[00:03:39.686] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:39.726] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:39.745] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:39.757] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:39.762] Server  LOG     ðŸ“¹ Frame received: 48675 bytes from camera cam_002
[00:03:39.771] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:39.951] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:39.992] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:40.011] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:40.021] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:40.025] Server  LOG     ðŸ“¹ Frame received: 50118 bytes from camera cam_003
[00:03:40.027] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:40.197] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:40.248] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:41.833] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:41.885] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:42.267] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:44.101] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:44.111] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:44.114] Server  LOG     ðŸ“¹ Frame received: 46717 bytes from camera cam_003
[00:03:44.115] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:44.132] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:44.142] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:44.144] Server  LOG     ðŸ“¹ Frame received: 47346 bytes from camera cam_001
[00:03:44.145] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:44.499] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:10.871Z"}
[00:03:44.499] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:10.944Z"}
[00:03:44.499] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:11.171Z"}
[00:03:44.499] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:11.187Z"}
[00:03:44.499] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:11.420Z"}
[00:03:44.499] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:11.467Z"}
[00:03:44.499] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:13.058Z"}
[00:03:44.499] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:13.108Z"}
[00:03:44.499] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:13.490Z"}
[00:03:45.277] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:45.285] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:45.288] Server  LOG     ðŸ“¹ Frame received: 52121 bytes from camera cam_002
[00:03:45.289] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:45.452] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:45.465] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:46.458] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:46.467] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:46.469] Server  LOG     ðŸ“¹ Frame received: 46717 bytes from camera cam_003
[00:03:46.470] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:46.718] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:47.828] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:16.674Z"}
[00:03:47.828] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:16.689Z"}
[00:03:47.930] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:48.093] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:48.104] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:48.106] Server  LOG     ðŸ“¹ Frame received: 47346 bytes from camera cam_001
[00:03:48.107] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:48.205] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:48.213] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:48.215] Server  LOG     ðŸ“¹ Frame received: 52121 bytes from camera cam_002
[00:03:48.216] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:48.408] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:48.419] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:48.422] Server  LOG     ðŸ“¹ Frame received: 46717 bytes from camera cam_003
[00:03:48.423] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:48.450] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:48.461] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:48.464] Server  LOG     ðŸ“¹ Frame received: 47346 bytes from camera cam_001
[00:03:48.465] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:48.483] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:48.497] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:48.502] Server  LOG     ðŸ“¹ Frame received: 52121 bytes from camera cam_002
[00:03:48.504] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:48.523] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:48.534] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:48.538] Server  LOG     ðŸ“¹ Frame received: 46717 bytes from camera cam_003
[00:03:48.539] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:49.282] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:17.942Z"}
[00:03:49.283] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:19.155Z"}
[00:03:49.684] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:49.858] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:49.866] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:49.870] Server  LOG     ðŸ“¹ Frame received: 47346 bytes from camera cam_001
[00:03:49.871] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:49.878] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:49.913] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:49.922] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:49.926] Server  LOG     ðŸ“¹ Frame received: 52121 bytes from camera cam_002
[00:03:49.928] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:50.032] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:50.047] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:50.057] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:50.060] Server  LOG     ðŸ“¹ Frame received: 46717 bytes from camera cam_003
[00:03:50.061] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:50.107] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:50.220] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:50.332] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:50.346] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:50.350] Server  LOG     ðŸ“¹ Frame received: 52121 bytes from camera cam_002
[00:03:50.351] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:50.365] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:50.410] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:50.414] Server  LOG     ðŸ“¹ Frame received: 47346 bytes from camera cam_001
[00:03:50.415] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:50.428] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:50.453] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:50.463] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:50.467] Server  LOG     ðŸ“¹ Frame received: 46717 bytes from camera cam_003
[00:03:50.468] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:51.378] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:20.908Z"}
[00:03:51.378] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:21.086Z"}
[00:03:51.378] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:21.256Z"}
[00:03:51.378] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:21.329Z"}
[00:03:51.378] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:21.335Z"}
[00:03:51.378] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:21.573Z"}
[00:03:51.426] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:51.454] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:51.466] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:51.471] Server  LOG     ðŸ“¹ Frame received: 47346 bytes from camera cam_001
[00:03:51.472] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:51.573] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:51.594] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:51.607] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:51.610] Server  LOG     ðŸ“¹ Frame received: 52121 bytes from camera cam_002
[00:03:51.613] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:51.794] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:51.811] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:51.825] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:51.830] Server  LOG     ðŸ“¹ Frame received: 46717 bytes from camera cam_003
[00:03:51.831] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:51.908] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:51.925] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:51.934] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:51.937] Server  LOG     ðŸ“¹ Frame received: 50112 bytes from camera cam_002
[00:03:51.937] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:52.029] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:52.054] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:52.073] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:52.081] Server  LOG     ðŸ“¹ Frame received: 47346 bytes from camera cam_001
[00:03:52.083] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:52.155] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:52.171] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:52.190] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:52.195] Server  LOG     ðŸ“¹ Frame received: 47346 bytes from camera cam_001
[00:03:52.197] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:53.239] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:53.258] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:53.268] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:53.270] Server  LOG     ðŸ“¹ Frame received: 52121 bytes from camera cam_002
[00:03:53.273] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:53.283] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:53.303] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:53.313] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:53.317] Server  LOG     ðŸ“¹ Frame received: 46717 bytes from camera cam_003
[00:03:53.320] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:53.333] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:53.361] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:53.372] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:53.377] Server  LOG     ðŸ“¹ Frame received: 47346 bytes from camera cam_001
[00:03:53.380] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:53.454] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:53.469] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:53.477] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:53.480] Server  LOG     ðŸ“¹ Frame received: 52121 bytes from camera cam_002
[00:03:53.481] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:53.558] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:53.573] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:03:53.581] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:03:53.583] Server  LOG     ðŸ“¹ Frame received: 46717 bytes from camera cam_003
[00:03:53.585] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:03:53.662] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:54.515] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:54.566] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:54.661] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:54.715] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:03:54.775] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:07.950] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:22.649Z"}
[00:04:07.950] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:22.798Z"}
[00:04:07.950] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:23.017Z"}
[00:04:07.950] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:23.131Z"}
[00:04:07.950] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:23.251Z"}
[00:04:07.950] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:23.378Z"}
[00:04:07.950] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:24.461Z"}
[00:04:07.950] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:24.483Z"}
[00:04:07.950] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:24.546Z"}
[00:04:07.950] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:24.677Z"}
[00:04:07.950] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:24.782Z"}
[00:04:07.950] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:24.886Z"}
[00:04:07.950] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:25.739Z"}
[00:04:07.950] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:25.791Z"}
[00:04:07.950] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:25.886Z"}
[00:04:07.951] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:25.940Z"}
[00:04:07.951] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:26.000Z"}
[00:04:08.255] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:08.310] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:08.321] Server  LOG     ðŸ“¹ Frame received: 52121 bytes from camera cam_002
[00:04:08.332] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:08.506] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:08.547] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:08.554] Server  LOG     ðŸ“¹ Frame received: 51932 bytes from camera cam_001
[00:04:08.561] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:08.601] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:08.616] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:08.621] Server  LOG     ðŸ“¹ Frame received: 52155 bytes from camera cam_002
[00:04:08.623] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:08.650] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:08.671] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:08.673] Server  LOG     ðŸ“¹ Frame received: 52713 bytes from camera cam_003
[00:04:08.678] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:08.993] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:09.012] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:09.018] Server  LOG     ðŸ“¹ Frame received: 53653 bytes from camera cam_001
[00:04:09.020] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:10.671] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:10.696] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:10.703] Server  LOG     ðŸ“¹ Frame received: 51483 bytes from camera cam_002
[00:04:10.709] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:11.147] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:11.225] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:11.361] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:11.400] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:11.417] Server  LOG     ðŸ“¹ Frame received: 53584 bytes from camera cam_003
[00:04:11.422] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:11.488] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:42.367Z"}
[00:04:11.488] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:42.421Z"}
[00:04:11.545] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:11.595] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:11.608] Server  LOG     ðŸ“¹ Frame received: 50787 bytes from camera cam_001
[00:04:11.615] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:11.775] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:11.819] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:11.838] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:11.844] Server  LOG     ðŸ“¹ Frame received: 52372 bytes from camera cam_002
[00:04:11.846] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:11.869] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:11.922] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:11.965] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:11.991] Server  LOG     ðŸ“¹ Frame received: 52243 bytes from camera cam_003
[00:04:12.002] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:12.027] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:42.994Z"}
[00:04:12.028] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:43.065Z"}
[00:04:12.076] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:12.208] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:12.276] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:12.288] Server  LOG     ðŸ“¹ Frame received: 52853 bytes from camera cam_003
[00:04:12.295] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:12.331] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:43.299Z"}
[00:04:14.471] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:14.505] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:14.868] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:14.974] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:15.089] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:15.100] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:15.106] Server  ERROR   âŒ [API] Error: Failed to parse body as FormData.
[00:04:15.172] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:04:15.254] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:04:15.260] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\trace\tracer.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:04:15.274] Server  ERROR   TypeError: Failed to parse body as FormData.
[00:04:15.283] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:15.290] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:15.468] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:17.155] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:17.271] Browser INFO    %cDownload the React DevTools for a better development experience: https://react.dev/link/react-devtools font-weight:bold
[00:04:18.177] Browser LOG     Starting inference for videos: [1,2,3]
[00:04:18.177] Browser LOG     Starting inference for video: 1 Gate 1 Entrance
[00:04:18.177] Browser LOG     Starting inference for video: 2 Main Hall
[00:04:18.177] Browser LOG     Starting inference for video: 3 Parking Area
[00:04:18.177] Browser LOG     %c[Vercel Web Analytics]%c Debug mode is enabled by default in development. No requests will be sent to the server. color: rgb(120, 120, 120) color: inherit
[00:04:18.177] Browser LOG     %c[Vercel Web Analytics]%c Running queued event color: rgb(120, 120, 120) color: inherit pageview {"path":"/admin/monitoring","route":"/admin/monitoring"}
[00:04:18.177] Browser LOG     %c[Vercel Web Analytics]%c [pageview] http://localhost:3000/admin/monitoring color: rgb(120, 120, 120) color: inherit {"dp":"/admin/monitoring","o":"http://localhost:3000/admin/monitoring","sdkn":"@vercel/analytics/next","sdkv":"1.5.0","sv":"0.1.3","ts":1763752128850}
[00:04:18.245] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:18.254] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:18.256] Server  LOG     ðŸ“¹ Frame received: 51322 bytes from camera cam_001
[00:04:18.257] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:18.364] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:18.374] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:18.378] Server  LOG     ðŸ“¹ Frame received: 9571 bytes from camera cam_002
[00:04:18.379] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:18.435] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:18.444] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:18.447] Server  LOG     ðŸ“¹ Frame received: 53162 bytes from camera cam_003
[00:04:18.448] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:19.555] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:19.637] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:19.736] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:20.162] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:20.170] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:20.173] Server  LOG     ðŸ“¹ Frame received: 51322 bytes from camera cam_001
[00:04:20.173] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:20.190] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:20.199] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:20.201] Server  LOG     ðŸ“¹ Frame received: 9571 bytes from camera cam_002
[00:04:20.202] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:20.272] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:20.281] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:20.283] Server  LOG     ðŸ“¹ Frame received: 53162 bytes from camera cam_003
[00:04:20.284] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:21.485] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:21.491] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:21.531] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:50.778Z"}
[00:04:21.531] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:50.862Z"}
[00:04:21.531] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:50.961Z"}
[00:04:21.591] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:21.664] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:21.672] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:21.674] Server  LOG     ðŸ“¹ Frame received: 51322 bytes from camera cam_001
[00:04:21.675] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:21.690] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:21.699] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:21.705] Server  LOG     ðŸ“¹ Frame received: 9571 bytes from camera cam_002
[00:04:21.707] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:21.780] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:21.788] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:21.790] Server  LOG     ðŸ“¹ Frame received: 53162 bytes from camera cam_003
[00:04:21.791] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:22.557] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:52.709Z"}
[00:04:22.557] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:52.715Z"}
[00:04:22.713] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:22.762] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:22.775] Server  LOG     ðŸ“¹ Frame received: 51322 bytes from camera cam_001
[00:04:22.782] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:22.837] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:22.858] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:22.860] Server  LOG     ðŸ“¹ Frame received: 9571 bytes from camera cam_002
[00:04:22.866] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:22.881] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:22.895] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:22.899] Server  LOG     ðŸ“¹ Frame received: 53162 bytes from camera cam_003
[00:04:22.900] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:22.908] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:52.815Z"}
[00:04:24.170] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:24.241] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:24.266] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:24.273] Server  LOG     ðŸ“¹ Frame received: 51322 bytes from camera cam_001
[00:04:24.277] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:24.298] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:24.323] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:55.392Z"}
[00:04:24.366] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:24.384] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:24.390] Server  LOG     ðŸ“¹ Frame received: 9571 bytes from camera cam_002
[00:04:24.394] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:24.414] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:24.440] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:55.476Z"}
[00:04:24.470] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:24.495] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:24.505] Server  LOG     ðŸ“¹ Frame received: 53162 bytes from camera cam_003
[00:04:24.510] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:24.523] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:55.606Z"}
[00:04:25.767] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:25.805] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:25.823] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:25.832] Server  LOG     ðŸ“¹ Frame received: 51322 bytes from camera cam_001
[00:04:25.837] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:25.874] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:56.992Z"}
[00:04:26.128] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:26.157] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:26.186] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:26.201] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:26.207] Server  LOG     ðŸ“¹ Frame received: 51720 bytes from camera cam_001
[00:04:26.210] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:26.240] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:26.254] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:26.261] Server  LOG     ðŸ“¹ Frame received: 11402 bytes from camera cam_002
[00:04:26.295] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:27.122] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:27.138] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:27.170] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:27.179] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:27.182] Server  LOG     ðŸ“¹ Frame received: 53150 bytes from camera cam_003
[00:04:27.185] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:27.204] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:27.214] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:27.218] Server  LOG     ðŸ“¹ Frame received: 51666 bytes from camera cam_001
[00:04:27.221] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:27.354] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:27.374] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:27.385] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:27.389] Server  LOG     ðŸ“¹ Frame received: 61762 bytes from camera cam_002
[00:04:27.390] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:28.147] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:28.170] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:28.183] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:28.197] Server  LOG     ðŸ“¹ Frame received: 52283 bytes from camera cam_003
[00:04:28.199] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:28.593] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:28.616] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:28.627] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:28.630] Server  LOG     ðŸ“¹ Frame received: 49376 bytes from camera cam_001
[00:04:28.631] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:28.690] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:28.704] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:28.716] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:28.718] Server  LOG     ðŸ“¹ Frame received: 64073 bytes from camera cam_002
[00:04:28.721] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:29.151] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:29.185] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:29.198] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:29.201] Server  LOG     ðŸ“¹ Frame received: 52375 bytes from camera cam_003
[00:04:29.204] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:29.352] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:29.371] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:29.379] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:29.384] Server  LOG     ðŸ“¹ Frame received: 48777 bytes from camera cam_001
[00:04:29.395] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:29.618] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:29.632] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:29.645] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:29.650] Server  LOG     ðŸ“¹ Frame received: 63377 bytes from camera cam_002
[00:04:29.651] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:30.381] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:57.348Z"}
[00:04:30.381] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:57.354Z"}
[00:04:30.381] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:58.335Z"}
[00:04:30.381] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:58.348Z"}
[00:04:30.381] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:58.562Z"}
[00:04:30.381] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:59.371Z"}
[00:04:30.381] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:59.809Z"}
[00:04:30.381] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:08:59.913Z"}
[00:04:30.381] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:00.369Z"}
[00:04:30.381] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:00.576Z"}
[00:04:30.381] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:00.842Z"}
[00:04:31.038] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:31.101] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:31.142] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:31.151] Server  LOG     ðŸ“¹ Frame received: 52228 bytes from camera cam_003
[00:04:31.154] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:31.179] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:02.260Z"}
[00:04:31.322] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:31.362] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:31.387] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:31.395] Server  LOG     ðŸ“¹ Frame received: 46701 bytes from camera cam_001
[00:04:31.404] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:31.427] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:02.545Z"}
[00:04:31.625] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:31.674] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:31.690] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:31.699] Server  LOG     ðŸ“¹ Frame received: 63545 bytes from camera cam_002
[00:04:31.704] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:31.730] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:02.846Z"}
[00:04:31.885] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:31.939] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:31.964] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:31.974] Server  LOG     ðŸ“¹ Frame received: 54066 bytes from camera cam_003
[00:04:31.979] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:31.995] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:03.108Z"}
[00:04:32.203] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:32.250] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:32.269] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:32.276] Server  LOG     ðŸ“¹ Frame received: 46407 bytes from camera cam_001
[00:04:32.280] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:32.312] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:03.425Z"}
[00:04:32.754] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:32.803] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:32.824] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:32.831] Server  LOG     ðŸ“¹ Frame received: 63371 bytes from camera cam_002
[00:04:32.835] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:32.859] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:03.976Z"}
[00:04:33.835] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:33.874] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:33.894] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:33.902] Server  LOG     ðŸ“¹ Frame received: 51948 bytes from camera cam_003
[00:04:33.909] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:33.944] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:05.058Z"}
[00:04:34.004] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:34.060] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:34.095] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:34.106] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:04:34.109] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:34.122] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:05.229Z"}
[00:04:34.500] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:34.539] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:34.559] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:34.581] Server  LOG     ðŸ“¹ Frame received: 63548 bytes from camera cam_002
[00:04:34.599] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:34.630] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:05.724Z"}
[00:04:34.905] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:34.965] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:34.978] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:34.984] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:04:34.988] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:35.008] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:35.073] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:35.110] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:35.116] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:04:35.120] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:35.134] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:06.127Z"}
[00:04:35.134] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:06.229Z"}
[00:04:35.414] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:35.472] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:35.492] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:35.505] Server  LOG     ðŸ“¹ Frame received: 59038 bytes from camera cam_002
[00:04:35.511] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:35.527] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:06.634Z"}
[00:04:36.682] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:36.723] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:36.745] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:36.752] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:04:36.756] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:36.789] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:07.903Z"}
[00:04:36.965] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:37.004] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:37.019] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:37.025] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:04:37.029] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:37.083] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:08.187Z"}
[00:04:37.400] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:37.446] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:37.468] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:37.485] Server  LOG     ðŸ“¹ Frame received: 59751 bytes from camera cam_002
[00:04:37.493] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:37.506] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:08.624Z"}
[00:04:37.750] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:37.833] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:37.864] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:37.870] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:04:37.882] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:37.899] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:08.963Z"}
[00:04:38.087] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:38.145] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:38.163] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:38.170] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:04:38.177] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:38.213] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:09.307Z"}
[00:04:38.304] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:38.353] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:38.385] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:38.407] Server  LOG     ðŸ“¹ Frame received: 61327 bytes from camera cam_002
[00:04:38.413] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:38.437] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:09.528Z"}
[00:04:39.594] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:39.660] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:39.677] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:39.683] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:04:39.686] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:39.703] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:39.724] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:10.812Z"}
[00:04:39.749] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:39.794] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:39.799] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:04:39.802] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:39.816] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:10.924Z"}
[00:04:40.082] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:40.170] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:40.219] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:40.232] Server  LOG     ðŸ“¹ Frame received: 61161 bytes from camera cam_002
[00:04:40.241] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:40.261] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:11.296Z"}
[00:04:40.875] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:40.927] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:40.950] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:40.959] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:04:40.967] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:40.987] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:12.098Z"}
[00:04:41.031] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:41.101] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:41.125] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:41.131] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:04:41.141] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:41.172] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:12.255Z"}
[00:04:41.287] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:41.339] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:41.364] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:41.370] Server  LOG     ðŸ“¹ Frame received: 60894 bytes from camera cam_002
[00:04:41.375] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:41.399] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:12.509Z"}
[00:04:42.366] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:42.397] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:42.424] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:42.432] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:04:42.434] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:42.471] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:13.589Z"}
[00:04:42.492] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:42.557] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:42.575] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:42.587] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:04:42.595] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:42.613] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:13.716Z"}
[00:04:42.849] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:42.885] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:42.900] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:42.904] Server  LOG     ðŸ“¹ Frame received: 60462 bytes from camera cam_002
[00:04:42.906] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:42.955] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:14.073Z"}
[00:04:43.548] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:43.603] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:43.630] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:43.642] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:04:43.648] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:43.661] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:14.773Z"}
[00:04:43.749] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:43.816] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:43.836] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:43.844] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:04:43.851] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:43.878] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:14.967Z"}
[00:04:44.028] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:44.068] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:44.102] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:44.110] Server  LOG     ðŸ“¹ Frame received: 60130 bytes from camera cam_002
[00:04:44.120] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:44.141] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:15.251Z"}
[00:04:45.217] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:45.274] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:45.297] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:45.304] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:04:45.310] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:45.332] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:16.439Z"}
[00:04:45.728] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:45.803] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:45.817] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:45.829] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:04:45.832] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:45.845] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:45.881] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:45.930] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:45.970] Server  LOG     ðŸ“¹ Frame received: 60770 bytes from camera cam_002
[00:04:45.991] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:46.683] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:46.706] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:46.740] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:46.753] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:46.763] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:04:46.769] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:46.784] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:46.805] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:46.813] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:04:46.823] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:46.851] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:46.977] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:46.996] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:47.004] Server  LOG     ðŸ“¹ Frame received: 59826 bytes from camera cam_002
[00:04:47.011] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:48.109] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:48.156] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:48.202] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:48.237] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:04:48.281] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:48.665] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:48.689] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:48.721] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:48.734] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:04:48.752] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:49.732] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:49.796] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:49.844] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:49.872] Server  LOG     ðŸ“¹ Frame received: 56843 bytes from camera cam_002
[00:04:49.886] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:50.520] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:50.560] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:50.587] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:50.589] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:04:50.594] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:51.050] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:51.094] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:51.118] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:51.120] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:04:51.121] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:51.585] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:51.602] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:51.612] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:51.615] Server  LOG     ðŸ“¹ Frame received: 57722 bytes from camera cam_002
[00:04:51.617] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:51.984] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:52.052] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:52.069] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:52.072] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:04:52.073] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:52.891] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:52.926] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:52.937] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:52.946] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:04:52.950] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:53.381] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:53.448] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:53.469] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:53.477] Server  LOG     ðŸ“¹ Frame received: 57030 bytes from camera cam_002
[00:04:53.484] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:53.808] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:53.831] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:53.859] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:53.860] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:04:53.863] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:53.922] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:53.979] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:53.992] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:54.040] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:04:54.054] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:54.853] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:54.873] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:54.885] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:54.892] Server  LOG     ðŸ“¹ Frame received: 57050 bytes from camera cam_002
[00:04:54.895] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:55.415] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:55.451] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:55.463] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:55.465] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:04:55.467] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:56.184] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:56.228] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:56.246] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:56.248] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:04:56.252] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:56.745] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:56.790] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:56.815] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:56.822] Server  LOG     ðŸ“¹ Frame received: 57362 bytes from camera cam_002
[00:04:56.824] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:57.033] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:57.067] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:57.109] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:57.113] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:04:57.127] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:58.093] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:58.121] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:58.135] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:58.146] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:04:58.153] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:58.201] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:58.234] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:58.246] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:58.255] Server  LOG     ðŸ“¹ Frame received: 56749 bytes from camera cam_002
[00:04:58.268] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:59.166] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:59.196] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:59.212] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:59.220] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:04:59.228] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:04:59.294] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:04:59.316] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:04:59.330] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:04:59.339] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:04:59.349] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:01.228] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:01.253] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:01.267] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:01.269] Server  LOG     ðŸ“¹ Frame received: 55201 bytes from camera cam_002
[00:05:01.272] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:01.378] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:01.463] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:01.505] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:01.532] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:05:01.552] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:02.193] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:02.256] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:02.275] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:02.283] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:05:02.287] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:02.419] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:02.449] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:02.462] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:02.467] Server  LOG     ðŸ“¹ Frame received: 54570 bytes from camera cam_002
[00:05:02.481] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:03.005] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:03.033] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:03.043] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:03.071] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:05:03.087] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:03.560] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:03.587] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:03.603] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:03.610] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:05:03.613] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:04.685] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:04.731] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:04.750] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:04.767] Server  LOG     ðŸ“¹ Frame received: 52118 bytes from camera cam_002
[00:05:04.779] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:04.975] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:05.018] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:05.030] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:05.038] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:05:05.045] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:05.844] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:05.887] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:05.901] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:05.905] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:05:05.909] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:05.929] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:05.995] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:06.027] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:06.064] Server  LOG     ðŸ“¹ Frame received: 51405 bytes from camera cam_002
[00:05:06.088] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:06.502] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:06.539] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:06.557] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:06.579] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:05:06.614] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:07.186] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:07.215] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:07.226] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:07.231] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:05:07.233] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:09.234] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:09.260] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:09.277] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:09.289] Server  LOG     ðŸ“¹ Frame received: 52622 bytes from camera cam_002
[00:05:09.308] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:09.656] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:09.779] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:09.826] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:09.857] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:05:09.865] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:10.176] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:10.205] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:10.214] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:10.217] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:05:10.220] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:10.233] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:10.254] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:10.266] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:10.274] Server  LOG     ðŸ“¹ Frame received: 49794 bytes from camera cam_002
[00:05:10.276] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:10.512] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:10.547] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:10.571] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:10.581] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:05:10.594] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:11.483] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:11.529] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:11.547] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:11.552] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:05:11.572] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:12.404] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:12.426] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:12.447] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:12.454] Server  LOG     ðŸ“¹ Frame received: 48581 bytes from camera cam_002
[00:05:12.460] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:12.505] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:12.532] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:12.547] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:12.568] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:05:12.572] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:12.872] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:12.908] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:12.922] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:12.960] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:05:12.976] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:13.034] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:13.051] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:13.064] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:13.069] Server  LOG     ðŸ“¹ Frame received: 48080 bytes from camera cam_002
[00:05:13.070] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:13.289] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:13.328] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:13.341] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:13.346] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:05:13.350] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:14.659] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:14.697] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:14.724] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:14.737] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:05:14.745] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:14.997] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:15.031] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:15.054] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:15.059] Server  LOG     ðŸ“¹ Frame received: 51266 bytes from camera cam_002
[00:05:15.060] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:15.073] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:15.108] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:15.127] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:15.135] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:05:15.135] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:15.679] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:15.699] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:15.728] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:15.730] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:05:15.741] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:16.324] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:16.337] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:16.358] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:16.366] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:16.368] Server  LOG     ðŸ“¹ Frame received: 49310 bytes from camera cam_002
[00:05:16.369] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:16.379] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:16.387] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:16.390] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:05:16.391] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:17.346] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:17.372] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:17.384] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:17.392] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:05:17.393] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:17.687] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:17.727] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:17.738] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:17.746] Server  LOG     ðŸ“¹ Frame received: 49575 bytes from camera cam_002
[00:05:17.747] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:18.083] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:18.113] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:18.137] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:18.140] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:05:18.149] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:18.519] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:18.572] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:18.585] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:18.592] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:05:18.596] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:18.830] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:18.878] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:18.891] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:18.900] Server  LOG     ðŸ“¹ Frame received: 47717 bytes from camera cam_002
[00:05:18.901] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:19.059] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:19.083] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:19.093] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:19.095] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:05:19.096] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:20.157] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:20.176] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:20.185] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:20.186] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:05:20.191] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:20.561] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:20.587] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:20.599] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:20.603] Server  LOG     ðŸ“¹ Frame received: 49582 bytes from camera cam_002
[00:05:20.606] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:21.035] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:21.048] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:21.098] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:21.106] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:21.108] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:05:21.112] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:21.135] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:21.148] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:21.155] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:05:21.156] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:21.626] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:21.642] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:21.660] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:21.668] Server  LOG     ðŸ“¹ Frame received: 49848 bytes from camera cam_002
[00:05:21.671] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:21.720] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:21.772] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:21.794] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:21.802] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:05:21.817] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:22.912] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:22.939] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:22.948] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:22.953] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:05:22.954] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:23.513] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:23.549] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:23.560] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:23.563] Server  LOG     ðŸ“¹ Frame received: 49629 bytes from camera cam_002
[00:05:23.565] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:23.943] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:23.964] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:23.981] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:23.982] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:05:23.983] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:24.155] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:24.233] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:24.377] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:25.954] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:26.550] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:26.726] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:28.211] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:28.220] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:28.224] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:05:28.225] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:28.244] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:28.253] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:28.255] Server  LOG     ðŸ“¹ Frame received: 52371 bytes from camera cam_002
[00:05:28.256] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:28.266] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:28.281] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:28.283] Server  LOG     ðŸ“¹ Frame received: 51004 bytes from camera cam_003
[00:05:28.284] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:30.003] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:30.051] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:30.184] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:05:30.432] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:05:30.440] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:05:30.442] Server  LOG     ðŸ“¹ Frame received: 42731 bytes from camera cam_001
[00:05:30.443] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:05:31.973] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:08:48.643] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:08:48.694] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:08:48.700] Server  LOG     ðŸ“¹ Frame received: 52729 bytes from camera cam_002
[00:08:48.702] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:08:50.366] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.1813
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:16.946Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:17.057Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:17.899Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:17.910Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:18.009Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:19.329Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:19.884Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:20.919Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:21.743Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:22.272Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:22.801Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:23.197Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:24.107Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:24.600Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:25.022Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:25.143Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:26.072Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:26.622Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:27.404Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:27.909Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:28.246Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:29.298Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:29.417Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:30.367Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:30.514Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:32.445Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:32.580Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:33.391Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:33.637Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:34.216Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:34.781Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:35.902Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:36.190Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:37.022Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:37.143Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:37.700Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:38.399Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:40.454Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:40.833Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:41.395Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:41.444Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:41.728Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:42.702Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:43.597Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:43.721Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:44.070Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:44.256Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:44.509Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:45.876Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:46.217Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:46.251Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:46.898Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:47.542Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:47.549Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:48.543Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:48.909Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:49.288Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:49.723Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:50.044Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:50.277Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:51.368Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:51.784Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:52.244Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:52.265Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:52.850Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:52.935Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:54.128Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:54.726Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:55.165Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:55.367Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:55.453Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:55.598Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:57.174Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:57.773Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:09:57.950Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:10:01.224Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:10:01.274Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:10:01.409Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:10:03.197Z"}
[00:08:51.169] Browser LOG     âœ… Real inference result: {"confidence":0.1813,"detection_id":"6920b9d1546593c4ead4b3f5","panic_detected":false,"timestamp":"2025-11-21T19:13:21.564Z"}
[00:09:06.228] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:09:06.236] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:09:06.238] Server  LOG     ðŸ“¹ Frame received: 53162 bytes from camera cam_003
[00:09:06.239] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:09:06.267] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:09:06.280] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:09:06.283] Server  LOG     ðŸ“¹ Frame received: 51322 bytes from camera cam_001
[00:09:06.284] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:09:06.305] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:09:06.315] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:09:06.321] Server  LOG     ðŸ“¹ Frame received: 52729 bytes from camera cam_002
[00:09:06.322] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:09:06.995] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:09:07.067] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5773
[00:09:07.123] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4303
[00:09:07.213] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:13:38.216Z"}
[00:09:08.166] Browser LOG     âœ… Real inference result: {"confidence":0.5773,"detection_id":"6920b9e1546593c4ead4b3fb","panic_detected":false,"timestamp":"2025-11-21T19:13:38.291Z"}
[00:09:08.166] Browser LOG     âœ… Real inference result: {"confidence":0.4303,"detection_id":"6920b9e2546593c4ead4b3fd","panic_detected":false,"timestamp":"2025-11-21T19:13:38.346Z"}
[00:10:06.218] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:10:06.225] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:10:06.227] Server  LOG     ðŸ“¹ Frame received: 51322 bytes from camera cam_001
[00:10:06.228] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:10:06.237] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:10:06.246] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:10:06.248] Server  LOG     ðŸ“¹ Frame received: 52729 bytes from camera cam_002
[00:10:06.249] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:10:06.263] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:10:06.271] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:10:06.273] Server  LOG     ðŸ“¹ Frame received: 53162 bytes from camera cam_003
[00:10:06.274] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:10:06.839] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.3909
[00:10:06.857] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5146
[00:10:06.883] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4737
[00:10:07.169] Browser LOG     âœ… Real inference result: {"confidence":0.3909,"detection_id":"6920ba1d546593c4ead4b409","panic_detected":false,"timestamp":"2025-11-21T19:14:38.064Z"}
[00:10:07.169] Browser LOG     âœ… Real inference result: {"confidence":0.5146,"detection_id":"6920ba1d546593c4ead4b405","panic_detected":false,"timestamp":"2025-11-21T19:14:38.081Z"}
[00:10:07.169] Browser LOG     âœ… Real inference result: {"confidence":0.4737,"detection_id":"6920ba1d546593c4ead4b407","panic_detected":false,"timestamp":"2025-11-21T19:14:38.107Z"}
[00:11:06.226] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:06.233] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:06.236] Server  LOG     ðŸ“¹ Frame received: 51322 bytes from camera cam_001
[00:11:06.237] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:06.254] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:06.262] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:06.264] Server  LOG     ðŸ“¹ Frame received: 53162 bytes from camera cam_003
[00:11:06.265] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:06.274] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:06.281] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:06.284] Server  LOG     ðŸ“¹ Frame received: 52729 bytes from camera cam_002
[00:11:06.285] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:06.929] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.3925
[00:11:06.935] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.6245
[00:11:06.943] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5005
[00:11:07.164] Browser LOG     âœ… Real inference result: {"confidence":0.3925,"detection_id":"6920ba59546593c4ead4b415","panic_detected":false,"timestamp":"2025-11-21T19:15:38.153Z"}
[00:11:07.164] Browser LOG     âœ… Real inference result: {"confidence":0.6245,"detection_id":"6920ba59546593c4ead4b411","panic_detected":false,"timestamp":"2025-11-21T19:15:38.154Z"}
[00:11:07.164] Browser LOG     âœ… Real inference result: {"confidence":0.5005,"detection_id":"6920ba59546593c4ead4b413","panic_detected":false,"timestamp":"2025-11-21T19:15:38.157Z"}
[00:11:37.149] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:37.167] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:37.168] Server  LOG     ðŸ“¹ Frame received: 51322 bytes from camera cam_001
[00:11:37.170] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:37.194] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:37.210] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:37.214] Server  LOG     ðŸ“¹ Frame received: 52729 bytes from camera cam_002
[00:11:37.218] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:37.307] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:37.345] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:37.349] Server  LOG     ðŸ“¹ Frame received: 53162 bytes from camera cam_003
[00:11:37.355] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:37.474] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:37.527] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:37.531] Server  LOG     ðŸ“¹ Frame received: 51322 bytes from camera cam_001
[00:11:37.543] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:37.600] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:37.634] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:37.639] Server  LOG     ðŸ“¹ Frame received: 52729 bytes from camera cam_002
[00:11:37.641] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:37.658] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:37.681] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:37.689] Server  LOG     ðŸ“¹ Frame received: 53162 bytes from camera cam_003
[00:11:37.695] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:39.248] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.6848
[00:11:39.264] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.3551
[00:11:39.293] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4236
[00:11:39.328] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:39.370] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:39.374] Server  LOG     ðŸ“¹ Frame received: 52420 bytes from camera cam_001
[00:11:39.378] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:39.401] Browser LOG     âœ… Real inference result: {"confidence":0.6848,"detection_id":"6920ba7a546593c4ead4b41f","panic_detected":false,"timestamp":"2025-11-21T19:16:10.471Z"}
[00:11:39.422] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:39.445] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:39.446] Server  LOG     ðŸ“¹ Frame received: 52102 bytes from camera cam_002
[00:11:39.451] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:39.487] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:39.522] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:39.534] Server  LOG     ðŸ“¹ Frame received: 53721 bytes from camera cam_003
[00:11:39.541] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:39.607] Browser LOG     âœ… Real inference result: {"confidence":0.3551,"detection_id":"6920ba7a546593c4ead4b41d","panic_detected":false,"timestamp":"2025-11-21T19:16:10.474Z"}
[00:11:39.607] Browser LOG     âœ… Real inference result: {"confidence":0.4236,"detection_id":"6920ba7a546593c4ead4b421","panic_detected":false,"timestamp":"2025-11-21T19:16:10.499Z"}
[00:11:39.639] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.3145
[00:11:39.702] Server  LOG     âœ… Inference successful - Panic: true, Confidence: 0.7403
[00:11:39.929] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:40.195] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:40.222] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:40.224] Server  LOG     ðŸ“¹ Frame received: 51148 bytes from camera cam_001
[00:11:40.229] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:40.248] Browser LOG     âœ… Real inference result: {"confidence":0.3145,"detection_id":"6920ba7a546593c4ead4b42d","panic_detected":false,"timestamp":"2025-11-21T19:16:10.685Z"}
[00:11:40.248] Browser LOG     âœ… Real inference result: {"confidence":0.7403,"detection_id":"6920ba7a546593c4ead4b427","panic_detected":true,"timestamp":"2025-11-21T19:16:10.749Z"}
[00:11:40.248] Browser LOG     Panic detected at: Gate 1 Entrance Confidence: 0.7403
[00:11:41.013] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5033
[00:11:41.189] Browser LOG     âœ… Real inference result: {"confidence":0.5033,"detection_id":"6920ba7a546593c4ead4b42b","panic_detected":false,"timestamp":"2025-11-21T19:16:11.407Z"}
[00:11:41.623] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.432
[00:11:41.755] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.6238
[00:11:41.825] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.2717
[00:11:42.080] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:42.144] Browser INFO    %cDownload the React DevTools for a better development experience: https://react.dev/link/react-devtools font-weight:bold
[00:11:42.259] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.6767
[00:11:42.581] Browser LOG     Starting inference for videos: [1,2,3]
[00:11:42.581] Browser LOG     Starting inference for video: 1 Gate 1 Entrance
[00:11:42.581] Browser LOG     Starting inference for video: 2 Main Hall
[00:11:42.581] Browser LOG     Starting inference for video: 3 Parking Area
[00:11:42.581] Browser LOG     %c[Vercel Web Analytics]%c Debug mode is enabled by default in development. No requests will be sent to the server. color: rgb(120, 120, 120) color: inherit
[00:11:42.581] Browser LOG     %c[Vercel Web Analytics]%c Running queued event color: rgb(120, 120, 120) color: inherit pageview {"path":"/admin/monitoring","route":"/admin/monitoring"}
[00:11:42.581] Browser LOG     %c[Vercel Web Analytics]%c [pageview] http://localhost:3000/admin/monitoring color: rgb(120, 120, 120) color: inherit {"dp":"/admin/monitoring","o":"http://localhost:3000/admin/monitoring","sdkn":"@vercel/analytics/next","sdkv":"1.5.0","sv":"0.1.3","ts":1763752573697}
[00:11:42.952] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:42.979] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:42.980] Server  LOG     ðŸ“¹ Frame received: 9570 bytes from camera cam_002
[00:11:42.981] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:43.015] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:43.030] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:43.043] Server  LOG     ðŸ“¹ Frame received: 53721 bytes from camera cam_003
[00:11:43.044] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:43.062] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:43.087] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:43.092] Server  LOG     ðŸ“¹ Frame received: 53252 bytes from camera cam_001
[00:11:43.102] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:43.496] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:43.540] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:43.549] Server  LOG     ðŸ“¹ Frame received: 52117 bytes from camera cam_001
[00:11:43.560] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:43.624] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:43.657] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:43.661] Server  LOG     ðŸ“¹ Frame received: 52394 bytes from camera cam_003
[00:11:43.675] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:43.733] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:43.763] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:43.772] Server  LOG     ðŸ“¹ Frame received: 37665 bytes from camera cam_002
[00:11:43.785] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:44.384] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.494
[00:11:44.501] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.2966
[00:11:44.536] Browser LOG     âœ… Real inference result: {"confidence":0.494,"detection_id":"6920ba7f546593c4ead4b447","panic_detected":false,"timestamp":"2025-11-21T19:16:15.589Z"}
[00:11:44.567] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:44.589] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:44.597] Server  LOG     ðŸ“¹ Frame received: 52293 bytes from camera cam_001
[00:11:44.608] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:44.645] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:44.672] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:44.688] Server  LOG     ðŸ“¹ Frame received: 61229 bytes from camera cam_002
[00:11:44.694] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:44.731] Browser LOG     âœ… Real inference result: {"confidence":0.2966,"detection_id":"6920ba7f546593c4ead4b445","panic_detected":false,"timestamp":"2025-11-21T19:16:15.595Z"}
[00:11:44.744] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.6935
[00:11:44.814] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:44.840] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:44.853] Server  LOG     ðŸ“¹ Frame received: 52827 bytes from camera cam_003
[00:11:44.868] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:44.893] Browser LOG     âœ… Real inference result: {"confidence":0.6935,"detection_id":"6920ba7f546593c4ead4b44d","panic_detected":false,"timestamp":"2025-11-21T19:16:15.892Z"}
[00:11:45.309] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.6921
[00:11:45.344] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:45.363] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:45.373] Server  LOG     ðŸ“¹ Frame received: 50091 bytes from camera cam_001
[00:11:45.376] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:45.419] Browser LOG     âœ… Real inference result: {"confidence":0.6921,"detection_id":"6920ba80546593c4ead4b451","panic_detected":false,"timestamp":"2025-11-21T19:16:16.531Z"}
[00:11:45.452] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4698
[00:11:45.515] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.3358
[00:11:45.587] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:45.643] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:45.661] Server  LOG     ðŸ“¹ Frame received: 63630 bytes from camera cam_002
[00:11:45.670] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:45.699] Browser LOG     âœ… Real inference result: {"confidence":0.4698,"detection_id":"6920ba80546593c4ead4b455","panic_detected":false,"timestamp":"2025-11-21T19:16:16.674Z"}
[00:11:45.699] Browser LOG     âœ… Real inference result: {"confidence":0.3358,"detection_id":"6920ba80546593c4ead4b457","panic_detected":false,"timestamp":"2025-11-21T19:16:16.703Z"}
[00:11:45.723] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:45.766] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:45.772] Server  LOG     ðŸ“¹ Frame received: 52389 bytes from camera cam_003
[00:11:45.781] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:46.248] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.6396
[00:11:46.291] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4419
[00:11:46.320] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:46.338] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:46.341] Server  LOG     ðŸ“¹ Frame received: 51105 bytes from camera cam_001
[00:11:46.349] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:46.382] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.3685
[00:11:46.479] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:46.521] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:46.528] Server  LOG     ðŸ“¹ Frame received: 63137 bytes from camera cam_002
[00:11:46.541] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:46.546] Browser LOG     âœ… Real inference result: {"confidence":0.6396,"detection_id":"6920ba81546593c4ead4b45d","panic_detected":false,"timestamp":"2025-11-21T19:16:17.471Z"}
[00:11:46.546] Browser LOG     âœ… Real inference result: {"confidence":0.4419,"detection_id":"6920ba81546593c4ead4b45f","panic_detected":false,"timestamp":"2025-11-21T19:16:17.510Z"}
[00:11:46.546] Browser LOG     âœ… Real inference result: {"confidence":0.3685,"detection_id":"6920ba81546593c4ead4b461","panic_detected":false,"timestamp":"2025-11-21T19:16:17.554Z"}
[00:11:46.568] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:46.618] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:46.632] Server  LOG     ðŸ“¹ Frame received: 53180 bytes from camera cam_003
[00:11:46.640] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:46.845] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.6483
[00:11:46.901] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:46.939] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:46.947] Server  LOG     ðŸ“¹ Frame received: 46722 bytes from camera cam_001
[00:11:46.952] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:46.979] Browser LOG     âœ… Real inference result: {"confidence":0.6483,"detection_id":"6920ba81546593c4ead4b469","panic_detected":false,"timestamp":"2025-11-21T19:16:18.067Z"}
[00:11:47.277] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.366
[00:11:47.311] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4277
[00:11:47.354] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:47.374] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:47.383] Server  LOG     ðŸ“¹ Frame received: 63711 bytes from camera cam_002
[00:11:47.388] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:47.439] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:47.457] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:47.465] Server  LOG     ðŸ“¹ Frame received: 51920 bytes from camera cam_003
[00:11:47.473] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:47.493] Browser LOG     âœ… Real inference result: {"confidence":0.366,"detection_id":"6920ba82546593c4ead4b46d","panic_detected":false,"timestamp":"2025-11-21T19:16:18.498Z"}
[00:11:47.493] Browser LOG     âœ… Real inference result: {"confidence":0.4277,"detection_id":"6920ba82546593c4ead4b46f","panic_detected":false,"timestamp":"2025-11-21T19:16:18.516Z"}
[00:11:47.792] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5986
[00:11:47.840] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:47.876] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:47.894] Server  LOG     ðŸ“¹ Frame received: 47228 bytes from camera cam_001
[00:11:47.910] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:47.951] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.3925
[00:11:47.987] Browser LOG     âœ… Real inference result: {"confidence":0.5986,"detection_id":"6920ba82546593c4ead4b475","panic_detected":false,"timestamp":"2025-11-21T19:16:19.014Z"}
[00:11:48.013] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:48.045] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:48.058] Server  LOG     ðŸ“¹ Frame received: 63220 bytes from camera cam_002
[00:11:48.060] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:48.091] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.3599
[00:11:48.126] Browser LOG     âœ… Real inference result: {"confidence":0.3925,"detection_id":"6920ba82546593c4ead4b477","panic_detected":false,"timestamp":"2025-11-21T19:16:19.096Z"}
[00:11:48.166] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:48.200] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:48.220] Server  LOG     ðŸ“¹ Frame received: 52722 bytes from camera cam_003
[00:11:48.229] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:48.253] Browser LOG     âœ… Real inference result: {"confidence":0.3599,"detection_id":"6920ba83546593c4ead4b47d","panic_detected":false,"timestamp":"2025-11-21T19:16:19.283Z"}
[00:11:48.394] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.6246
[00:11:48.467] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:48.577] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:48.613] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:11:48.618] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:48.640] Browser LOG     âœ… Real inference result: {"confidence":0.6246,"detection_id":"6920ba83546593c4ead4b481","panic_detected":false,"timestamp":"2025-11-21T19:16:19.616Z"}
[00:11:48.908] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.3697
[00:11:48.959] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.3838
[00:11:49.005] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:49.035] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:49.052] Server  LOG     ðŸ“¹ Frame received: 63325 bytes from camera cam_002
[00:11:49.058] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:49.111] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:49.152] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:49.194] Server  LOG     ðŸ“¹ Frame received: 49563 bytes from camera cam_003
[00:11:49.212] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:49.277] Browser LOG     âœ… Real inference result: {"confidence":0.3697,"detection_id":"6920ba83546593c4ead4b485","panic_detected":false,"timestamp":"2025-11-21T19:16:20.130Z"}
[00:11:49.277] Browser LOG     âœ… Real inference result: {"confidence":0.3838,"detection_id":"6920ba83546593c4ead4b487","panic_detected":false,"timestamp":"2025-11-21T19:16:20.135Z"}
[00:11:49.610] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.6248
[00:11:49.741] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:49.776] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:49.786] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:11:49.797] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:49.819] Browser LOG     âœ… Real inference result: {"confidence":0.6248,"detection_id":"6920ba84546593c4ead4b48d","panic_detected":false,"timestamp":"2025-11-21T19:16:20.825Z"}
[00:11:49.855] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.3544
[00:11:49.908] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:49.923] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:49.937] Server  LOG     ðŸ“¹ Frame received: 63056 bytes from camera cam_002
[00:11:49.945] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:49.976] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4117
[00:11:50.007] Browser LOG     âœ… Real inference result: {"confidence":0.3544,"detection_id":"6920ba84546593c4ead4b491","panic_detected":false,"timestamp":"2025-11-21T19:16:21.079Z"}
[00:11:50.045] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:50.072] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:50.077] Server  LOG     ðŸ“¹ Frame received: 53054 bytes from camera cam_003
[00:11:50.084] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:50.107] Browser LOG     âœ… Real inference result: {"confidence":0.4117,"detection_id":"6920ba85546593c4ead4b495","panic_detected":false,"timestamp":"2025-11-21T19:16:21.159Z"}
[00:11:50.361] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.6036
[00:11:50.459] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:50.491] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:50.498] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:11:50.501] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:50.520] Browser LOG     âœ… Real inference result: {"confidence":0.6036,"detection_id":"6920ba85546593c4ead4b499","panic_detected":false,"timestamp":"2025-11-21T19:16:21.582Z"}
[00:11:51.005] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4273
[00:11:51.053] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.393
[00:11:51.117] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:51.138] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:51.144] Server  LOG     ðŸ“¹ Frame received: 62407 bytes from camera cam_002
[00:11:51.145] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:51.175] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:51.209] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:51.220] Server  LOG     ðŸ“¹ Frame received: 50117 bytes from camera cam_003
[00:11:51.231] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:51.262] Browser LOG     âœ… Real inference result: {"confidence":0.4273,"detection_id":"6920ba86546593c4ead4b49f","panic_detected":false,"timestamp":"2025-11-21T19:16:22.226Z"}
[00:11:51.262] Browser LOG     âœ… Real inference result: {"confidence":0.393,"detection_id":"6920ba85546593c4ead4b49d","panic_detected":false,"timestamp":"2025-11-21T19:16:22.234Z"}
[00:11:51.296] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5723
[00:11:51.358] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:51.380] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:51.385] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:11:51.387] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:51.419] Browser LOG     âœ… Real inference result: {"confidence":0.5723,"detection_id":"6920ba86546593c4ead4b4a5","panic_detected":false,"timestamp":"2025-11-21T19:16:22.517Z"}
[00:11:51.516] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.3825
[00:11:51.573] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:51.603] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:51.619] Server  LOG     ðŸ“¹ Frame received: 57009 bytes from camera cam_002
[00:11:51.624] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:51.667] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.436
[00:11:51.742] Browser LOG     âœ… Real inference result: {"confidence":0.3825,"detection_id":"6920ba86546593c4ead4b4a9","panic_detected":false,"timestamp":"2025-11-21T19:16:22.738Z"}
[00:11:51.779] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:51.813] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:51.818] Server  LOG     ðŸ“¹ Frame received: 46598 bytes from camera cam_003
[00:11:51.827] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:51.840] Browser LOG     âœ… Real inference result: {"confidence":0.436,"detection_id":"6920ba86546593c4ead4b4ab","panic_detected":false,"timestamp":"2025-11-21T19:16:22.782Z"}
[00:11:51.940] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5417
[00:11:51.996] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:52.025] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:52.032] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:11:52.035] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:52.057] Browser LOG     âœ… Real inference result: {"confidence":0.5417,"detection_id":"6920ba87546593c4ead4b4b1","panic_detected":false,"timestamp":"2025-11-21T19:16:23.164Z"}
[00:11:52.606] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4109
[00:11:52.661] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:52.688] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:52.695] Server  LOG     ðŸ“¹ Frame received: 59038 bytes from camera cam_002
[00:11:52.700] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:52.723] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4531
[00:11:52.777] Browser LOG     âœ… Real inference result: {"confidence":0.4109,"detection_id":"6920ba87546593c4ead4b4b5","panic_detected":false,"timestamp":"2025-11-21T19:16:23.829Z"}
[00:11:52.811] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:52.843] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:52.858] Server  LOG     ðŸ“¹ Frame received: 48098 bytes from camera cam_003
[00:11:52.867] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:52.881] Browser LOG     âœ… Real inference result: {"confidence":0.4531,"detection_id":"6920ba87546593c4ead4b4b7","panic_detected":false,"timestamp":"2025-11-21T19:16:23.885Z"}
[00:11:52.890] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5575
[00:11:53.004] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:53.042] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:53.049] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:11:53.054] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:53.082] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.3926
[00:11:53.105] Browser LOG     âœ… Real inference result: {"confidence":0.5575,"detection_id":"6920ba87546593c4ead4b4bd","panic_detected":false,"timestamp":"2025-11-21T19:16:24.094Z"}
[00:11:53.149] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:53.168] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:53.172] Server  LOG     ðŸ“¹ Frame received: 59014 bytes from camera cam_002
[00:11:53.175] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:53.194] Browser LOG     âœ… Real inference result: {"confidence":0.3926,"detection_id":"6920ba88546593c4ead4b4c1","panic_detected":false,"timestamp":"2025-11-21T19:16:24.227Z"}
[00:11:53.324] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4359
[00:11:53.382] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:53.444] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:53.451] Server  LOG     ðŸ“¹ Frame received: 49498 bytes from camera cam_003
[00:11:53.458] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:53.480] Browser LOG     âœ… Real inference result: {"confidence":0.4359,"detection_id":"6920ba88546593c4ead4b4c5","panic_detected":false,"timestamp":"2025-11-21T19:16:24.546Z"}
[00:11:53.569] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5591
[00:11:53.682] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:53.714] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:53.744] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:11:53.764] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:53.779] Browser LOG     âœ… Real inference result: {"confidence":0.5591,"detection_id":"6920ba88546593c4ead4b4c9","panic_detected":false,"timestamp":"2025-11-21T19:16:24.763Z"}
[00:11:54.483] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4187
[00:11:54.527] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:54.549] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:54.553] Server  LOG     ðŸ“¹ Frame received: 61568 bytes from camera cam_002
[00:11:54.554] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:54.571] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.3831
[00:11:54.623] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:54.644] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:54.650] Server  LOG     ðŸ“¹ Frame received: 47177 bytes from camera cam_003
[00:11:54.653] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:54.678] Browser LOG     âœ… Real inference result: {"confidence":0.4187,"detection_id":"6920ba89546593c4ead4b4cd","panic_detected":false,"timestamp":"2025-11-21T19:16:25.704Z"}
[00:11:54.678] Browser LOG     âœ… Real inference result: {"confidence":0.3831,"detection_id":"6920ba89546593c4ead4b4cf","panic_detected":false,"timestamp":"2025-11-21T19:16:25.794Z"}
[00:11:54.696] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5403
[00:11:54.749] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.3902
[00:11:54.812] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:54.826] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:54.831] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:11:54.834] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:54.863] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:54.885] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:54.902] Server  LOG     ðŸ“¹ Frame received: 61171 bytes from camera cam_002
[00:11:54.915] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:54.959] Browser LOG     âœ… Real inference result: {"confidence":0.5403,"detection_id":"6920ba89546593c4ead4b4d3","panic_detected":false,"timestamp":"2025-11-21T19:16:25.918Z"}
[00:11:54.959] Browser LOG     âœ… Real inference result: {"confidence":0.3902,"detection_id":"6920ba89546593c4ead4b4d7","panic_detected":false,"timestamp":"2025-11-21T19:16:25.921Z"}
[00:11:54.966] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4436
[00:11:55.015] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:55.046] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:55.055] Server  LOG     ðŸ“¹ Frame received: 46598 bytes from camera cam_003
[00:11:55.064] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:55.078] Browser LOG     âœ… Real inference result: {"confidence":0.4436,"detection_id":"6920ba89546593c4ead4b4dd","panic_detected":false,"timestamp":"2025-11-21T19:16:26.116Z"}
[00:11:55.207] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5572
[00:11:55.283] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:55.310] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:55.317] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:11:55.321] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:55.341] Browser LOG     âœ… Real inference result: {"confidence":0.5572,"detection_id":"6920ba8a546593c4ead4b4e1","panic_detected":false,"timestamp":"2025-11-21T19:16:26.428Z"}
[00:11:56.025] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4073
[00:11:56.076] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:56.102] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:56.110] Server  LOG     ðŸ“¹ Frame received: 61110 bytes from camera cam_002
[00:11:56.118] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:56.132] Browser LOG     âœ… Real inference result: {"confidence":0.4073,"detection_id":"6920ba8b546593c4ead4b4e5","panic_detected":false,"timestamp":"2025-11-21T19:16:27.247Z"}
[00:11:56.208] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5477
[00:11:56.260] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:56.272] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:56.278] Server  LOG     ðŸ“¹ Frame received: 47758 bytes from camera cam_003
[00:11:56.286] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:56.307] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4278
[00:11:56.334] Browser LOG     âœ… Real inference result: {"confidence":0.5477,"detection_id":"6920ba8b546593c4ead4b4e9","panic_detected":false,"timestamp":"2025-11-21T19:16:27.433Z"}
[00:11:56.367] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:56.385] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:56.389] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:11:56.396] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:56.425] Browser LOG     âœ… Real inference result: {"confidence":0.4278,"detection_id":"6920ba8b546593c4ead4b4eb","panic_detected":false,"timestamp":"2025-11-21T19:16:27.489Z"}
[00:11:56.431] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4005
[00:11:56.472] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4467
[00:11:56.529] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:56.553] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:56.559] Server  LOG     ðŸ“¹ Frame received: 60829 bytes from camera cam_002
[00:11:56.561] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:56.588] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:56.611] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:56.625] Server  LOG     ðŸ“¹ Frame received: 56165 bytes from camera cam_003
[00:11:56.632] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:56.651] Browser LOG     âœ… Real inference result: {"confidence":0.4005,"detection_id":"6920ba8b546593c4ead4b4f1","panic_detected":false,"timestamp":"2025-11-21T19:16:27.575Z"}
[00:11:56.651] Browser LOG     âœ… Real inference result: {"confidence":0.4467,"detection_id":"6920ba8b546593c4ead4b4f3","panic_detected":false,"timestamp":"2025-11-21T19:16:27.644Z"}
[00:11:56.693] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5438
[00:11:56.748] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:56.768] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:56.784] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:11:56.791] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:56.814] Browser LOG     âœ… Real inference result: {"confidence":0.5438,"detection_id":"6920ba8b546593c4ead4b4f9","panic_detected":false,"timestamp":"2025-11-21T19:16:27.916Z"}
[00:11:57.610] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4042
[00:11:57.663] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:57.675] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:57.680] Server  LOG     ðŸ“¹ Frame received: 60367 bytes from camera cam_002
[00:11:57.681] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:57.702] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4587
[00:11:57.736] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5445
[00:11:57.764] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:57.779] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:57.785] Server  LOG     ðŸ“¹ Frame received: 52812 bytes from camera cam_003
[00:11:57.786] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:57.814] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:57.825] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:57.831] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:11:57.832] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:57.855] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4509
[00:11:57.876] Browser LOG     âœ… Real inference result: {"confidence":0.4042,"detection_id":"6920ba8c546593c4ead4b4fd","panic_detected":false,"timestamp":"2025-11-21T19:16:28.829Z"}
[00:11:57.876] Browser LOG     âœ… Real inference result: {"confidence":0.4587,"detection_id":"6920ba8c546593c4ead4b4ff","panic_detected":false,"timestamp":"2025-11-21T19:16:28.919Z"}
[00:11:57.876] Browser LOG     âœ… Real inference result: {"confidence":0.5445,"detection_id":"6920ba8c546593c4ead4b503","panic_detected":false,"timestamp":"2025-11-21T19:16:28.954Z"}
[00:11:57.897] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.3953
[00:11:58.036] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:58.073] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:58.089] Server  LOG     ðŸ“¹ Frame received: 60025 bytes from camera cam_002
[00:11:58.098] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:58.145] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5461
[00:11:58.215] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:58.258] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:58.264] Server  LOG     ðŸ“¹ Frame received: 54550 bytes from camera cam_003
[00:11:58.270] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:58.289] Browser LOG     âœ… Real inference result: {"confidence":0.4509,"detection_id":"6920ba8c546593c4ead4b507","panic_detected":false,"timestamp":"2025-11-21T19:16:29.044Z"}
[00:11:58.289] Browser LOG     âœ… Real inference result: {"confidence":0.3953,"detection_id":"6920ba8c546593c4ead4b50b","panic_detected":false,"timestamp":"2025-11-21T19:16:29.076Z"}
[00:11:58.289] Browser LOG     âœ… Real inference result: {"confidence":0.5461,"detection_id":"6920ba8d546593c4ead4b511","panic_detected":false,"timestamp":"2025-11-21T19:16:29.231Z"}
[00:11:58.370] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:58.445] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:58.452] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:11:58.458] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:59.354] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4355
[00:11:59.377] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4102
[00:11:59.437] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:59.463] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:59.466] Server  LOG     ðŸ“¹ Frame received: 60733 bytes from camera cam_002
[00:11:59.470] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:59.487] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:59.501] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:59.505] Server  LOG     ðŸ“¹ Frame received: 52603 bytes from camera cam_003
[00:11:59.506] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:59.517] Browser LOG     âœ… Real inference result: {"confidence":0.4355,"detection_id":"6920ba8e546593c4ead4b517","panic_detected":false,"timestamp":"2025-11-21T19:16:30.578Z"}
[00:11:59.517] Browser LOG     âœ… Real inference result: {"confidence":0.4102,"detection_id":"6920ba8e546593c4ead4b515","panic_detected":false,"timestamp":"2025-11-21T19:16:30.583Z"}
[00:11:59.521] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.3998
[00:11:59.548] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5354
[00:11:59.578] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4264
[00:11:59.616] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5255
[00:11:59.652] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:59.672] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:59.674] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:11:59.680] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:59.701] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:59.716] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:59.721] Server  LOG     ðŸ“¹ Frame received: 59719 bytes from camera cam_002
[00:11:59.729] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:59.772] Browser LOG     âœ… Real inference result: {"confidence":0.3998,"detection_id":"6920ba8e546593c4ead4b519","panic_detected":false,"timestamp":"2025-11-21T19:16:30.665Z"}
[00:11:59.772] Browser LOG     âœ… Real inference result: {"confidence":0.5354,"detection_id":"6920ba8e546593c4ead4b51b","panic_detected":false,"timestamp":"2025-11-21T19:16:30.688Z"}
[00:11:59.772] Browser LOG     âœ… Real inference result: {"confidence":0.4264,"detection_id":"6920ba8e546593c4ead4b521","panic_detected":false,"timestamp":"2025-11-21T19:16:30.769Z"}
[00:11:59.772] Browser LOG     âœ… Real inference result: {"confidence":0.5255,"detection_id":"6920ba8e546593c4ead4b527","panic_detected":false,"timestamp":"2025-11-21T19:16:30.829Z"}
[00:11:59.791] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:59.806] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:59.810] Server  LOG     ðŸ“¹ Frame received: 56228 bytes from camera cam_003
[00:11:59.813] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:11:59.847] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:11:59.876] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:11:59.903] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:11:59.929] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:00.937] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.3854
[00:12:00.961] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4384
[00:12:01.013] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:01.026] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:01.029] Server  LOG     ðŸ“¹ Frame received: 57231 bytes from camera cam_002
[00:12:01.034] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:01.062] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:01.072] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:01.075] Server  LOG     ðŸ“¹ Frame received: 51912 bytes from camera cam_003
[00:12:01.076] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:01.091] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5092
[00:12:01.117] Browser LOG     âœ… Real inference result: {"confidence":0.3854,"detection_id":"6920ba8f546593c4ead4b52d","panic_detected":false,"timestamp":"2025-11-21T19:16:32.158Z"}
[00:12:01.117] Browser LOG     âœ… Real inference result: {"confidence":0.4384,"detection_id":"6920ba8f546593c4ead4b52f","panic_detected":false,"timestamp":"2025-11-21T19:16:32.164Z"}
[00:12:01.120] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4024
[00:12:01.138] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5149
[00:12:01.192] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:01.201] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:01.203] Server  LOG     ðŸ“¹ Frame received: 55189 bytes from camera cam_003
[00:12:01.207] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:01.223] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:01.246] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:01.252] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:01.256] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:01.292] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:01.312] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:01.320] Server  LOG     ðŸ“¹ Frame received: 57801 bytes from camera cam_002
[00:12:01.326] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:01.368] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4369
[00:12:01.402] Browser LOG     âœ… Real inference result: {"confidence":0.5092,"detection_id":"6920ba90546593c4ead4b535","panic_detected":false,"timestamp":"2025-11-21T19:16:32.239Z"}
[00:12:01.402] Browser LOG     âœ… Real inference result: {"confidence":0.4024,"detection_id":"6920ba90546593c4ead4b531","panic_detected":false,"timestamp":"2025-11-21T19:16:32.306Z"}
[00:12:01.402] Browser LOG     âœ… Real inference result: {"confidence":0.5149,"detection_id":"6920ba90546593c4ead4b539","panic_detected":false,"timestamp":"2025-11-21T19:16:32.311Z"}
[00:12:01.464] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:01.494] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:01.508] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:01.518] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:01.554] Browser LOG     âœ… Real inference result: {"confidence":0.4369,"detection_id":"6920ba90546593c4ead4b541","panic_detected":false,"timestamp":"2025-11-21T19:16:32.415Z"}
[00:12:02.421] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.3899
[00:12:02.477] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4353
[00:12:02.548] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:02.570] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:02.575] Server  LOG     ðŸ“¹ Frame received: 56610 bytes from camera cam_002
[00:12:02.576] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:02.609] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:02.626] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:02.633] Server  LOG     ðŸ“¹ Frame received: 49283 bytes from camera cam_003
[00:12:02.640] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:02.666] Browser LOG     âœ… Real inference result: {"confidence":0.3899,"detection_id":"6920ba91546593c4ead4b545","panic_detected":false,"timestamp":"2025-11-21T19:16:33.641Z"}
[00:12:02.666] Browser LOG     âœ… Real inference result: {"confidence":0.4353,"detection_id":"6920ba91546593c4ead4b547","panic_detected":false,"timestamp":"2025-11-21T19:16:33.685Z"}
[00:12:02.713] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.437
[00:12:02.753] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:02.779] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:02.785] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:02.791] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:02.824] Browser LOG     âœ… Real inference result: {"confidence":0.437,"detection_id":"6920ba91546593c4ead4b54d","panic_detected":false,"timestamp":"2025-11-21T19:16:33.936Z"}
[00:12:03.839] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:03.869] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:03.896] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:03.922] Server  LOG     ðŸ“¹ Frame received: 56854 bytes from camera cam_002
[00:12:03.936] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:03.973] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:04.000] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba92546593c4ead4b551","panic_detected":false,"timestamp":"2025-11-21T19:16:35.061Z"}
[00:12:04.028] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:04.059] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:04.070] Server  LOG     ðŸ“¹ Frame received: 55005 bytes from camera cam_003
[00:12:04.077] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:04.096] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:04.118] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba92546593c4ead4b553","panic_detected":false,"timestamp":"2025-11-21T19:16:35.113Z"}
[00:12:04.136] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:04.167] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:04.173] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:04.180] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:04.206] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba93546593c4ead4b559","panic_detected":false,"timestamp":"2025-11-21T19:16:35.239Z"}
[00:12:05.171] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:05.221] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:05.238] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:05.241] Server  LOG     ðŸ“¹ Frame received: 57108 bytes from camera cam_002
[00:12:05.242] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:05.256] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:05.307] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:05.334] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:05.340] Server  LOG     ðŸ“¹ Frame received: 49921 bytes from camera cam_003
[00:12:05.343] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:05.364] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:05.390] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba94546593c4ead4b55d","panic_detected":false,"timestamp":"2025-11-21T19:16:36.393Z"}
[00:12:05.390] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba94546593c4ead4b55f","panic_detected":false,"timestamp":"2025-11-21T19:16:36.452Z"}
[00:12:05.450] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:05.472] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:05.484] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:05.488] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:05.499] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba94546593c4ead4b563","panic_detected":false,"timestamp":"2025-11-21T19:16:36.530Z"}
[00:12:06.362] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:06.453] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:06.475] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:06.486] Server  LOG     ðŸ“¹ Frame received: 56612 bytes from camera cam_002
[00:12:06.491] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:06.516] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:06.540] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba95546593c4ead4b569","panic_detected":false,"timestamp":"2025-11-21T19:16:37.586Z"}
[00:12:06.548] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:06.604] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:06.629] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:06.633] Server  LOG     ðŸ“¹ Frame received: 48870 bytes from camera cam_003
[00:12:06.644] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:06.685] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:06.715] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:06.729] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:06.737] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:06.758] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba95546593c4ead4b56b","panic_detected":false,"timestamp":"2025-11-21T19:16:37.657Z"}
[00:12:06.758] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba95546593c4ead4b56f","panic_detected":false,"timestamp":"2025-11-21T19:16:37.706Z"}
[00:12:07.562] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:07.624] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:07.646] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:07.654] Server  LOG     ðŸ“¹ Frame received: 54789 bytes from camera cam_002
[00:12:07.672] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:07.700] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:07.734] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:07.751] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba96546593c4ead4b575","panic_detected":false,"timestamp":"2025-11-21T19:16:38.786Z"}
[00:12:07.791] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:07.811] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:07.814] Server  LOG     ðŸ“¹ Frame received: 47889 bytes from camera cam_003
[00:12:07.819] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:07.849] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:07.874] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:07.883] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:07.894] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:07.952] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba96546593c4ead4b577","panic_detected":false,"timestamp":"2025-11-21T19:16:38.851Z"}
[00:12:07.952] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba96546593c4ead4b579","panic_detected":false,"timestamp":"2025-11-21T19:16:38.880Z"}
[00:12:08.943] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:08.978] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:09.014] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:09.058] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:09.083] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:09.088] Server  LOG     ðŸ“¹ Frame received: 54073 bytes from camera cam_002
[00:12:09.090] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:09.116] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:09.138] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:09.143] Server  LOG     ðŸ“¹ Frame received: 50587 bytes from camera cam_003
[00:12:09.154] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:09.195] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba98546593c4ead4b583","panic_detected":false,"timestamp":"2025-11-21T19:16:40.162Z"}
[00:12:09.195] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba97546593c4ead4b581","panic_detected":false,"timestamp":"2025-11-21T19:16:40.172Z"}
[00:12:09.195] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba98546593c4ead4b585","panic_detected":false,"timestamp":"2025-11-21T19:16:40.220Z"}
[00:12:09.208] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:09.224] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:09.235] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:09.250] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:10.208] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:10.248] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:10.265] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:10.272] Server  LOG     ðŸ“¹ Frame received: 52422 bytes from camera cam_002
[00:12:10.279] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:10.307] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:10.338] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:10.382] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba99546593c4ead4b58d","panic_detected":false,"timestamp":"2025-11-21T19:16:41.428Z"}
[00:12:10.415] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:10.439] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:10.445] Server  LOG     ðŸ“¹ Frame received: 47681 bytes from camera cam_003
[00:12:10.447] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:10.479] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:10.495] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:10.499] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:10.506] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:10.529] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba99546593c4ead4b591","panic_detected":false,"timestamp":"2025-11-21T19:16:41.476Z"}
[00:12:10.529] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba99546593c4ead4b58f","panic_detected":false,"timestamp":"2025-11-21T19:16:41.516Z"}
[00:12:11.527] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:11.570] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:11.620] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:11.638] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:11.651] Server  LOG     ðŸ“¹ Frame received: 50996 bytes from camera cam_002
[00:12:11.652] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:11.680] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:11.709] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:11.719] Server  LOG     ðŸ“¹ Frame received: 49190 bytes from camera cam_003
[00:12:11.724] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:11.754] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:11.777] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba9a546593c4ead4b59b","panic_detected":false,"timestamp":"2025-11-21T19:16:42.752Z"}
[00:12:11.777] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba9a546593c4ead4b599","panic_detected":false,"timestamp":"2025-11-21T19:16:42.761Z"}
[00:12:11.802] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:11.832] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:11.838] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:11.845] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:11.866] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba9a546593c4ead4b5a1","panic_detected":false,"timestamp":"2025-11-21T19:16:42.844Z"}
[00:12:12.769] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:12.812] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:12.879] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:12.900] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:12.918] Server  LOG     ðŸ“¹ Frame received: 51689 bytes from camera cam_002
[00:12:12.925] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:12.957] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:12.992] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:13.005] Server  LOG     ðŸ“¹ Frame received: 48437 bytes from camera cam_003
[00:12:13.014] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:13.053] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:13.090] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba9b546593c4ead4b5a5","panic_detected":false,"timestamp":"2025-11-21T19:16:43.989Z"}
[00:12:13.090] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba9b546593c4ead4b5a7","panic_detected":false,"timestamp":"2025-11-21T19:16:44.004Z"}
[00:12:13.120] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:13.140] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:13.146] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:13.153] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:13.170] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba9b546593c4ead4b5ad","panic_detected":false,"timestamp":"2025-11-21T19:16:44.091Z"}
[00:12:14.243] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:14.285] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:14.305] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:14.309] Server  LOG     ðŸ“¹ Frame received: 49693 bytes from camera cam_002
[00:12:14.310] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:14.353] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:14.367] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba9d546593c4ead4b5b1","panic_detected":false,"timestamp":"2025-11-21T19:16:45.467Z"}
[00:12:14.381] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:14.454] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:14.475] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:14.485] Server  LOG     ðŸ“¹ Frame received: 54435 bytes from camera cam_003
[00:12:14.488] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:14.513] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:14.533] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:14.539] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:14.550] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:14.589] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba9d546593c4ead4b5b7","panic_detected":false,"timestamp":"2025-11-21T19:16:45.574Z"}
[00:12:14.589] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba9d546593c4ead4b5b5","panic_detected":false,"timestamp":"2025-11-21T19:16:45.578Z"}
[00:12:15.378] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:15.428] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:15.447] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:15.457] Server  LOG     ðŸ“¹ Frame received: 48701 bytes from camera cam_002
[00:12:15.458] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:15.479] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:15.492] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba9e546593c4ead4b5bf","panic_detected":false,"timestamp":"2025-11-21T19:16:46.602Z"}
[00:12:15.535] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:15.579] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:15.581] Server  LOG     ðŸ“¹ Frame received: 52876 bytes from camera cam_003
[00:12:15.594] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:15.612] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:15.631] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba9e546593c4ead4b5bd","panic_detected":false,"timestamp":"2025-11-21T19:16:46.637Z"}
[00:12:15.687] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:15.713] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:15.719] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:15.723] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:15.750] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba9e546593c4ead4b5c5","panic_detected":false,"timestamp":"2025-11-21T19:16:46.741Z"}
[00:12:16.747] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:16.770] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:16.786] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:16.858] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:16.873] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:16.879] Server  LOG     ðŸ“¹ Frame received: 48139 bytes from camera cam_002
[00:12:16.882] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:16.922] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:16.968] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:16.976] Server  LOG     ðŸ“¹ Frame received: 52372 bytes from camera cam_003
[00:12:16.984] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:17.012] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:17.052] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:17.062] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:17.069] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:17.089] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba9f546593c4ead4b5c9","panic_detected":false,"timestamp":"2025-11-21T19:16:47.971Z"}
[00:12:17.089] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba9f546593c4ead4b5cd","panic_detected":false,"timestamp":"2025-11-21T19:16:47.975Z"}
[00:12:17.089] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920ba9f546593c4ead4b5cb","panic_detected":false,"timestamp":"2025-11-21T19:16:47.978Z"}
[00:12:17.782] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:17.827] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:17.865] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:17.883] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:17.890] Server  LOG     ðŸ“¹ Frame received: 50815 bytes from camera cam_002
[00:12:17.895] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:17.915] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:18.014] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baa0546593c4ead4b5d7","panic_detected":false,"timestamp":"2025-11-21T19:16:49.003Z"}
[00:12:18.014] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baa0546593c4ead4b5d5","panic_detected":false,"timestamp":"2025-11-21T19:16:49.044Z"}
[00:12:18.044] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:18.085] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:18.093] Server  LOG     ðŸ“¹ Frame received: 51542 bytes from camera cam_003
[00:12:18.098] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:18.129] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baa0546593c4ead4b5d9","panic_detected":false,"timestamp":"2025-11-21T19:16:49.093Z"}
[00:12:18.153] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:18.184] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:18.195] Server  LOG     ðŸ“¹ Frame received: 49192 bytes from camera cam_002
[00:12:18.208] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:19.541] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:19.563] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:19.577] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:19.581] Server  LOG     ðŸ“¹ Frame received: 54368 bytes from camera cam_003
[00:12:19.583] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:19.650] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:19.670] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:19.680] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:19.684] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:19.688] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:19.842] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:19.894] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:19.962] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:19.977] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:20.006] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:20.532] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:20.568] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:20.583] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:20.588] Server  LOG     ðŸ“¹ Frame received: 49258 bytes from camera cam_002
[00:12:20.589] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:20.654] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:20.687] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:20.702] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:20.713] Server  LOG     ðŸ“¹ Frame received: 51670 bytes from camera cam_003
[00:12:20.714] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:20.863] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:20.883] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:20.897] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:20.904] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:20.909] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:22.276] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:22.303] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:22.317] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:22.325] Server  LOG     ðŸ“¹ Frame received: 48374 bytes from camera cam_002
[00:12:22.330] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:22.349] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:22.393] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:22.411] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:22.429] Server  LOG     ðŸ“¹ Frame received: 53344 bytes from camera cam_003
[00:12:22.443] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:23.028] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:23.043] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:23.067] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:23.077] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:23.084] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:23.090] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:23.126] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:23.179] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:23.218] Server  LOG     ðŸ“¹ Frame received: 49890 bytes from camera cam_002
[00:12:23.250] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:23.577] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:23.597] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:23.609] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:23.615] Server  LOG     ðŸ“¹ Frame received: 48819 bytes from camera cam_003
[00:12:23.622] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:24.026] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:24.045] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:24.056] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:24.062] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:24.063] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:25.390] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:25.413] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:25.428] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:25.438] Server  LOG     ðŸ“¹ Frame received: 49924 bytes from camera cam_002
[00:12:25.441] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:25.782] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:25.806] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:25.820] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:25.841] Server  LOG     ðŸ“¹ Frame received: 52425 bytes from camera cam_003
[00:12:25.842] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:26.128] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:26.153] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:26.164] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:26.172] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:26.199] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:26.605] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:26.653] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:26.666] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:26.667] Server  LOG     ðŸ“¹ Frame received: 49233 bytes from camera cam_002
[00:12:26.673] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:26.830] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:26.868] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:26.895] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:26.910] Server  LOG     ðŸ“¹ Frame received: 49187 bytes from camera cam_003
[00:12:26.918] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:27.657] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:27.681] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:27.695] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:27.698] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:27.708] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:28.147] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:28.161] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:28.179] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:28.189] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:28.194] Server  LOG     ðŸ“¹ Frame received: 49621 bytes from camera cam_002
[00:12:28.195] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:28.237] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:28.246] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:28.261] Server  LOG     ðŸ“¹ Frame received: 46717 bytes from camera cam_003
[00:12:28.263] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:28.601] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:28.658] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:28.679] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:28.695] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:28.699] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:29.047] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:29.098] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:29.106] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:29.108] Server  LOG     ðŸ“¹ Frame received: 51796 bytes from camera cam_002
[00:12:29.154] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:29.186] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:29.254] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:29.277] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:29.299] Server  LOG     ðŸ“¹ Frame received: 47825 bytes from camera cam_003
[00:12:29.314] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:30.448] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:30.499] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:30.549] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:30.580] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:30.610] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:30.831] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:30.862] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:30.879] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:30.889] Server  LOG     ðŸ“¹ Frame received: 49277 bytes from camera cam_002
[00:12:30.896] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:31.448] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:31.478] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:31.493] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:31.497] Server  LOG     ðŸ“¹ Frame received: 47324 bytes from camera cam_003
[00:12:31.499] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:31.928] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:31.947] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:31.957] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:31.965] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:31.966] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:32.083] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:32.116] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:32.127] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:32.132] Server  LOG     ðŸ“¹ Frame received: 49264 bytes from camera cam_002
[00:12:32.133] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:32.573] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:32.607] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:32.623] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:32.631] Server  LOG     ðŸ“¹ Frame received: 48212 bytes from camera cam_003
[00:12:32.644] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:33.595] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:33.626] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:33.646] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:33.652] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:33.653] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:33.914] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:33.946] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:33.958] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:33.960] Server  LOG     ðŸ“¹ Frame received: 48980 bytes from camera cam_002
[00:12:33.966] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:34.039] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:34.062] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:34.073] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:34.076] Server  LOG     ðŸ“¹ Frame received: 47784 bytes from camera cam_003
[00:12:34.084] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:34.168] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:34.187] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:34.202] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:34.208] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:34.213] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:34.463] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:34.487] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:34.502] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:34.505] Server  LOG     ðŸ“¹ Frame received: 49033 bytes from camera cam_002
[00:12:34.511] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:35.070] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:35.098] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:35.124] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:35.131] Server  LOG     ðŸ“¹ Frame received: 48810 bytes from camera cam_003
[00:12:35.154] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:35.755] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:35.772] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:35.795] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:35.805] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:35.827] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:36.532] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:36.562] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:36.575] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:36.582] Server  LOG     ðŸ“¹ Frame received: 48681 bytes from camera cam_002
[00:12:36.588] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:36.736] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:36.762] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:36.790] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:36.796] Server  LOG     ðŸ“¹ Frame received: 54211 bytes from camera cam_003
[00:12:36.798] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:37.014] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:37.048] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:37.077] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:37.103] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:37.104] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:37.284] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:37.320] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:37.335] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:37.340] Server  LOG     ðŸ“¹ Frame received: 49066 bytes from camera cam_002
[00:12:37.347] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:37.814] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:37.858] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:37.871] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:37.879] Server  LOG     ðŸ“¹ Frame received: 53485 bytes from camera cam_003
[00:12:37.887] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:38.756] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:38.803] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:38.817] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:38.831] Server  LOG     ðŸ“¹ Frame received: 52209 bytes from camera cam_003
[00:12:38.839] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:39.281] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:39.306] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:39.320] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:39.323] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:39.325] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:39.487] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:39.555] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:39.701] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:39.783] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:39.805] Server  LOG     ðŸ“¹ Frame received: 49575 bytes from camera cam_002
[00:12:39.855] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:39.949] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:39.987] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:40.001] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:40.020] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:40.672] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:40.700] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:40.714] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:40.716] Server  LOG     ðŸ“¹ Frame received: 50215 bytes from camera cam_002
[00:12:40.729] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:41.365] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:41.404] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:41.424] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:41.429] Server  LOG     ðŸ“¹ Frame received: 51968 bytes from camera cam_003
[00:12:41.436] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:43.507] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:43.552] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:43.565] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:43.569] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:43.571] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:43.585] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:43.612] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:43.627] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:43.631] Server  LOG     ðŸ“¹ Frame received: 50563 bytes from camera cam_002
[00:12:43.633] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:43.660] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:43.686] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:43.705] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:43.712] Server  LOG     ðŸ“¹ Frame received: 54633 bytes from camera cam_003
[00:12:43.723] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:44.027] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:44.055] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:44.091] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:44.094] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:44.094] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:44.132] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:44.167] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:44.208] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:44.222] Server  LOG     ðŸ“¹ Frame received: 50507 bytes from camera cam_002
[00:12:44.230] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:44.497] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:44.519] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:44.532] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:44.540] Server  LOG     ðŸ“¹ Frame received: 52747 bytes from camera cam_003
[00:12:44.557] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:45.591] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:45.660] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:45.678] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:45.693] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:45.694] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:45.861] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:45.909] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:45.923] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:45.931] Server  LOG     ðŸ“¹ Frame received: 50835 bytes from camera cam_002
[00:12:45.934] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:45.992] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:46.069] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:46.115] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:46.160] Server  LOG     ðŸ“¹ Frame received: 53008 bytes from camera cam_003
[00:12:46.180] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:46.755] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5
[00:12:46.797] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:46.810] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:46.814] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:46.817] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:47.250] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:47.275] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.287] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.345] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.361] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.388] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.397] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.402] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.416] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.422] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.459] Server  ERROR   TypeError: fetch failed
[00:12:47.477] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:47.480] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.490] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.492] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.498] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.500] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.505] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.507] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.515] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.529] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.535] Server  ERROR   TypeError: fetch failed
[00:12:47.561] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:47.563] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.568] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.570] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.571] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.576] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.578] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.583] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.585] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.589] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.598] Server  ERROR   TypeError: fetch failed
[00:12:47.619] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:47.626] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.632] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.638] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.640] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.646] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.650] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.653] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.654] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.659] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.666] Server  ERROR   TypeError: fetch failed
[00:12:47.680] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:47.688] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.698] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.701] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.707] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.713] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.716] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.719] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.723] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.730] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.737] Server  ERROR   TypeError: fetch failed
[00:12:47.771] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:47.779] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.786] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.792] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.803] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.806] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.811] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.825] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.846] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.855] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:47.882] Server  ERROR   TypeError: fetch failed
[00:12:47.950] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:47.970] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:47.973] Server  LOG     ðŸ“¹ Frame received: 52166 bytes from camera cam_002
[00:12:47.978] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:47.997] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:48.020] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:48.022] Server  LOG     ðŸ“¹ Frame received: 49874 bytes from camera cam_003
[00:12:48.034] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:48.077] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:48.091] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:48.096] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:48.097] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:48.130] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:48.146] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:48.150] Server  LOG     ðŸ“¹ Frame received: 51729 bytes from camera cam_002
[00:12:48.151] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:48.164] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:48.172] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:48.175] Server  LOG     ðŸ“¹ Frame received: 52066 bytes from camera cam_003
[00:12:48.178] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:48.187] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:48.202] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:48.206] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:48.208] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:48.222] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:48.228] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.235] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.236] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.240] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.242] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.248] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.251] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.255] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.258] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.269] Server  ERROR   TypeError: fetch failed
[00:12:48.278] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:48.281] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.284] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.285] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.290] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.292] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.296] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.297] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.299] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.300] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.303] Server  ERROR   TypeError: fetch failed
[00:12:48.332] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:48.334] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.339] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.341] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.345] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.348] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.351] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.353] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.354] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.355] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.362] Server  ERROR   TypeError: fetch failed
[00:12:48.374] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:48.381] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.389] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.390] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.395] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.396] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.401] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.403] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.405] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.409] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.415] Server  ERROR   TypeError: fetch failed
[00:12:48.424] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:48.429] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.431] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.435] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.437] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.438] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.443] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.445] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.449] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.451] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.458] Server  ERROR   TypeError: fetch failed
[00:12:48.470] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:48.472] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.477] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.478] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.480] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.484] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.485] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.487] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.489] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.492] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.494] Server  ERROR   TypeError: fetch failed
[00:12:48.556] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:48.570] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:48.576] Server  LOG     ðŸ“¹ Frame received: 52001 bytes from camera cam_002
[00:12:48.581] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:48.595] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:48.611] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:48.617] Server  LOG     ðŸ“¹ Frame received: 50118 bytes from camera cam_003
[00:12:48.619] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:48.658] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:48.671] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:48.674] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:48.679] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:48.692] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:48.709] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:48.714] Server  LOG     ðŸ“¹ Frame received: 52121 bytes from camera cam_002
[00:12:48.721] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:48.735] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:48.750] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:48.754] Server  LOG     ðŸ“¹ Frame received: 47086 bytes from camera cam_003
[00:12:48.760] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:48.775] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:48.792] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:48.797] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:48.811] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:48.868] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:48.881] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.932] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.936] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.937] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.943] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.946] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.950] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.952] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.965] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:48.980] Server  ERROR   TypeError: fetch failed
[00:12:49.015] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:49.020] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.028] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.030] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.035] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.040] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.043] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.049] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.053] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.057] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.071] Server  ERROR   TypeError: fetch failed
[00:12:49.101] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:49.105] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.110] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.112] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.118] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.119] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.126] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.132] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.138] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.140] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.147] Server  ERROR   TypeError: fetch failed
[00:12:49.166] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:49.173] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.181] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.188] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.198] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.202] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.205] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.212] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.228] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.230] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.235] Server  ERROR   TypeError: fetch failed
[00:12:49.250] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:49.257] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.264] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.277] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.279] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.285] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.289] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.299] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.330] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.333] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.341] Server  ERROR   TypeError: fetch failed
[00:12:49.354] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:49.362] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.374] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.376] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.381] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.387] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.389] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.394] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.396] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.407] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.414] Server  ERROR   TypeError: fetch failed
[00:12:49.463] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:49.485] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:49.487] Server  LOG     ðŸ“¹ Frame received: 48388 bytes from camera cam_003
[00:12:49.489] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:49.503] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:49.516] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:49.519] Server  LOG     ðŸ“¹ Frame received: 51764 bytes from camera cam_002
[00:12:49.520] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:49.540] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:49.555] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:49.561] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:49.562] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:49.578] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:49.588] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:49.591] Server  LOG     ðŸ“¹ Frame received: 51206 bytes from camera cam_002
[00:12:49.594] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:49.609] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:49.625] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:49.632] Server  LOG     ðŸ“¹ Frame received: 47368 bytes from camera cam_003
[00:12:49.636] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:49.652] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:49.666] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:49.668] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:49.671] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:49.685] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:49.691] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.699] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.702] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.705] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.707] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.709] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.712] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.714] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.716] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.720] Server  ERROR   TypeError: fetch failed
[00:12:49.730] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:49.739] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.742] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.744] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.748] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.750] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.752] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.755] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.757] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.760] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.764] Server  ERROR   TypeError: fetch failed
[00:12:49.780] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:49.784] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.789] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.791] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.793] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.796] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.798] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.799] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.803] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.811] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.814] Server  ERROR   TypeError: fetch failed
[00:12:49.826] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:49.831] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.834] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.838] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.840] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.841] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.845] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.846] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.848] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.852] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.858] Server  ERROR   TypeError: fetch failed
[00:12:49.865] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:49.869] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.874] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.876] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.879] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.882] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.883] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.885] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.888] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.889] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.892] Server  ERROR   TypeError: fetch failed
[00:12:49.900] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:49.903] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.907] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.909] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.910] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.917] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.924] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.936] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.938] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.944] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:49.952] Server  ERROR   TypeError: fetch failed
[00:12:49.994] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:50.004] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:50.007] Server  LOG     ðŸ“¹ Frame received: 50921 bytes from camera cam_002
[00:12:50.007] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:50.021] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:50.047] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:50.050] Server  LOG     ðŸ“¹ Frame received: 49286 bytes from camera cam_003
[00:12:50.055] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:50.076] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:50.091] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:50.097] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:50.098] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:50.111] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:50.127] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:50.132] Server  LOG     ðŸ“¹ Frame received: 49585 bytes from camera cam_002
[00:12:50.136] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:50.151] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:50.166] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:50.171] Server  LOG     ðŸ“¹ Frame received: 47877 bytes from camera cam_003
[00:12:50.173] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:50.197] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:50.210] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:50.216] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:50.220] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:50.239] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:50.242] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.249] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.250] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.252] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.257] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.262] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.268] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.270] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.272] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.279] Server  ERROR   TypeError: fetch failed
[00:12:50.291] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:50.297] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.299] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.302] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.305] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.310] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.312] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.407] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.445] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.493] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.533] Server  ERROR   TypeError: fetch failed
[00:12:50.548] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:50.553] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.556] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.560] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.561] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.563] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.565] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.566] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.568] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.569] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.610] Server  ERROR   TypeError: fetch failed
[00:12:50.624] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:50.629] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.632] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.637] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.639] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.642] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.644] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.646] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.653] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.657] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.661] Server  ERROR   TypeError: fetch failed
[00:12:50.689] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:50.696] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.699] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.701] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.707] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.709] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.716] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.722] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.725] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.727] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.733] Server  ERROR   TypeError: fetch failed
[00:12:50.749] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:50.752] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.756] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.763] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.767] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.770] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.774] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.776] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.779] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.785] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:50.796] Server  ERROR   TypeError: fetch failed
[00:12:50.834] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:50.854] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:50.858] Server  LOG     ðŸ“¹ Frame received: 49419 bytes from camera cam_002
[00:12:50.859] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:50.871] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:50.883] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:50.889] Server  LOG     ðŸ“¹ Frame received: 49089 bytes from camera cam_003
[00:12:50.893] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:50.917] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:50.933] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:50.935] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:50.936] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:50.948] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:50.959] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:50.964] Server  LOG     ðŸ“¹ Frame received: 48532 bytes from camera cam_002
[00:12:50.966] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:50.992] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:51.006] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:51.008] Server  LOG     ðŸ“¹ Frame received: 53162 bytes from camera cam_003
[00:12:51.011] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:51.021] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:51.034] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:51.040] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:51.041] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:51.057] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:51.063] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.068] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.070] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.074] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.077] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.081] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.083] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.087] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.090] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.097] Server  ERROR   TypeError: fetch failed
[00:12:51.110] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:51.115] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.119] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.123] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.126] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.129] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.132] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.136] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.139] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.143] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.146] Server  ERROR   TypeError: fetch failed
[00:12:51.157] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:51.160] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.163] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.165] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.167] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.171] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.173] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.176] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.180] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.183] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.187] Server  ERROR   TypeError: fetch failed
[00:12:51.198] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:51.201] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.202] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.205] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.208] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.209] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.211] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.214] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.215] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.216] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.222] Server  ERROR   TypeError: fetch failed
[00:12:51.238] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:51.242] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.244] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.246] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.250] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.251] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.255] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.258] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.261] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.264] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.270] Server  ERROR   TypeError: fetch failed
[00:12:51.278] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:51.282] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.284] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.286] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.289] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.292] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.293] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.299] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.303] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.305] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.311] Server  ERROR   TypeError: fetch failed
[00:12:51.341] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:51.358] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:51.360] Server  LOG     ðŸ“¹ Frame received: 48723 bytes from camera cam_002
[00:12:51.361] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:51.382] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:51.403] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:51.408] Server  LOG     ðŸ“¹ Frame received: 53354 bytes from camera cam_003
[00:12:51.410] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:51.429] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:51.441] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:51.444] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:51.451] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:51.470] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:51.485] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:51.487] Server  LOG     ðŸ“¹ Frame received: 48955 bytes from camera cam_002
[00:12:51.490] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:51.512] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:51.525] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:51.529] Server  LOG     ðŸ“¹ Frame received: 51835 bytes from camera cam_003
[00:12:51.533] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:51.548] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:51.561] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:51.562] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:51.563] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:51.581] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:51.585] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.591] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.598] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.603] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.605] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.616] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.618] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.619] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.625] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.631] Server  ERROR   TypeError: fetch failed
[00:12:51.644] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:51.651] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.653] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.658] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.661] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.665] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.666] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.671] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.673] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.674] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.680] Server  ERROR   TypeError: fetch failed
[00:12:51.694] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:51.701] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.708] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.709] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.714] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.720] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.722] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.729] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.730] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.735] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.740] Server  ERROR   TypeError: fetch failed
[00:12:51.755] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:51.758] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.763] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.765] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.769] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.771] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.776] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.778] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.783] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.784] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.790] Server  ERROR   TypeError: fetch failed
[00:12:51.820] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:51.824] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.826] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.829] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.834] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.836] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.840] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.851] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.854] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.861] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.865] Server  ERROR   TypeError: fetch failed
[00:12:51.876] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:51.880] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.883] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.885] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.888] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.890] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.892] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.895] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.900] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.908] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:51.911] Server  ERROR   TypeError: fetch failed
[00:12:51.951] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:51.961] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:51.971] Server  LOG     ðŸ“¹ Frame received: 50622 bytes from camera cam_002
[00:12:51.973] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:51.987] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:52.000] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:52.004] Server  LOG     ðŸ“¹ Frame received: 52238 bytes from camera cam_003
[00:12:52.006] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:52.020] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:52.032] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:52.034] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:52.037] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:52.049] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:52.064] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:52.068] Server  LOG     ðŸ“¹ Frame received: 48528 bytes from camera cam_002
[00:12:52.069] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:52.088] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:52.101] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:52.103] Server  LOG     ðŸ“¹ Frame received: 53688 bytes from camera cam_003
[00:12:52.104] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:52.119] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:52.131] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:52.133] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:52.138] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:52.151] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:52.153] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.157] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.158] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.160] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.165] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.168] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.172] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.178] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.186] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.188] Server  ERROR   TypeError: fetch failed
[00:12:52.199] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:52.202] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.206] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.208] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.211] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.214] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.216] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.217] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.221] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.223] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.228] Server  ERROR   TypeError: fetch failed
[00:12:52.237] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:52.241] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.244] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.247] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.249] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.250] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.251] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.254] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.256] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.258] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.264] Server  ERROR   TypeError: fetch failed
[00:12:52.278] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:52.282] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.285] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.286] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.290] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.292] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.293] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.295] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.297] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.299] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.302] Server  ERROR   TypeError: fetch failed
[00:12:52.309] Server  ERROR   âŒ [API] Error: fetch failed
[00:12:52.312] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\[root-of-the-server]__593c20e3._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.316] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\compiled\next-server\app-route-turbo.runtime.dev.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.317] Server  ERROR   D:\indra_netra\frontend\.next\dev\server\chunks\9e883_next_676e83c6._.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.319] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\base-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.320] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\next-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.321] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\dev\next-dev-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.323] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\trace\trace.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.325] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\router-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.326] Server  ERROR   D:\indra_netra\frontend\node_modules\next\dist\server\lib\start-server.js: Invalid source map. Only conformant source maps can be used to find the original code. Cause: Error: sourceMapURL could not be parsed
[00:12:52.330] Server  ERROR   TypeError: fetch failed
[00:12:52.374] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:52.382] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:52.387] Server  LOG     ðŸ“¹ Frame received: 48557 bytes from camera cam_002
[00:12:52.388] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:52.400] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:52.421] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:52.437] Server  LOG     ðŸ“¹ Frame received: 52914 bytes from camera cam_003
[00:12:52.443] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:52.459] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:52.493] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:52.499] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:52.501] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:52.529] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:52.541] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:52.547] Server  LOG     ðŸ“¹ Frame received: 47665 bytes from camera cam_002
[00:12:52.548] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:52.566] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:52.582] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:52.587] Server  LOG     ðŸ“¹ Frame received: 52104 bytes from camera cam_003
[00:12:52.594] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:54.532] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:12:54.561] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.0966
[00:12:54.584] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:54.593] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:54.597] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:12:54.604] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:54.616] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.6666
[00:12:54.636] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.3814
[00:12:54.694] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:54.804] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:54.825] Server  LOG     ðŸ“¹ Frame received: 48518 bytes from camera cam_002
[00:12:54.873] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:54.945] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:12:55.025] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:12:55.082] Server  LOG     ðŸ“¹ Frame received: 53215 bytes from camera cam_003
[00:12:55.096] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:12:55.205] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:12:55.359] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.2478
[00:12:56.443] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4123
[00:12:56.459] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0
[00:12:56.673] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4396
[00:13:02.781] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:02.792] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:02.796] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:13:02.797] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:02.888] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:02.902] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:02.915] Server  LOG     ðŸ“¹ Frame received: 48569 bytes from camera cam_002
[00:13:02.916] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:03.393] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:03.403] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:03.408] Server  LOG     ðŸ“¹ Frame received: 50010 bytes from camera cam_003
[00:13:03.437] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:04.018] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.6922
[00:13:04.105] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.3737
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2680273724348448
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4438988650869063
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3977946906774128
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.11291580360598402
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.13588453093339792
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.03644247014758367
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1882623860885646
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8483323328737621
[00:13:04.600] Browser LOG     Panic detected at: Parking Area Confidence: 0.8483323328737621
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4459162553576038
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7953300747040213
[00:13:04.600] Browser LOG     Panic detected at: Main Hall Confidence: 0.7953300747040213
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3095084712289242
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.24768642053078865
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.02314667484511984
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.31972155131719976
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.23432036901518727
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2934614347764341
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1797229876586548
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.028876077931402344
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9852960308337821
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4454778585397907
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.24589635297933887
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.20483978631927158
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3038328235457649
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3963910295322984
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8138902168984359
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3314395785752927
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.022597840538511216
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7766934452942837
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.010694415683128622
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.600] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7168237423826562
[00:13:04.600] Browser LOG     Panic detected at: Gate 1 Entrance Confidence: 0.7168237423826562
[00:13:04.600] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.601] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8461376409423196
[00:13:04.601] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.601] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.31739635497857593
[00:13:04.601] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.601] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.35137466987725086
[00:13:04.601] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.601] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1956740377526256
[00:13:04.601] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.601] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.33180051937790395
[00:13:04.601] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.601] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.14298698339721722
[00:13:04.601] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.601] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1927078507462358
[00:13:04.601] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.601] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.20755611179227812
[00:13:04.601] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.601] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.18092943257202726
[00:13:04.601] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.601] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7246145031254734
[00:13:04.601] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.601] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8055208227566765
[00:13:04.601] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.601] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.823734739742951
[00:13:04.601] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.601] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4898386069159658
[00:13:04.601] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.601] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.09080952155973887
[00:13:04.601] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.601] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4298958063468176
[00:13:04.601] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.601] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7935033071066401
[00:13:04.601] Browser WARN    âš ï¸ Inference API failed, status: 500 using simulation
[00:13:04.601] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.18407691721935238
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baa2546593c4ead4b5e1","panic_detected":false,"timestamp":"2025-11-21T19:16:50.763Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baa2546593c4ead4b5e3","panic_detected":false,"timestamp":"2025-11-21T19:16:50.872Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baa2546593c4ead4b5e9","panic_detected":false,"timestamp":"2025-11-21T19:16:51.053Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baa3546593c4ead4b5ed","panic_detected":false,"timestamp":"2025-11-21T19:16:51.755Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baa3546593c4ead4b5f1","panic_detected":false,"timestamp":"2025-11-21T19:16:51.876Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baa3546593c4ead4b5f5","panic_detected":false,"timestamp":"2025-11-21T19:16:52.073Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baa5546593c4ead4b5f9","panic_detected":false,"timestamp":"2025-11-21T19:16:53.498Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baa5546593c4ead4b5fb","panic_detected":false,"timestamp":"2025-11-21T19:16:53.546Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baa6546593c4ead4b601","panic_detected":false,"timestamp":"2025-11-21T19:16:54.241Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baa6546593c4ead4b603","panic_detected":false,"timestamp":"2025-11-21T19:16:54.255Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baa6546593c4ead4b609","panic_detected":false,"timestamp":"2025-11-21T19:16:54.798Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baa7546593c4ead4b60d","panic_detected":false,"timestamp":"2025-11-21T19:16:55.246Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baa8546593c4ead4b611","panic_detected":false,"timestamp":"2025-11-21T19:16:56.608Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baa8546593c4ead4b615","panic_detected":false,"timestamp":"2025-11-21T19:16:56.992Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baa9546593c4ead4b619","panic_detected":false,"timestamp":"2025-11-21T19:16:57.339Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baa9546593c4ead4b61d","panic_detected":false,"timestamp":"2025-11-21T19:16:57.817Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baa9546593c4ead4b621","panic_detected":false,"timestamp":"2025-11-21T19:16:58.023Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baaa546593c4ead4b625","panic_detected":false,"timestamp":"2025-11-21T19:16:58.865Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baab546593c4ead4b629","panic_detected":false,"timestamp":"2025-11-21T19:16:59.358Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baab546593c4ead4b62b","panic_detected":false,"timestamp":"2025-11-21T19:16:59.377Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baab546593c4ead4b631","panic_detected":false,"timestamp":"2025-11-21T19:16:59.817Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baac546593c4ead4b635","panic_detected":false,"timestamp":"2025-11-21T19:17:00.257Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baac546593c4ead4b637","panic_detected":false,"timestamp":"2025-11-21T19:17:00.336Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baad546593c4ead4b63d","panic_detected":false,"timestamp":"2025-11-21T19:17:01.665Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baad546593c4ead4b641","panic_detected":false,"timestamp":"2025-11-21T19:17:02.044Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baae546593c4ead4b645","panic_detected":false,"timestamp":"2025-11-21T19:17:02.662Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baae546593c4ead4b649","panic_detected":false,"timestamp":"2025-11-21T19:17:03.149Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baaf546593c4ead4b64d","panic_detected":false,"timestamp":"2025-11-21T19:17:03.294Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baaf546593c4ead4b651","panic_detected":false,"timestamp":"2025-11-21T19:17:03.789Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920bab0546593c4ead4b655","panic_detected":false,"timestamp":"2025-11-21T19:17:04.814Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920bab0546593c4ead4b659","panic_detected":false,"timestamp":"2025-11-21T19:17:05.131Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920bab1546593c4ead4b65d","panic_detected":false,"timestamp":"2025-11-21T19:17:05.248Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920bab1546593c4ead4b661","panic_detected":false,"timestamp":"2025-11-21T19:17:05.384Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920bab1546593c4ead4b665","panic_detected":false,"timestamp":"2025-11-21T19:17:05.672Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920bab2546593c4ead4b669","panic_detected":false,"timestamp":"2025-11-21T19:17:06.290Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920bab2546593c4ead4b66d","panic_detected":false,"timestamp":"2025-11-21T19:17:06.974Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920bab3546593c4ead4b671","panic_detected":false,"timestamp":"2025-11-21T19:17:07.746Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920bab3546593c4ead4b675","panic_detected":false,"timestamp":"2025-11-21T19:17:07.955Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920bab4546593c4ead4b679","panic_detected":false,"timestamp":"2025-11-21T19:17:08.232Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920bab4546593c4ead4b67d","panic_detected":false,"timestamp":"2025-11-21T19:17:08.491Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920bab4546593c4ead4b681","panic_detected":false,"timestamp":"2025-11-21T19:17:09.011Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920bab5546593c4ead4b685","panic_detected":false,"timestamp":"2025-11-21T19:17:09.970Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920bab6546593c4ead4b689","panic_detected":false,"timestamp":"2025-11-21T19:17:10.497Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920bab6546593c4ead4b68d","panic_detected":false,"timestamp":"2025-11-21T19:17:10.680Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920bab6546593c4ead4b68f","panic_detected":false,"timestamp":"2025-11-21T19:17:10.719Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920bab7546593c4ead4b695","panic_detected":false,"timestamp":"2025-11-21T19:17:11.890Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920bab8546593c4ead4b699","panic_detected":false,"timestamp":"2025-11-21T19:17:12.560Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baba546593c4ead4b69d","panic_detected":false,"timestamp":"2025-11-21T19:17:14.731Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baba546593c4ead4b69f","panic_detected":false,"timestamp":"2025-11-21T19:17:14.806Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920baba546593c4ead4b6a3","panic_detected":false,"timestamp":"2025-11-21T19:17:14.872Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920babb546593c4ead4b6a9","panic_detected":false,"timestamp":"2025-11-21T19:17:15.248Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920babb546593c4ead4b6ad","panic_detected":false,"timestamp":"2025-11-21T19:17:15.352Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920babb546593c4ead4b6b1","panic_detected":false,"timestamp":"2025-11-21T19:17:15.718Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920babc546593c4ead4b6b5","panic_detected":false,"timestamp":"2025-11-21T19:17:16.808Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920babc546593c4ead4b6b9","panic_detected":false,"timestamp":"2025-11-21T19:17:17.072Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920babc546593c4ead4b6bb","panic_detected":false,"timestamp":"2025-11-21T19:17:17.192Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.5,"detection_id":"6920babd546593c4ead4b6c1","panic_detected":false,"timestamp":"2025-11-21T19:17:17.974Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:17:25.721Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.0966,"detection_id":"6920bac515119379e5f21d6e","panic_detected":false,"timestamp":"2025-11-21T19:17:25.779Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.6666,"detection_id":"6920bac515119379e5f21d70","panic_detected":false,"timestamp":"2025-11-21T19:17:25.787Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.3814,"detection_id":"6920bac515119379e5f21d74","panic_detected":false,"timestamp":"2025-11-21T19:17:25.798Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:17:26.003Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.2478,"detection_id":"6920bac515119379e5f21d72","panic_detected":false,"timestamp":"2025-11-21T19:17:26.240Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.4123,"detection_id":"6920bac715119379e5f21d85","panic_detected":false,"timestamp":"2025-11-21T19:17:27.663Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0,"error":"Processing error - fallback mode","message":"Detection processed in fallback mode","panic_detected":false,"timestamp":"2025-11-21T19:17:27.669Z"}
[00:13:04.601] Browser LOG     âœ… Real inference result: {"confidence":0.4396,"detection_id":"6920bac715119379e5f21d8d","panic_detected":false,"timestamp":"2025-11-21T19:17:27.893Z"}
[00:13:04.673] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.3308
[00:13:05.101] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:05.113] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:05.116] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:13:05.117] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:05.150] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:05.160] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:05.171] Server  LOG     ðŸ“¹ Frame received: 48569 bytes from camera cam_002
[00:13:05.172] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:05.251] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:05.261] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:05.264] Server  LOG     ðŸ“¹ Frame received: 50010 bytes from camera cam_003
[00:13:05.265] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:05.971] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:05.980] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:05.984] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:13:05.986] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:05.990] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5578
[00:13:05.998] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.3655
[00:13:06.031] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:06.040] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:06.042] Server  LOG     ðŸ“¹ Frame received: 48569 bytes from camera cam_002
[00:13:06.043] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:06.158] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.3468
[00:13:06.911] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.3114
[00:13:06.924] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5625
[00:13:07.290] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:07.304] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:07.307] Server  LOG     ðŸ“¹ Frame received: 50010 bytes from camera cam_003
[00:13:07.310] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:08.125] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:08.134] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:08.136] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:13:08.138] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:08.146] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4493
[00:13:08.208] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:08.216] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:08.219] Server  LOG     ðŸ“¹ Frame received: 48569 bytes from camera cam_002
[00:13:08.222] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:08.300] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:08.308] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:08.310] Server  LOG     ðŸ“¹ Frame received: 50010 bytes from camera cam_003
[00:13:08.311] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:08.830] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5995
[00:13:08.958] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.2794
[00:13:09.063] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4044
[00:13:09.297] Browser LOG     âœ… Real inference result: {"confidence":0.6922,"detection_id":"6920bacf15119379e5f21d91","panic_detected":false,"timestamp":"2025-11-21T19:17:35.242Z"}
[00:13:09.297] Browser LOG     âœ… Real inference result: {"confidence":0.3737,"detection_id":"6920bacf15119379e5f21d93","panic_detected":false,"timestamp":"2025-11-21T19:17:35.324Z"}
[00:13:09.298] Browser LOG     âœ… Real inference result: {"confidence":0.3308,"detection_id":"6920bacf15119379e5f21d99","panic_detected":false,"timestamp":"2025-11-21T19:17:35.898Z"}
[00:13:09.298] Browser LOG     âœ… Real inference result: {"confidence":0.5578,"detection_id":"6920bad115119379e5f21d9d","panic_detected":false,"timestamp":"2025-11-21T19:17:37.205Z"}
[00:13:09.298] Browser LOG     âœ… Real inference result: {"confidence":0.3655,"detection_id":"6920bad115119379e5f21d9f","panic_detected":false,"timestamp":"2025-11-21T19:17:37.213Z"}
[00:13:09.298] Browser LOG     âœ… Real inference result: {"confidence":0.3468,"detection_id":"6920bad115119379e5f21da5","panic_detected":false,"timestamp":"2025-11-21T19:17:37.378Z"}
[00:13:09.298] Browser LOG     âœ… Real inference result: {"confidence":0.5625,"detection_id":"6920bad215119379e5f21dab","panic_detected":false,"timestamp":"2025-11-21T19:17:38.149Z"}
[00:13:09.298] Browser LOG     âœ… Real inference result: {"confidence":0.3114,"detection_id":"6920bad115119379e5f21da9","panic_detected":false,"timestamp":"2025-11-21T19:17:38.134Z"}
[00:13:09.341] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:09.351] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:09.354] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:13:09.355] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:09.887] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:09.896] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:09.900] Server  LOG     ðŸ“¹ Frame received: 48569 bytes from camera cam_002
[00:13:09.901] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:10.027] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5366
[00:13:10.430] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:10.438] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:10.440] Server  LOG     ðŸ“¹ Frame received: 50010 bytes from camera cam_003
[00:13:10.441] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:10.716] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.3619
[00:13:11.207] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4751
[00:13:11.412] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:11.420] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:11.422] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:13:11.423] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:11.445] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:11.458] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:11.464] Server  LOG     ðŸ“¹ Frame received: 48569 bytes from camera cam_002
[00:13:11.470] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:11.492] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:11.514] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:11.518] Server  LOG     ðŸ“¹ Frame received: 50010 bytes from camera cam_003
[00:13:11.520] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:12.251] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.516
[00:13:12.262] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4291
[00:13:12.320] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4388
[00:13:12.616] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:12.629] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:12.631] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:13:12.632] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:12.670] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:12.686] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:12.691] Server  LOG     ðŸ“¹ Frame received: 48569 bytes from camera cam_002
[00:13:12.692] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:12.779] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:12.811] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:12.824] Server  LOG     ðŸ“¹ Frame received: 50010 bytes from camera cam_003
[00:13:12.827] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:13.672] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.522
[00:13:13.832] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4827
[00:13:13.901] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4479
[00:13:14.173] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:14.181] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:14.183] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:13:14.184] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:14.326] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:14.336] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:14.338] Server  LOG     ðŸ“¹ Frame received: 48569 bytes from camera cam_002
[00:13:14.339] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:14.916] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:14.926] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:14.928] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5131
[00:13:14.935] Server  LOG     ðŸ“¹ Frame received: 50010 bytes from camera cam_003
[00:13:14.936] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:15.145] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5095
[00:13:15.971] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4733
[00:13:18.422] Browser LOG     âœ… Real inference result: {"confidence":0.4493,"detection_id":"6920bad315119379e5f21db1","panic_detected":false,"timestamp":"2025-11-21T19:17:39.371Z"}
[00:13:18.422] Browser LOG     âœ… Real inference result: {"confidence":0.5995,"detection_id":"6920bad315119379e5f21db5","panic_detected":false,"timestamp":"2025-11-21T19:17:40.052Z"}
[00:13:18.422] Browser LOG     âœ… Real inference result: {"confidence":0.2794,"detection_id":"6920bad415119379e5f21db9","panic_detected":false,"timestamp":"2025-11-21T19:17:40.183Z"}
[00:13:18.422] Browser LOG     âœ… Real inference result: {"confidence":0.4044,"detection_id":"6920bad415119379e5f21dbd","panic_detected":false,"timestamp":"2025-11-21T19:17:40.288Z"}
[00:13:18.422] Browser LOG     âœ… Real inference result: {"confidence":0.5366,"detection_id":"6920bad515119379e5f21dc1","panic_detected":false,"timestamp":"2025-11-21T19:17:41.251Z"}
[00:13:18.422] Browser LOG     âœ… Real inference result: {"confidence":0.3619,"detection_id":"6920bad515119379e5f21dc5","panic_detected":false,"timestamp":"2025-11-21T19:17:41.940Z"}
[00:13:18.422] Browser LOG     âœ… Real inference result: {"confidence":0.4751,"detection_id":"6920bad615119379e5f21dc9","panic_detected":false,"timestamp":"2025-11-21T19:17:42.432Z"}
[00:13:18.422] Browser LOG     âœ… Real inference result: {"confidence":0.516,"detection_id":"6920bad715119379e5f21dcd","panic_detected":false,"timestamp":"2025-11-21T19:17:43.474Z"}
[00:13:18.422] Browser LOG     âœ… Real inference result: {"confidence":0.4291,"detection_id":"6920bad715119379e5f21dcf","panic_detected":false,"timestamp":"2025-11-21T19:17:43.487Z"}
[00:13:18.422] Browser LOG     âœ… Real inference result: {"confidence":0.4388,"detection_id":"6920bad715119379e5f21dd3","panic_detected":false,"timestamp":"2025-11-21T19:17:43.545Z"}
[00:13:18.422] Browser LOG     âœ… Real inference result: {"confidence":0.522,"detection_id":"6920bad815119379e5f21dd9","panic_detected":false,"timestamp":"2025-11-21T19:17:44.886Z"}
[00:13:18.422] Browser LOG     âœ… Real inference result: {"confidence":0.4827,"detection_id":"6920bad815119379e5f21ddd","panic_detected":false,"timestamp":"2025-11-21T19:17:45.055Z"}
[00:13:18.422] Browser LOG     âœ… Real inference result: {"confidence":0.4479,"detection_id":"6920bad815119379e5f21ddf","panic_detected":false,"timestamp":"2025-11-21T19:17:45.126Z"}
[00:13:19.468] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:19.480] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:19.482] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:13:19.483] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:19.859] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:19.871] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:19.921] Server  LOG     ðŸ“¹ Frame received: 48569 bytes from camera cam_002
[00:13:19.923] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:20.327] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5038
[00:13:20.473] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:20.484] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:20.486] Server  LOG     ðŸ“¹ Frame received: 50010 bytes from camera cam_003
[00:13:20.487] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:20.779] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5173
[00:13:21.404] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4525
[00:13:21.672] Browser LOG     âœ… Real inference result: {"confidence":0.5131,"detection_id":"6920bada15119379e5f21de5","panic_detected":false,"timestamp":"2025-11-21T19:17:46.147Z"}
[00:13:21.672] Browser LOG     âœ… Real inference result: {"confidence":0.5095,"detection_id":"6920bada15119379e5f21de9","panic_detected":false,"timestamp":"2025-11-21T19:17:46.358Z"}
[00:13:21.672] Browser LOG     âœ… Real inference result: {"confidence":0.4733,"detection_id":"6920badb15119379e5f21ded","panic_detected":false,"timestamp":"2025-11-21T19:17:47.192Z"}
[00:13:22.290] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:22.301] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:22.303] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:13:22.304] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:22.324] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:22.333] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:22.337] Server  LOG     ðŸ“¹ Frame received: 48569 bytes from camera cam_002
[00:13:22.338] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:22.373] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:22.388] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:22.395] Server  LOG     ðŸ“¹ Frame received: 50010 bytes from camera cam_003
[00:13:22.396] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:23.317] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.481
[00:13:23.333] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5457
[00:13:23.409] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4261
[00:13:23.484] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:23.496] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:23.499] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:13:23.501] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:23.569] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:23.582] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:23.587] Server  LOG     ðŸ“¹ Frame received: 48569 bytes from camera cam_002
[00:13:23.588] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:24.130] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:24.142] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:24.145] Server  LOG     ðŸ“¹ Frame received: 50010 bytes from camera cam_003
[00:13:24.146] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:24.264] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4894
[00:13:24.365] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.531
[00:13:24.732] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:24.742] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:24.744] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:13:24.745] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:24.924] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4213
[00:13:25.635] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:25.651] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:25.681] Server  LOG     ðŸ“¹ Frame received: 48569 bytes from camera cam_002
[00:13:25.682] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:25.714] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4909
[00:13:25.963] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:25.973] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:25.975] Server  LOG     ðŸ“¹ Frame received: 50010 bytes from camera cam_003
[00:13:25.978] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:26.470] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.547
[00:13:26.780] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:26.788] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:26.789] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:13:26.790] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:26.809] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:26.819] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:26.822] Server  LOG     ðŸ“¹ Frame received: 48569 bytes from camera cam_002
[00:13:26.824] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:26.839] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:26.859] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:26.862] Server  LOG     ðŸ“¹ Frame received: 50010 bytes from camera cam_003
[00:13:26.880] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:26.893] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4495
[00:13:27.735] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:27.747] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:27.749] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:13:27.750] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:27.796] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4758
[00:13:27.831] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5406
[00:13:27.867] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.467
[00:13:27.961] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:27.970] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:27.972] Server  LOG     ðŸ“¹ Frame received: 48569 bytes from camera cam_002
[00:13:27.974] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:28.663] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4896
[00:13:28.835] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:28.847] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:28.853] Server  LOG     ðŸ“¹ Frame received: 50010 bytes from camera cam_003
[00:13:28.854] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:28.867] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5456
[00:13:29.519] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:29.526] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:29.528] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:13:29.529] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:29.680] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4622
[00:13:30.673] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4911
[00:13:30.871] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:30.880] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:30.882] Server  LOG     ðŸ“¹ Frame received: 48569 bytes from camera cam_002
[00:13:30.883] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:31.860] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:13:31.873] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:13:31.878] Server  LOG     ðŸ“¹ Frame received: 50010 bytes from camera cam_003
[00:13:31.879] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:13:31.968] Browser LOG     âœ… Real inference result: {"confidence":0.5038,"detection_id":"6920badf15119379e5f21df1","panic_detected":false,"timestamp":"2025-11-21T19:17:51.551Z"}
[00:13:31.968] Browser LOG     âœ… Real inference result: {"confidence":0.5173,"detection_id":"6920badf15119379e5f21df5","panic_detected":false,"timestamp":"2025-11-21T19:17:52.003Z"}
[00:13:31.968] Browser LOG     âœ… Real inference result: {"confidence":0.4525,"detection_id":"6920bae015119379e5f21df9","panic_detected":false,"timestamp":"2025-11-21T19:17:52.628Z"}
[00:13:31.968] Browser LOG     âœ… Real inference result: {"confidence":0.481,"detection_id":"6920bae215119379e5f21dff","panic_detected":false,"timestamp":"2025-11-21T19:17:54.540Z"}
[00:13:31.969] Browser LOG     âœ… Real inference result: {"confidence":0.5457,"detection_id":"6920bae215119379e5f21dfd","panic_detected":false,"timestamp":"2025-11-21T19:17:54.544Z"}
[00:13:31.969] Browser LOG     âœ… Real inference result: {"confidence":0.4261,"detection_id":"6920bae215119379e5f21e01","panic_detected":false,"timestamp":"2025-11-21T19:17:54.633Z"}
[00:13:31.969] Browser LOG     âœ… Real inference result: {"confidence":0.4894,"detection_id":"6920bae315119379e5f21e09","panic_detected":false,"timestamp":"2025-11-21T19:17:55.489Z"}
[00:13:31.969] Browser LOG     âœ… Real inference result: {"confidence":0.531,"detection_id":"6920bae315119379e5f21e0d","panic_detected":false,"timestamp":"2025-11-21T19:17:55.586Z"}
[00:13:31.969] Browser LOG     âœ… Real inference result: {"confidence":0.4213,"detection_id":"6920bae315119379e5f21e11","panic_detected":false,"timestamp":"2025-11-21T19:17:56.130Z"}
[00:13:31.969] Browser LOG     âœ… Real inference result: {"confidence":0.4909,"detection_id":"6920bae415119379e5f21e15","panic_detected":false,"timestamp":"2025-11-21T19:17:56.935Z"}
[00:13:31.969] Browser LOG     âœ… Real inference result: {"confidence":0.547,"detection_id":"6920bae515119379e5f21e19","panic_detected":false,"timestamp":"2025-11-21T19:17:57.693Z"}
[00:13:32.080] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5641
[00:13:32.579] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4506
[00:13:33.175] Browser LOG     âœ… Real inference result: {"confidence":0.4495,"detection_id":"6920bae515119379e5f21e1d","panic_detected":false,"timestamp":"2025-11-21T19:17:58.045Z"}
[00:13:33.175] Browser LOG     âœ… Real inference result: {"confidence":0.467,"detection_id":"6920bae615119379e5f21e25","panic_detected":false,"timestamp":"2025-11-21T19:17:59.088Z"}
[00:13:33.175] Browser LOG     âœ… Real inference result: {"confidence":0.5406,"detection_id":"6920bae615119379e5f21e23","panic_detected":false,"timestamp":"2025-11-21T19:17:59.052Z"}
[00:13:33.175] Browser LOG     âœ… Real inference result: {"confidence":0.4758,"detection_id":"6920bae615119379e5f21e21","panic_detected":false,"timestamp":"2025-11-21T19:17:59.016Z"}
[00:13:33.175] Browser LOG     âœ… Real inference result: {"confidence":0.4896,"detection_id":"6920bae715119379e5f21e2d","panic_detected":false,"timestamp":"2025-11-21T19:17:59.887Z"}
[00:13:33.175] Browser LOG     âœ… Real inference result: {"confidence":0.5456,"detection_id":"6920bae715119379e5f21e31","panic_detected":false,"timestamp":"2025-11-21T19:18:00.086Z"}
[00:13:33.175] Browser LOG     âœ… Real inference result: {"confidence":0.4622,"detection_id":"6920bae815119379e5f21e35","panic_detected":false,"timestamp":"2025-11-21T19:18:00.905Z"}
[00:13:33.175] Browser LOG     âœ… Real inference result: {"confidence":0.4911,"detection_id":"6920bae915119379e5f21e39","panic_detected":false,"timestamp":"2025-11-21T19:18:01.891Z"}
[00:13:33.175] Browser LOG     âœ… Real inference result: {"confidence":0.5641,"detection_id":"6920baeb15119379e5f21e3d","panic_detected":false,"timestamp":"2025-11-21T19:18:03.305Z"}
[00:13:33.175] Browser LOG     âœ… Real inference result: {"confidence":0.4506,"detection_id":"6920baeb15119379e5f21e41","panic_detected":false,"timestamp":"2025-11-21T19:18:03.805Z"}
[00:14:06.267] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:14:06.276] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:14:06.277] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:14:06.278] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:14:06.296] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:14:06.314] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:14:06.316] Server  LOG     ðŸ“¹ Frame received: 48569 bytes from camera cam_002
[00:14:06.317] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:14:07.387] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5718
[00:14:07.396] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4818
[00:14:07.415] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:14:07.427] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:14:07.428] Server  LOG     ðŸ“¹ Frame received: 50010 bytes from camera cam_003
[00:14:07.429] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:14:08.586] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4585
[00:14:08.607] Browser LOG     âœ… Real inference result: {"confidence":0.5718,"detection_id":"6920bb0e15119379e5f21e45","panic_detected":false,"timestamp":"2025-11-21T19:18:38.608Z"}
[00:14:08.607] Browser LOG     âœ… Real inference result: {"confidence":0.4818,"detection_id":"6920bb0e15119379e5f21e47","panic_detected":false,"timestamp":"2025-11-21T19:18:38.611Z"}
[00:14:09.176] Browser LOG     âœ… Real inference result: {"confidence":0.4585,"detection_id":"6920bb0f15119379e5f21e4d","panic_detected":false,"timestamp":"2025-11-21T19:18:39.809Z"}
[00:15:06.239] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:15:06.252] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:15:06.256] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:15:06.258] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:15:07.234] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:15:07.252] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:15:07.263] Server  LOG     ðŸ“¹ Frame received: 48569 bytes from camera cam_002
[00:15:07.267] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:15:07.393] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5006
[00:15:07.616] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:15:07.630] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:15:07.635] Server  LOG     ðŸ“¹ Frame received: 50010 bytes from camera cam_003
[00:15:07.636] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:15:08.366] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5432
[00:15:08.643] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4442
[00:15:09.188] Browser LOG     âœ… Real inference result: {"confidence":0.5006,"detection_id":"6920bb4a15119379e5f21e51","panic_detected":false,"timestamp":"2025-11-21T19:19:38.612Z"}
[00:15:09.188] Browser LOG     âœ… Real inference result: {"confidence":0.5432,"detection_id":"6920bb4b15119379e5f21e55","panic_detected":false,"timestamp":"2025-11-21T19:19:39.591Z"}
[00:15:09.188] Browser LOG     âœ… Real inference result: {"confidence":0.4442,"detection_id":"6920bb4b15119379e5f21e59","panic_detected":false,"timestamp":"2025-11-21T19:19:39.866Z"}
[00:16:06.244] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:16:06.258] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:16:06.261] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:16:06.263] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:16:06.384] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:16:06.394] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:16:06.397] Server  LOG     ðŸ“¹ Frame received: 48569 bytes from camera cam_002
[00:16:06.398] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:16:06.609] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:16:06.620] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:16:06.622] Server  LOG     ðŸ“¹ Frame received: 50010 bytes from camera cam_003
[00:16:06.623] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:16:07.224] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4981
[00:16:07.261] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5442
[00:16:07.541] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4444
[00:16:08.164] Browser LOG     âœ… Real inference result: {"confidence":0.4981,"detection_id":"6920bb8615119379e5f21e5d","panic_detected":false,"timestamp":"2025-11-21T19:20:38.447Z"}
[00:16:08.164] Browser LOG     âœ… Real inference result: {"confidence":0.5442,"detection_id":"6920bb8615119379e5f21e5f","panic_detected":false,"timestamp":"2025-11-21T19:20:38.484Z"}
[00:16:08.164] Browser LOG     âœ… Real inference result: {"confidence":0.4444,"detection_id":"6920bb8615119379e5f21e65","panic_detected":false,"timestamp":"2025-11-21T19:20:38.765Z"}
[00:17:06.197] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:17:06.203] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:17:06.206] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:17:06.206] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:17:06.215] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:17:06.222] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:17:06.224] Server  LOG     ðŸ“¹ Frame received: 48569 bytes from camera cam_002
[00:17:06.225] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:17:06.242] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:17:06.249] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:17:06.252] Server  LOG     ðŸ“¹ Frame received: 50010 bytes from camera cam_003
[00:17:06.254] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:17:06.756] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4961
[00:17:06.816] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5377
[00:17:06.834] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4375
[00:17:07.164] Browser LOG     âœ… Real inference result: {"confidence":0.4961,"detection_id":"6920bbc115119379e5f21e69","panic_detected":false,"timestamp":"2025-11-21T19:21:37.981Z"}
[00:17:07.164] Browser LOG     âœ… Real inference result: {"confidence":0.5377,"detection_id":"6920bbc115119379e5f21e6b","panic_detected":false,"timestamp":"2025-11-21T19:21:38.035Z"}
[00:17:07.164] Browser LOG     âœ… Real inference result: {"confidence":0.4375,"detection_id":"6920bbc115119379e5f21e6d","panic_detected":false,"timestamp":"2025-11-21T19:21:38.041Z"}
[00:18:06.198] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:18:06.204] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:18:06.205] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:18:06.206] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:18:06.227] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:18:06.247] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:18:06.252] Server  LOG     ðŸ“¹ Frame received: 48569 bytes from camera cam_002
[00:18:06.253] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:18:06.264] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:18:06.274] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:18:06.277] Server  LOG     ðŸ“¹ Frame received: 50010 bytes from camera cam_003
[00:18:06.279] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:18:06.757] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5158
[00:18:06.761] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5385
[00:18:06.777] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4378
[00:18:07.173] Browser LOG     âœ… Real inference result: {"confidence":0.5158,"detection_id":"6920bbfd15119379e5f21e75","panic_detected":false,"timestamp":"2025-11-21T19:22:37.982Z"}
[00:18:07.173] Browser LOG     âœ… Real inference result: {"confidence":0.5385,"detection_id":"6920bbfd15119379e5f21e77","panic_detected":false,"timestamp":"2025-11-21T19:22:37.983Z"}
[00:18:07.174] Browser LOG     âœ… Real inference result: {"confidence":0.4378,"detection_id":"6920bbfd15119379e5f21e79","panic_detected":false,"timestamp":"2025-11-21T19:22:38.002Z"}
[00:19:06.210] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:19:06.216] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:19:06.218] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:19:06.219] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:19:06.230] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:19:06.238] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:19:06.240] Server  LOG     ðŸ“¹ Frame received: 48569 bytes from camera cam_002
[00:19:06.241] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:19:06.258] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:19:06.266] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:19:06.269] Server  LOG     ðŸ“¹ Frame received: 50010 bytes from camera cam_003
[00:19:06.271] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:19:06.823] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5216
[00:19:06.828] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4562
[00:19:06.835] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5512
[00:19:07.161] Browser LOG     âœ… Real inference result: {"confidence":0.5216,"detection_id":"6920bc3915119379e5f21e81","panic_detected":false,"timestamp":"2025-11-21T19:23:38.048Z"}
[00:19:07.161] Browser LOG     âœ… Real inference result: {"confidence":0.4562,"detection_id":"6920bc3915119379e5f21e85","panic_detected":false,"timestamp":"2025-11-21T19:23:38.049Z"}
[00:19:07.161] Browser LOG     âœ… Real inference result: {"confidence":0.5512,"detection_id":"6920bc3915119379e5f21e83","panic_detected":false,"timestamp":"2025-11-21T19:23:38.053Z"}
[00:20:06.203] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:20:06.209] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:20:06.211] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:20:06.212] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:20:06.222] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:20:06.232] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:20:06.233] Server  LOG     ðŸ“¹ Frame received: 48569 bytes from camera cam_002
[00:20:06.234] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:20:06.244] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:20:06.252] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:20:06.253] Server  LOG     ðŸ“¹ Frame received: 50010 bytes from camera cam_003
[00:20:06.254] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:20:06.759] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5402
[00:20:06.763] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4623
[00:20:06.770] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5401
[00:20:07.171] Browser LOG     âœ… Real inference result: {"confidence":0.5402,"detection_id":"6920bc7515119379e5f21e8d","panic_detected":false,"timestamp":"2025-11-21T19:24:37.984Z"}
[00:20:07.171] Browser LOG     âœ… Real inference result: {"confidence":0.4623,"detection_id":"6920bc7515119379e5f21e91","panic_detected":false,"timestamp":"2025-11-21T19:24:37.985Z"}
[00:20:07.171] Browser LOG     âœ… Real inference result: {"confidence":0.5401,"detection_id":"6920bc7515119379e5f21e8f","panic_detected":false,"timestamp":"2025-11-21T19:24:37.986Z"}
[00:21:06.206] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:21:06.212] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:21:06.214] Server  LOG     ðŸ“¹ Frame received: 41863 bytes from camera cam_001
[00:21:06.215] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:21:06.224] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:21:06.232] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:21:06.233] Server  LOG     ðŸ“¹ Frame received: 48569 bytes from camera cam_002
[00:21:06.234] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:21:06.253] Server  WARN     âš  ./frontend/app/api/inference/route.ts:4:14
Next.js can't recognize the exported `config` field in route. Page config in `config` is deprecated and ignored, use individual exports instead.
  2 | import { NextRequest, NextResponse } from 'next/server'
  3 |
> 4 | export const config = {
    |              ^^^^^^
  5 |   maxDuration: 30,
  6 | }
  7 |

The exported configuration object in a source file needs to have a very specific format from which some properties can be statically parsed at compiled-time.

https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config


[00:21:06.264] Server  LOG     ðŸ”„ [API] Proxying inference request to backend...
[00:21:06.266] Server  LOG     ðŸ“¹ Frame received: 50010 bytes from camera cam_003
[00:21:06.266] Server  LOG     ðŸš€ Forwarding to: http://localhost:5001/api/inference/lstm-panic-detect
[00:21:06.756] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.553
[00:21:06.760] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.5297
[00:21:06.765] Server  LOG     âœ… Inference successful - Panic: false, Confidence: 0.4818
[00:21:07.176] Browser LOG     âœ… Real inference result: {"confidence":0.553,"detection_id":"6920bcb115119379e5f21e9b","panic_detected":false,"timestamp":"2025-11-21T19:25:37.981Z"}
[00:21:07.176] Browser LOG     âœ… Real inference result: {"confidence":0.5297,"detection_id":"6920bcb115119379e5f21e99","panic_detected":false,"timestamp":"2025-11-21T19:25:37.982Z"}
[00:21:07.176] Browser LOG     âœ… Real inference result: {"confidence":0.4818,"detection_id":"6920bcb115119379e5f21e9d","panic_detected":false,"timestamp":"2025-11-21T19:25:37.989Z"}
[00:21:15.320] Server  LOG      âœ“ Compiled in 1534ms
[00:21:20.617] Server  ERROR   Backend returned 400
[00:21:20.643] Server  ERROR   Backend returned 400
[00:21:20.649] Server  ERROR   Backend returned 400
[00:21:20.659] Server  ERROR   Backend returned 400
[00:21:20.686] Server  ERROR   Backend returned 400
[00:21:20.711] Server  ERROR   Backend returned 400
[00:21:20.867] Server  ERROR   Backend returned 400
[00:21:20.869] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:20.869] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.12918394289551005
[00:21:20.869] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:20.869] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.31303301812454515
[00:21:20.869] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:20.869] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.29283102524826
[00:21:20.869] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:20.869] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8170606138329556
[00:21:20.869] Browser LOG     Panic detected at: Gate 1 Entrance Confidence: 0.8170606138329556
[00:21:20.869] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:20.869] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.25153830852314957
[00:21:20.869] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:20.869] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2883187568981092
[00:21:20.874] Server  ERROR   Backend returned 400
[00:21:20.910] Server  ERROR   Backend returned 400
[00:21:20.918] Server  ERROR   Backend returned 400
[00:21:20.937] Server  ERROR   Backend returned 400
[00:21:20.958] Server  ERROR   Backend returned 400
[00:21:21.034] Server  ERROR   Backend returned 400
[00:21:21.039] Server  ERROR   Backend returned 400
[00:21:21.059] Server  ERROR   Backend returned 400
[00:21:21.132] Server  ERROR   Backend returned 400
[00:21:21.147] Server  ERROR   Backend returned 400
[00:21:21.162] Server  ERROR   Backend returned 400
[00:21:21.269] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:21.269] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8810921990331951
[00:21:21.269] Browser LOG     Panic detected at: Parking Area Confidence: 0.8810921990331951
[00:21:21.269] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:21.269] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9860291854944928
[00:21:21.269] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:21.269] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.11456744148610526
[00:21:21.269] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:21.269] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9852293333950184
[00:21:21.269] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:21.269] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7708966219553569
[00:21:21.269] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:21.269] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.31324764368396946
[00:21:21.269] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:21.269] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4945502551223368
[00:21:21.269] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:21.269] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.10699740899814048
[00:21:21.269] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:21.269] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3705626257246277
[00:21:21.269] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:21.269] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.01386501043054067
[00:21:21.269] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:21.269] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8656291209266531
[00:21:21.269] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:21.269] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7930139016577146
[00:21:21.269] Browser LOG     Panic detected at: Main Hall Confidence: 0.7930139016577146
[00:21:21.513] Server  ERROR   Backend returned 400
[00:21:21.546] Server  ERROR   Backend returned 400
[00:21:21.587] Server  ERROR   Backend returned 400
[00:21:21.705] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:21.705] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9511328641037591
[00:21:21.705] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:21.705] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9485654140787658
[00:21:21.705] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:21.705] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9731743143702019
[00:21:21.974] Server  ERROR   Backend returned 400
[00:21:22.021] Server  ERROR   Backend returned 400
[00:21:22.065] Server  ERROR   Backend returned 400
[00:21:22.172] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:22.172] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9212224532832084
[00:21:22.172] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:22.172] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.016946694766395654
[00:21:22.172] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:22.172] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9642937278813282
[00:21:22.446] Server  ERROR   Backend returned 400
[00:21:22.474] Server  ERROR   Backend returned 400
[00:21:22.492] Server  ERROR   Backend returned 400
[00:21:22.597] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:22.597] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.22132800608958675
[00:21:22.597] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:22.597] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9123096999173388
[00:21:22.597] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:22.597] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8628087063081882
[00:21:22.950] Server  ERROR   Backend returned 400
[00:21:22.974] Server  ERROR   Backend returned 400
[00:21:22.989] Server  ERROR   Backend returned 400
[00:21:23.093] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:23.093] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.251234133086038
[00:21:23.093] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:23.093] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2126635438430985
[00:21:23.093] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:23.093] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.11940546612176356
[00:21:23.441] Server  ERROR   Backend returned 400
[00:21:23.469] Server  ERROR   Backend returned 400
[00:21:23.483] Server  ERROR   Backend returned 400
[00:21:23.588] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:23.588] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.06362019418003279
[00:21:23.588] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:23.588] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4390834115436088
[00:21:23.588] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:23.588] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.45889722670477
[00:21:23.953] Server  ERROR   Backend returned 400
[00:21:23.981] Server  ERROR   Backend returned 400
[00:21:23.999] Server  ERROR   Backend returned 400
[00:21:24.104] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:24.104] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9956981645470587
[00:21:24.104] Browser LOG     Panic detected at: Parking Area Confidence: 0.9956981645470587
[00:21:24.104] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:24.104] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4622232669040712
[00:21:24.104] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:24.104] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9233769970674129
[00:21:24.104] Browser LOG     Panic detected at: Main Hall Confidence: 0.9233769970674129
[00:21:24.439] Server  ERROR   Backend returned 400
[00:21:24.459] Server  ERROR   Backend returned 400
[00:21:24.481] Server  ERROR   Backend returned 400
[00:21:24.586] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:24.586] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.23121714604432708
[00:21:24.586] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:24.586] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.20401048108731756
[00:21:24.586] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:24.586] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.13318668030889974
[00:21:24.933] Server  ERROR   Backend returned 400
[00:21:24.972] Server  ERROR   Backend returned 400
[00:21:24.975] Server  ERROR   Backend returned 400
[00:21:25.083] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:25.083] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.42897864917035006
[00:21:25.083] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:25.083] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2093786092879567
[00:21:25.083] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:25.083] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2715419376725979
[00:21:25.445] Server  ERROR   Backend returned 400
[00:21:25.459] Server  ERROR   Backend returned 400
[00:21:25.490] Server  ERROR   Backend returned 400
[00:21:25.594] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:25.594] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4437086429613659
[00:21:25.594] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:25.594] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.40330977623596537
[00:21:25.594] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:25.594] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3354552148845131
[00:21:25.933] Server  ERROR   Backend returned 400
[00:21:25.960] Server  ERROR   Backend returned 400
[00:21:25.980] Server  ERROR   Backend returned 400
[00:21:26.083] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:26.083] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8535644876524251
[00:21:26.083] Browser LOG     Panic detected at: Gate 1 Entrance Confidence: 0.8535644876524251
[00:21:26.083] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:26.083] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.40662339739841613
[00:21:26.083] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:26.083] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4522413485889506
[00:21:26.445] Server  ERROR   Backend returned 400
[00:21:26.465] Server  ERROR   Backend returned 400
[00:21:26.478] Server  ERROR   Backend returned 400
[00:21:26.583] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:26.583] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7542279464420455
[00:21:26.583] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:26.583] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7536849180961483
[00:21:26.583] Browser LOG     Panic detected at: Main Hall Confidence: 0.7536849180961483
[00:21:26.583] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:26.583] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.12809537367554863
[00:21:26.944] Server  ERROR   Backend returned 400
[00:21:26.977] Server  ERROR   Backend returned 400
[00:21:26.993] Server  ERROR   Backend returned 400
[00:21:27.098] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:27.098] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3709012561700029
[00:21:27.098] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:27.098] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1513210163476275
[00:21:27.098] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:27.098] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.936010452800935
[00:21:27.098] Browser LOG     Panic detected at: Parking Area Confidence: 0.936010452800935
[00:21:27.436] Server  ERROR   Backend returned 400
[00:21:27.472] Server  ERROR   Backend returned 400
[00:21:27.474] Server  ERROR   Backend returned 400
[00:21:27.585] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:27.585] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9735100867746302
[00:21:27.585] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:27.585] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.94420853985424
[00:21:27.585] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:27.585] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.041396441307053555
[00:21:27.935] Server  ERROR   Backend returned 400
[00:21:27.965] Server  ERROR   Backend returned 400
[00:21:27.968] Server  ERROR   Backend returned 400
[00:21:28.080] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:28.080] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.13269563938920115
[00:21:28.080] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:28.080] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.031288806102954425
[00:21:28.080] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:28.080] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1373769409803317
[00:21:28.432] Server  ERROR   Backend returned 400
[00:21:28.467] Server  ERROR   Backend returned 400
[00:21:28.479] Server  ERROR   Backend returned 400
[00:21:28.596] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:28.596] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3269123248321117
[00:21:28.596] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:28.596] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.24615140773591615
[00:21:28.596] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:28.596] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.20379390932291908
[00:21:28.935] Server  ERROR   Backend returned 400
[00:21:28.964] Server  ERROR   Backend returned 400
[00:21:28.966] Server  ERROR   Backend returned 400
[00:21:29.082] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:29.082] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4361266012365096
[00:21:29.082] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:29.082] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.802334896161626
[00:21:29.082] Browser LOG     Panic detected at: Main Hall Confidence: 0.802334896161626
[00:21:29.082] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:29.082] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.29352695592191774
[00:21:29.439] Server  ERROR   Backend returned 400
[00:21:29.470] Server  ERROR   Backend returned 400
[00:21:29.474] Server  ERROR   Backend returned 400
[00:21:29.583] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:29.583] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7304738245722849
[00:21:29.583] Browser LOG     Panic detected at: Gate 1 Entrance Confidence: 0.7304738245722849
[00:21:29.583] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:29.583] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7449588940256733
[00:21:29.583] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:29.583] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9128041797833933
[00:21:29.583] Browser LOG     Panic detected at: Parking Area Confidence: 0.9128041797833933
[00:21:29.938] Server  ERROR   Backend returned 400
[00:21:29.960] Server  ERROR   Backend returned 400
[00:21:29.975] Server  ERROR   Backend returned 400
[00:21:30.092] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:30.092] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.36560983041540873
[00:21:30.092] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:30.092] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1625647120381019
[00:21:30.092] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:30.092] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.19444227339003234
[00:21:30.432] Server  ERROR   Backend returned 400
[00:21:30.467] Server  ERROR   Backend returned 400
[00:21:30.472] Server  ERROR   Backend returned 400
[00:21:30.580] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:30.580] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4840092817673056
[00:21:30.580] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:30.580] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.012139804321893655
[00:21:30.580] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:30.580] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.12193853136851196
[00:21:30.931] Server  ERROR   Backend returned 400
[00:21:30.973] Server  ERROR   Backend returned 400
[00:21:30.976] Server  ERROR   Backend returned 400
[00:21:31.086] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:31.086] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9984070723127045
[00:21:31.086] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:31.086] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.12137503544238609
[00:21:31.086] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:31.086] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2870467176983592
[00:21:31.433] Server  ERROR   Backend returned 400
[00:21:31.472] Server  ERROR   Backend returned 400
[00:21:31.476] Server  ERROR   Backend returned 400
[00:21:31.587] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:31.587] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9081951687071328
[00:21:31.587] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:31.587] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8330208672768646
[00:21:31.587] Browser LOG     Panic detected at: Main Hall Confidence: 0.8330208672768646
[00:21:31.587] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:31.587] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.43242001317646944
[00:21:31.936] Server  ERROR   Backend returned 400
[00:21:31.967] Server  ERROR   Backend returned 400
[00:21:31.981] Server  ERROR   Backend returned 400
[00:21:32.089] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:32.089] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.030188466880073372
[00:21:32.089] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:32.089] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.33887148349434854
[00:21:32.089] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:32.089] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.022476764969804774
[00:21:32.436] Server  ERROR   Backend returned 400
[00:21:32.459] Server  ERROR   Backend returned 400
[00:21:32.472] Server  ERROR   Backend returned 400
[00:21:32.575] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:32.575] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.47371265336541624
[00:21:32.575] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:32.575] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.08091984433753674
[00:21:32.575] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:32.575] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.12214361071051089
[00:21:32.940] Server  ERROR   Backend returned 400
[00:21:32.969] Server  ERROR   Backend returned 400
[00:21:32.974] Server  ERROR   Backend returned 400
[00:21:33.085] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:33.085] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8490822708540027
[00:21:33.085] Browser LOG     Panic detected at: Gate 1 Entrance Confidence: 0.8490822708540027
[00:21:33.085] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:33.085] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.41173853939724886
[00:21:33.085] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:33.085] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2777442226822755
[00:21:33.437] Server  ERROR   Backend returned 400
[00:21:33.465] Server  ERROR   Backend returned 400
[00:21:33.481] Server  ERROR   Backend returned 400
[00:21:33.585] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:33.585] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.08550641529636449
[00:21:33.585] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:33.585] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.18999235240213752
[00:21:33.585] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:33.585] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.10829759057164828
[00:21:33.936] Server  ERROR   Backend returned 400
[00:21:33.964] Server  ERROR   Backend returned 400
[00:21:33.985] Server  ERROR   Backend returned 400
[00:21:34.092] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:34.092] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4009238042492707
[00:21:34.092] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:34.092] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.10501706919954878
[00:21:34.092] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:34.092] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1402178470528055
[00:21:34.435] Server  ERROR   Backend returned 400
[00:21:34.465] Server  ERROR   Backend returned 400
[00:21:34.468] Server  ERROR   Backend returned 400
[00:21:34.579] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:34.579] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.28707304323064725
[00:21:34.579] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:34.579] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.27220977423406556
[00:21:34.579] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:34.579] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.19126619124334776
[00:21:34.936] Server  ERROR   Backend returned 400
[00:21:34.962] Server  ERROR   Backend returned 400
[00:21:34.975] Server  ERROR   Backend returned 400
[00:21:35.090] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:35.090] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.47554525159915423
[00:21:35.090] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:35.090] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7460777642565058
[00:21:35.090] Browser LOG     Panic detected at: Main Hall Confidence: 0.7460777642565058
[00:21:35.090] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:35.090] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.011084613560809586
[00:21:35.433] Server  ERROR   Backend returned 400
[00:21:35.465] Server  ERROR   Backend returned 400
[00:21:35.478] Server  ERROR   Backend returned 400
[00:21:35.582] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:35.582] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.11344165489481
[00:21:35.582] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:35.582] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.40606445089313886
[00:21:35.582] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:35.582] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.12021049611435175
[00:21:35.932] Server  ERROR   Backend returned 400
[00:21:35.977] Server  ERROR   Backend returned 400
[00:21:35.979] Server  ERROR   Backend returned 400
[00:21:36.088] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:36.088] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.42426242177766377
[00:21:36.088] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:36.088] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9702981258151995
[00:21:36.088] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:36.088] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3837571105070714
[00:21:36.448] Server  ERROR   Backend returned 400
[00:21:36.488] Server  ERROR   Backend returned 400
[00:21:36.501] Server  ERROR   Backend returned 400
[00:21:36.606] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:36.606] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9771218180152337
[00:21:36.606] Browser LOG     Panic detected at: Gate 1 Entrance Confidence: 0.9771218180152337
[00:21:36.606] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:36.606] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9397794825907612
[00:21:36.606] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:36.606] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2644348177132826
[00:21:36.936] Server  ERROR   Backend returned 400
[00:21:36.965] Server  ERROR   Backend returned 400
[00:21:36.980] Server  ERROR   Backend returned 400
[00:21:37.099] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:37.099] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.20095069161081397
[00:21:37.099] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:37.099] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7440835206851072
[00:21:37.099] Browser LOG     Panic detected at: Main Hall Confidence: 0.7440835206851072
[00:21:37.099] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:37.099] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.47093027564814866
[00:21:37.440] Server  ERROR   Backend returned 400
[00:21:37.462] Server  ERROR   Backend returned 400
[00:21:37.475] Server  ERROR   Backend returned 400
[00:21:37.580] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:37.580] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3810302761660251
[00:21:37.580] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:37.580] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3665060471219851
[00:21:37.580] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:37.580] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8851805205440678
[00:21:37.580] Browser LOG     Panic detected at: Parking Area Confidence: 0.8851805205440678
[00:21:37.937] Server  ERROR   Backend returned 400
[00:21:37.959] Server  ERROR   Backend returned 400
[00:21:37.970] Server  ERROR   Backend returned 400
[00:21:38.073] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:38.073] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.15226411673243556
[00:21:38.073] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:38.073] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.049707524039447726
[00:21:38.073] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:38.073] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.15894046511449578
[00:21:38.435] Server  ERROR   Backend returned 400
[00:21:38.471] Server  ERROR   Backend returned 400
[00:21:38.475] Server  ERROR   Backend returned 400
[00:21:38.583] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:38.583] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.20161556904340072
[00:21:38.583] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:38.583] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2840874160236272
[00:21:38.583] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:38.583] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9511819314985546
[00:21:38.941] Server  ERROR   Backend returned 400
[00:21:38.968] Server  ERROR   Backend returned 400
[00:21:38.973] Server  ERROR   Backend returned 400
[00:21:39.083] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:39.083] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3844031200952398
[00:21:39.083] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:39.083] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9625592895388857
[00:21:39.083] Browser LOG     Panic detected at: Main Hall Confidence: 0.9625592895388857
[00:21:39.083] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:39.083] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1816560180679595
[00:21:39.437] Server  ERROR   Backend returned 400
[00:21:39.465] Server  ERROR   Backend returned 400
[00:21:39.479] Server  ERROR   Backend returned 400
[00:21:39.585] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:39.585] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3203738507272711
[00:21:39.585] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:39.585] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3070455925650386
[00:21:39.585] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:39.585] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.020979182593826973
[00:21:39.936] Server  ERROR   Backend returned 400
[00:21:39.964] Server  ERROR   Backend returned 400
[00:21:39.980] Server  ERROR   Backend returned 400
[00:21:40.084] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:40.084] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.940593817143222
[00:21:40.084] Browser LOG     Panic detected at: Gate 1 Entrance Confidence: 0.940593817143222
[00:21:40.084] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:40.084] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8500786957200965
[00:21:40.084] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:40.084] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.06975943199229384
[00:21:40.434] Server  ERROR   Backend returned 400
[00:21:40.453] Server  ERROR   Backend returned 400
[00:21:40.474] Server  ERROR   Backend returned 400
[00:21:40.584] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:40.585] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.04709818108078434
[00:21:40.585] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:40.585] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.06493250651335386
[00:21:40.585] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:40.585] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.918695432998292
[00:21:40.585] Browser LOG     Panic detected at: Parking Area Confidence: 0.918695432998292
[00:21:40.939] Server  ERROR   Backend returned 400
[00:21:40.961] Server  ERROR   Backend returned 400
[00:21:40.973] Server  ERROR   Backend returned 400
[00:21:41.078] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:41.078] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.057893660199867525
[00:21:41.078] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:41.078] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4294656994604674
[00:21:41.078] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:41.078] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.23675578828971422
[00:21:41.435] Server  ERROR   Backend returned 400
[00:21:41.460] Server  ERROR   Backend returned 400
[00:21:41.476] Server  ERROR   Backend returned 400
[00:21:41.580] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:41.580] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.44182919612884036
[00:21:41.580] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:41.580] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.06573672976995076
[00:21:41.580] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:41.580] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.10530244652093151
[00:21:41.940] Server  ERROR   Backend returned 400
[00:21:41.962] Server  ERROR   Backend returned 400
[00:21:41.974] Server  ERROR   Backend returned 400
[00:21:42.081] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:42.081] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9937121457391664
[00:21:42.081] Browser LOG     Panic detected at: Gate 1 Entrance Confidence: 0.9937121457391664
[00:21:42.081] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:42.081] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.09545276608198289
[00:21:42.081] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:42.081] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7790455657520718
[00:21:42.438] Server  ERROR   Backend returned 400
[00:21:42.466] Server  ERROR   Backend returned 400
[00:21:42.483] Server  ERROR   Backend returned 400
[00:21:42.588] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:42.588] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3461037029846956
[00:21:42.588] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:42.588] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8945508753308866
[00:21:42.588] Browser LOG     Panic detected at: Main Hall Confidence: 0.8945508753308866
[00:21:42.588] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:42.588] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.0062042658764042136
[00:21:42.935] Server  ERROR   Backend returned 400
[00:21:42.959] Server  ERROR   Backend returned 400
[00:21:42.974] Server  ERROR   Backend returned 400
[00:21:43.077] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:43.077] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8723460951380823
[00:21:43.077] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:43.077] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8487656190820271
[00:21:43.077] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:43.077] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4097696611194801
[00:21:43.434] Server  ERROR   Backend returned 400
[00:21:43.457] Server  ERROR   Backend returned 400
[00:21:43.468] Server  ERROR   Backend returned 400
[00:21:43.574] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:43.574] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1721808460653666
[00:21:43.574] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:43.574] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.27153538876768213
[00:21:43.574] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:43.574] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7216640151349898
[00:21:43.574] Browser LOG     Panic detected at: Parking Area Confidence: 0.7216640151349898
[00:21:43.936] Server  ERROR   Backend returned 400
[00:21:43.954] Server  ERROR   Backend returned 400
[00:21:43.974] Server  ERROR   Backend returned 400
[00:21:44.078] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:44.079] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.39187977672371194
[00:21:44.079] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:44.079] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7324781947734533
[00:21:44.079] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:44.079] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.09702391901783858
[00:21:44.440] Server  ERROR   Backend returned 400
[00:21:44.454] Server  ERROR   Backend returned 400
[00:21:44.500] Server  ERROR   Backend returned 400
[00:21:44.603] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:44.604] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8476293561400754
[00:21:44.604] Browser LOG     Panic detected at: Gate 1 Entrance Confidence: 0.8476293561400754
[00:21:44.604] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:44.604] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.36776045115749056
[00:21:44.604] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:44.604] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.29076615840705444
[00:21:44.946] Server  ERROR   Backend returned 400
[00:21:44.956] Server  ERROR   Backend returned 400
[00:21:44.979] Server  ERROR   Backend returned 400
[00:21:45.093] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:45.093] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9034458574561246
[00:21:45.093] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:45.093] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3853756447285649
[00:21:45.093] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:45.093] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.12163475499392007
[00:21:45.434] Server  ERROR   Backend returned 400
[00:21:45.455] Server  ERROR   Backend returned 400
[00:21:45.473] Server  ERROR   Backend returned 400
[00:21:45.577] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:45.577] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.38576239525137523
[00:21:45.577] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:45.578] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8539662249726405
[00:21:45.578] Browser LOG     Panic detected at: Main Hall Confidence: 0.8539662249726405
[00:21:45.578] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:45.578] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3207223021384456
[00:21:45.932] Server  ERROR   Backend returned 400
[00:21:45.953] Server  ERROR   Backend returned 400
[00:21:45.974] Server  ERROR   Backend returned 400
[00:21:46.083] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:46.083] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.19788312812267317
[00:21:46.083] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:46.083] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.32419031153735356
[00:21:46.083] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:46.083] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3868328611187027
[00:21:46.433] Server  ERROR   Backend returned 400
[00:21:46.459] Server  ERROR   Backend returned 400
[00:21:46.474] Server  ERROR   Backend returned 400
[00:21:46.577] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:46.577] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7603910023536443
[00:21:46.577] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:46.577] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.37912427599588105
[00:21:46.577] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:46.577] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.20212112768175378
[00:21:46.934] Server  ERROR   Backend returned 400
[00:21:46.963] Server  ERROR   Backend returned 400
[00:21:46.989] Server  ERROR   Backend returned 400
[00:21:47.095] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:47.095] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7165715812534191
[00:21:47.095] Browser LOG     Panic detected at: Gate 1 Entrance Confidence: 0.7165715812534191
[00:21:47.095] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:47.095] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.41474740645479463
[00:21:47.095] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:47.095] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7095593216714594
[00:21:47.095] Browser LOG     Panic detected at: Parking Area Confidence: 0.7095593216714594
[00:21:47.434] Server  ERROR   Backend returned 400
[00:21:47.463] Server  ERROR   Backend returned 400
[00:21:47.480] Server  ERROR   Backend returned 400
[00:21:47.585] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:47.585] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.20427531543626493
[00:21:47.585] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:47.585] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.39081651486499164
[00:21:47.585] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:47.585] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.01526761322101794
[00:21:47.934] Server  ERROR   Backend returned 400
[00:21:47.951] Server  ERROR   Backend returned 400
[00:21:47.965] Server  ERROR   Backend returned 400
[00:21:48.068] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:48.068] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8878807616392332
[00:21:48.068] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:48.068] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2963464460089856
[00:21:48.068] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:48.068] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9524222528617194
[00:21:48.434] Server  ERROR   Backend returned 400
[00:21:48.458] Server  ERROR   Backend returned 400
[00:21:48.472] Server  ERROR   Backend returned 400
[00:21:48.576] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:48.576] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4125255920427956
[00:21:48.576] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:48.576] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.41658543241533236
[00:21:48.576] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:48.576] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2762326177694615
[00:21:48.938] Server  ERROR   Backend returned 400
[00:21:48.958] Server  ERROR   Backend returned 400
[00:21:48.980] Server  ERROR   Backend returned 400
[00:21:49.085] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:49.085] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.37160695821854894
[00:21:49.085] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:49.085] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.25342977483713924
[00:21:49.085] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:49.085] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.740750907062169
[00:21:49.439] Server  ERROR   Backend returned 400
[00:21:49.458] Server  ERROR   Backend returned 400
[00:21:49.473] Server  ERROR   Backend returned 400
[00:21:49.588] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:49.588] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.29876264864251123
[00:21:49.588] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:49.588] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.07695885604102354
[00:21:49.588] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:49.588] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9459976562511001
[00:21:49.588] Browser LOG     Panic detected at: Parking Area Confidence: 0.9459976562511001
[00:21:49.933] Server  ERROR   Backend returned 400
[00:21:49.959] Server  ERROR   Backend returned 400
[00:21:49.963] Server  ERROR   Backend returned 400
[00:21:50.073] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:50.073] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.455685889348066
[00:21:50.073] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:50.073] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.23696009826151043
[00:21:50.073] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:50.073] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.20641646025714439
[00:21:50.435] Server  ERROR   Backend returned 400
[00:21:50.453] Server  ERROR   Backend returned 400
[00:21:50.466] Server  ERROR   Backend returned 400
[00:21:50.570] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:50.570] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3752459718168739
[00:21:50.570] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:50.570] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9386987619052517
[00:21:50.570] Browser LOG     Panic detected at: Main Hall Confidence: 0.9386987619052517
[00:21:50.570] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:50.570] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.35428212953720567
[00:21:50.938] Server  ERROR   Backend returned 400
[00:21:50.959] Server  ERROR   Backend returned 400
[00:21:50.980] Server  ERROR   Backend returned 400
[00:21:51.084] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:51.084] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.37861256884754113
[00:21:51.084] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:51.084] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1333272650984128
[00:21:51.084] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:51.084] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7446335467716318
[00:21:51.435] Server  ERROR   Backend returned 400
[00:21:51.463] Server  ERROR   Backend returned 400
[00:21:51.466] Server  ERROR   Backend returned 400
[00:21:51.576] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:51.576] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9767897417465994
[00:21:51.576] Browser LOG     Panic detected at: Gate 1 Entrance Confidence: 0.9767897417465994
[00:21:51.576] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:51.576] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2468513797278209
[00:21:51.576] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:51.576] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.13021956335260415
[00:21:51.943] Server  ERROR   Backend returned 400
[00:21:51.957] Server  ERROR   Backend returned 400
[00:21:51.971] Server  ERROR   Backend returned 400
[00:21:52.075] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:52.075] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4016056067476376
[00:21:52.075] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:52.075] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7810942037452576
[00:21:52.075] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:52.075] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.891060359306347
[00:21:52.075] Browser LOG     Panic detected at: Parking Area Confidence: 0.891060359306347
[00:21:52.442] Server  ERROR   Backend returned 400
[00:21:52.462] Server  ERROR   Backend returned 400
[00:21:52.475] Server  ERROR   Backend returned 400
[00:21:52.580] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:52.581] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.01686566970115544
[00:21:52.581] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:52.581] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1047262729927787
[00:21:52.581] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:52.581] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3567935202636167
[00:21:52.936] Server  ERROR   Backend returned 400
[00:21:52.953] Server  ERROR   Backend returned 400
[00:21:52.973] Server  ERROR   Backend returned 400
[00:21:53.077] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:53.077] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3569084607799745
[00:21:53.077] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:53.077] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.945932847821414
[00:21:53.077] Browser LOG     Panic detected at: Main Hall Confidence: 0.945932847821414
[00:21:53.077] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:53.077] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.06506154038851797
[00:21:53.438] Server  ERROR   Backend returned 400
[00:21:53.453] Server  ERROR   Backend returned 400
[00:21:53.469] Server  ERROR   Backend returned 400
[00:21:53.575] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:53.575] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.039727092919427553
[00:21:53.575] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:53.575] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8029187897370019
[00:21:53.575] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:53.575] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8816991518200477
[00:21:53.939] Server  ERROR   Backend returned 400
[00:21:53.960] Server  ERROR   Backend returned 400
[00:21:53.974] Server  ERROR   Backend returned 400
[00:21:54.088] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:54.088] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.07990517912730716
[00:21:54.088] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:54.088] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.13950958164274868
[00:21:54.088] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:54.088] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.33823934890239526
[00:21:54.436] Server  ERROR   Backend returned 400
[00:21:54.457] Server  ERROR   Backend returned 400
[00:21:54.468] Server  ERROR   Backend returned 400
[00:21:54.573] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:54.573] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7174428706706587
[00:21:54.573] Browser LOG     Panic detected at: Gate 1 Entrance Confidence: 0.7174428706706587
[00:21:54.573] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:54.574] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9991604098333791
[00:21:54.574] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:54.574] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.20797174629705611
[00:21:54.935] Server  ERROR   Backend returned 400
[00:21:54.959] Server  ERROR   Backend returned 400
[00:21:54.972] Server  ERROR   Backend returned 400
[00:21:55.076] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:55.076] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9505244103187378
[00:21:55.076] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:55.076] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.06651336236102429
[00:21:55.076] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:55.076] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.18694782744739918
[00:21:55.439] Server  ERROR   Backend returned 400
[00:21:55.462] Server  ERROR   Backend returned 400
[00:21:55.476] Server  ERROR   Backend returned 400
[00:21:55.581] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:55.581] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9860630625447425
[00:21:55.581] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:55.581] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.30787960893368477
[00:21:55.581] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:55.581] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.34844210630424105
[00:21:55.935] Server  ERROR   Backend returned 400
[00:21:55.951] Server  ERROR   Backend returned 400
[00:21:55.967] Server  ERROR   Backend returned 400
[00:21:56.072] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:56.072] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1952733438641296
[00:21:56.072] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:56.072] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2444993873241192
[00:21:56.072] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:56.072] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.06381703278145229
[00:21:56.436] Server  ERROR   Backend returned 400
[00:21:56.452] Server  ERROR   Backend returned 400
[00:21:56.465] Server  ERROR   Backend returned 400
[00:21:56.569] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:56.569] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9166236961908941
[00:21:56.569] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:56.569] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.02252896071919952
[00:21:56.569] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:56.569] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8840883481147979
[00:21:56.569] Browser LOG     Panic detected at: Parking Area Confidence: 0.8840883481147979
[00:21:56.934] Server  ERROR   Backend returned 400
[00:21:56.959] Server  ERROR   Backend returned 400
[00:21:56.973] Server  ERROR   Backend returned 400
[00:21:57.077] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:57.077] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.958163330970924
[00:21:57.077] Browser LOG     Panic detected at: Gate 1 Entrance Confidence: 0.958163330970924
[00:21:57.077] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:57.077] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.05090813006666517
[00:21:57.077] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:57.077] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.37622496819590845
[00:21:57.435] Server  ERROR   Backend returned 400
[00:21:57.459] Server  ERROR   Backend returned 400
[00:21:57.472] Server  ERROR   Backend returned 400
[00:21:57.582] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:57.582] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.17193668044084004
[00:21:57.582] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:57.582] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1701235397111474
[00:21:57.582] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:57.582] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3617616783023212
[00:21:57.951] Server  ERROR   Backend returned 400
[00:21:57.984] Server  ERROR   Backend returned 400
[00:21:58.006] Server  ERROR   Backend returned 400
[00:21:58.111] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:58.111] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7967724336794191
[00:21:58.111] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:58.111] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2189231772123426
[00:21:58.111] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:58.111] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7191504621933735
[00:21:58.434] Server  ERROR   Backend returned 400
[00:21:58.471] Server  ERROR   Backend returned 400
[00:21:58.474] Server  ERROR   Backend returned 400
[00:21:58.582] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:58.582] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.22127026305501818
[00:21:58.582] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:58.582] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3735840916461835
[00:21:58.582] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:58.582] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4215951412127575
[00:21:58.946] Server  ERROR   Backend returned 400
[00:21:58.965] Server  ERROR   Backend returned 400
[00:21:58.977] Server  ERROR   Backend returned 400
[00:21:59.081] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:59.081] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.44401259559694517
[00:21:59.081] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:59.081] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.42620766914099795
[00:21:59.081] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:59.081] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.810116237354007
[00:21:59.081] Browser LOG     Panic detected at: Parking Area Confidence: 0.810116237354007
[00:21:59.435] Server  ERROR   Backend returned 400
[00:21:59.455] Server  ERROR   Backend returned 400
[00:21:59.469] Server  ERROR   Backend returned 400
[00:21:59.575] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:59.575] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9582738598331138
[00:21:59.575] Browser LOG     Panic detected at: Gate 1 Entrance Confidence: 0.9582738598331138
[00:21:59.575] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:59.575] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.24156746359120074
[00:21:59.575] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:21:59.575] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4854016374015107
[00:21:59.933] Server  ERROR   Backend returned 400
[00:21:59.953] Server  ERROR   Backend returned 400
[00:21:59.969] Server  ERROR   Backend returned 400
[00:22:00.075] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:00.075] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7568713254992515
[00:22:00.075] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:00.075] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.008410605378372848
[00:22:00.075] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:00.075] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9601870856027026
[00:22:00.433] Server  ERROR   Backend returned 400
[00:22:00.454] Server  ERROR   Backend returned 400
[00:22:00.466] Server  ERROR   Backend returned 400
[00:22:00.570] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:00.570] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1846056052123608
[00:22:00.570] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:00.570] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.30740826455564296
[00:22:00.570] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:00.570] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7183953209610568
[00:22:00.938] Server  ERROR   Backend returned 400
[00:22:00.953] Server  ERROR   Backend returned 400
[00:22:00.970] Server  ERROR   Backend returned 400
[00:22:01.076] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:01.076] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8286749232530989
[00:22:01.076] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:01.076] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7463570774811439
[00:22:01.076] Browser LOG     Panic detected at: Main Hall Confidence: 0.7463570774811439
[00:22:01.076] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:01.076] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.0912752935618294
[00:22:01.433] Server  ERROR   Backend returned 400
[00:22:01.460] Server  ERROR   Backend returned 400
[00:22:01.476] Server  ERROR   Backend returned 400
[00:22:01.592] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:01.592] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.19843844024282137
[00:22:01.592] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:01.592] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.0018844014978842138
[00:22:01.592] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:01.592] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.22886436084488393
[00:22:01.932] Server  ERROR   Backend returned 400
[00:22:01.953] Server  ERROR   Backend returned 400
[00:22:01.970] Server  ERROR   Backend returned 400
[00:22:02.076] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:02.076] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8461662694438539
[00:22:02.076] Browser LOG     Panic detected at: Gate 1 Entrance Confidence: 0.8461662694438539
[00:22:02.076] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:02.076] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9084085979704285
[00:22:02.076] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:02.076] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.08677622870805085
[00:22:02.437] Server  ERROR   Backend returned 400
[00:22:02.459] Server  ERROR   Backend returned 400
[00:22:02.475] Server  ERROR   Backend returned 400
[00:22:02.580] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:02.580] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.12891899107472155
[00:22:02.580] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:02.580] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4894491821867835
[00:22:02.580] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:02.580] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.13149900497548206
[00:22:02.934] Server  ERROR   Backend returned 400
[00:22:02.959] Server  ERROR   Backend returned 400
[00:22:02.974] Server  ERROR   Backend returned 400
[00:22:03.085] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:03.085] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.39507653186680486
[00:22:03.085] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:03.085] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.19496251110804363
[00:22:03.085] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:03.085] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.16023342608292823
[00:22:03.435] Server  ERROR   Backend returned 400
[00:22:03.461] Server  ERROR   Backend returned 400
[00:22:03.486] Server  ERROR   Backend returned 400
[00:22:03.591] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:03.591] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9442710748840851
[00:22:03.591] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:03.591] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2917264045498906
[00:22:03.591] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:03.591] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8758227641471339
[00:22:03.591] Browser LOG     Panic detected at: Parking Area Confidence: 0.8758227641471339
[00:22:03.940] Server  ERROR   Backend returned 400
[00:22:03.966] Server  ERROR   Backend returned 400
[00:22:03.987] Server  ERROR   Backend returned 400
[00:22:04.090] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:04.090] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2590190867746304
[00:22:04.090] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:04.090] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7142792855541168
[00:22:04.090] Browser LOG     Panic detected at: Main Hall Confidence: 0.7142792855541168
[00:22:04.090] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:04.090] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7888527425669801
[00:22:04.444] Server  ERROR   Backend returned 400
[00:22:04.454] Server  ERROR   Backend returned 400
[00:22:04.473] Server  ERROR   Backend returned 400
[00:22:04.576] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:04.576] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.05961217246526013
[00:22:04.576] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:04.576] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8617892459525283
[00:22:04.576] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:04.576] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7281239097446148
[00:22:04.946] Server  ERROR   Backend returned 400
[00:22:04.964] Server  ERROR   Backend returned 400
[00:22:04.978] Server  ERROR   Backend returned 400
[00:22:05.085] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:05.085] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.09771160175861243
[00:22:05.085] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:05.085] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2985230241288457
[00:22:05.085] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:05.085] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3305239415110594
[00:22:05.433] Server  ERROR   Backend returned 400
[00:22:05.461] Server  ERROR   Backend returned 400
[00:22:05.476] Server  ERROR   Backend returned 400
[00:22:05.579] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:05.580] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.04047569736340567
[00:22:05.580] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:05.580] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8568535092914693
[00:22:05.580] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:05.580] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.41359286266940265
[00:22:05.953] Server  ERROR   Backend returned 400
[00:22:05.967] Server  ERROR   Backend returned 400
[00:22:05.972] Server  ERROR   Backend returned 400
[00:22:06.081] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:06.081] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.21225923898880678
[00:22:06.081] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:06.081] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7825791821448419
[00:22:06.081] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:06.081] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4380429922696612
[00:22:06.444] Server  ERROR   Backend returned 400
[00:22:06.458] Server  ERROR   Backend returned 400
[00:22:06.476] Server  ERROR   Backend returned 400
[00:22:06.581] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:06.581] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7693296999579733
[00:22:06.581] Browser LOG     Panic detected at: Gate 1 Entrance Confidence: 0.7693296999579733
[00:22:06.581] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:06.581] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.14203293989077953
[00:22:06.581] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:06.581] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8595279417217632
[00:22:06.581] Browser LOG     Panic detected at: Parking Area Confidence: 0.8595279417217632
[00:22:06.950] Server  ERROR   Backend returned 400
[00:22:06.966] Server  ERROR   Backend returned 400
[00:22:06.980] Server  ERROR   Backend returned 400
[00:22:07.083] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:07.083] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3254793693915171
[00:22:07.083] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:07.083] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.12999161167532053
[00:22:07.083] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:07.083] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.48034281046175653
[00:22:07.443] Server  ERROR   Backend returned 400
[00:22:07.461] Server  ERROR   Backend returned 400
[00:22:07.475] Server  ERROR   Backend returned 400
[00:22:07.579] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:07.579] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.34306647646705885
[00:22:07.579] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:07.579] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3042534868325936
[00:22:07.579] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:07.579] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.32408052139183796
[00:22:07.940] Server  ERROR   Backend returned 400
[00:22:07.958] Server  ERROR   Backend returned 400
[00:22:07.975] Server  ERROR   Backend returned 400
[00:22:08.080] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:08.080] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.025158948636321565
[00:22:08.080] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:08.080] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3883182444702874
[00:22:08.080] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:08.080] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.15352670385587525
[00:22:08.445] Server  ERROR   Backend returned 400
[00:22:08.466] Server  ERROR   Backend returned 400
[00:22:08.480] Server  ERROR   Backend returned 400
[00:22:08.584] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:08.584] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9007318224171369
[00:22:08.584] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:08.584] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4794948832020197
[00:22:08.584] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:08.584] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8911034164390239
[00:22:08.584] Browser LOG     Panic detected at: Parking Area Confidence: 0.8911034164390239
[00:22:08.933] Server  ERROR   Backend returned 400
[00:22:08.954] Server  ERROR   Backend returned 400
[00:22:08.972] Server  ERROR   Backend returned 400
[00:22:09.076] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:09.076] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.03727310911330345
[00:22:09.076] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:09.076] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3387516195247521
[00:22:09.076] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:09.076] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.06608950498429339
[00:22:09.436] Server  ERROR   Backend returned 400
[00:22:09.454] Server  ERROR   Backend returned 400
[00:22:09.474] Server  ERROR   Backend returned 400
[00:22:09.579] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:09.579] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8839811606515131
[00:22:09.579] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:09.579] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4493674744324015
[00:22:09.579] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:09.579] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8667708194083003
[00:22:09.579] Browser LOG     Panic detected at: Main Hall Confidence: 0.8667708194083003
[00:22:09.935] Server  ERROR   Backend returned 400
[00:22:09.955] Server  ERROR   Backend returned 400
[00:22:09.968] Server  ERROR   Backend returned 400
[00:22:10.072] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:10.072] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.42603289500684227
[00:22:10.072] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:10.072] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.05453578068855491
[00:22:10.072] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:10.072] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.05774610551924503
[00:22:10.432] Server  ERROR   Backend returned 400
[00:22:10.463] Server  ERROR   Backend returned 400
[00:22:10.480] Server  ERROR   Backend returned 400
[00:22:10.626] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:10.628] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7953511886144354
[00:22:10.628] Browser LOG     Panic detected at: Gate 1 Entrance Confidence: 0.7953511886144354
[00:22:10.628] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:10.628] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.36784645960851475
[00:22:10.628] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:10.628] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4771423072457304
[00:22:10.934] Server  ERROR   Backend returned 400
[00:22:10.962] Server  ERROR   Backend returned 400
[00:22:10.975] Server  ERROR   Backend returned 400
[00:22:11.079] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:11.079] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3908914879682248
[00:22:11.079] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:11.079] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9692398502575491
[00:22:11.079] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:11.079] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.14381113607664292
[00:22:11.439] Server  ERROR   Backend returned 400
[00:22:11.459] Server  ERROR   Backend returned 400
[00:22:11.473] Server  ERROR   Backend returned 400
[00:22:11.579] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:11.579] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.0777061087517642
[00:22:11.579] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:11.579] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8823751856411672
[00:22:11.579] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:11.579] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8458796210885241
[00:22:11.579] Browser LOG     Panic detected at: Parking Area Confidence: 0.8458796210885241
[00:22:11.932] Server  ERROR   Backend returned 400
[00:22:11.959] Server  ERROR   Backend returned 400
[00:22:11.961] Server  ERROR   Backend returned 400
[00:22:12.072] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:12.072] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.36336814635198583
[00:22:12.072] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:12.072] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.35066950114475415
[00:22:12.072] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:12.072] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3974563751892925
[00:22:12.440] Server  ERROR   Backend returned 400
[00:22:12.467] Server  ERROR   Backend returned 400
[00:22:12.472] Server  ERROR   Backend returned 400
[00:22:12.580] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:12.580] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9997572579710143
[00:22:12.580] Browser LOG     Panic detected at: Gate 1 Entrance Confidence: 0.9997572579710143
[00:22:12.580] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:12.580] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8804344144655827
[00:22:12.580] Browser LOG     Panic detected at: Main Hall Confidence: 0.8804344144655827
[00:22:12.580] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:12.580] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3531517865716682
[00:22:12.934] Server  ERROR   Backend returned 400
[00:22:12.957] Server  ERROR   Backend returned 400
[00:22:12.970] Server  ERROR   Backend returned 400
[00:22:13.076] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:13.076] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.29852510019169726
[00:22:13.076] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:13.076] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.0276849923220972
[00:22:13.076] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:13.076] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9524858795918867
[00:22:13.439] Server  ERROR   Backend returned 400
[00:22:13.472] Server  ERROR   Backend returned 400
[00:22:13.474] Server  ERROR   Backend returned 400
[00:22:13.583] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:13.583] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8714135518023907
[00:22:13.583] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:13.583] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.07687831580397525
[00:22:13.583] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:13.583] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4210795266086712
[00:22:13.940] Server  ERROR   Backend returned 400
[00:22:13.960] Server  ERROR   Backend returned 400
[00:22:13.973] Server  ERROR   Backend returned 400
[00:22:14.085] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:14.085] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.24542974198218365
[00:22:14.085] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:14.085] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4914857758636851
[00:22:14.085] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:14.085] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3236065305023266
[00:22:14.437] Server  ERROR   Backend returned 400
[00:22:14.462] Server  ERROR   Backend returned 400
[00:22:14.473] Server  ERROR   Backend returned 400
[00:22:14.577] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:14.578] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9871612541019814
[00:22:14.578] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:14.578] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7629925602576008
[00:22:14.578] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:14.578] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.17113731815661504
[00:22:14.931] Server  ERROR   Backend returned 400
[00:22:14.960] Server  ERROR   Backend returned 400
[00:22:14.974] Server  ERROR   Backend returned 400
[00:22:15.078] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:15.078] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.16834325419958496
[00:22:15.078] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:15.078] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9211060091591357
[00:22:15.078] Browser LOG     Panic detected at: Main Hall Confidence: 0.9211060091591357
[00:22:15.078] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:15.078] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8158067908092254
[00:22:15.078] Browser LOG     Panic detected at: Parking Area Confidence: 0.8158067908092254
[00:22:15.432] Server  ERROR   Backend returned 400
[00:22:15.464] Server  ERROR   Backend returned 400
[00:22:15.467] Server  ERROR   Backend returned 400
[00:22:15.591] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:15.591] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.0173383296040745
[00:22:15.591] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:15.591] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3650543795955202
[00:22:15.591] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:15.591] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3388943984164366
[00:22:15.931] Server  ERROR   Backend returned 400
[00:22:15.966] Server  ERROR   Backend returned 400
[00:22:15.970] Server  ERROR   Backend returned 400
[00:22:16.087] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:16.087] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.923979399629903
[00:22:16.087] Browser LOG     Panic detected at: Gate 1 Entrance Confidence: 0.923979399629903
[00:22:16.087] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:16.087] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.13651598469367177
[00:22:16.087] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:16.087] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.23448790299147126
[00:22:16.440] Server  ERROR   Backend returned 400
[00:22:16.486] Server  ERROR   Backend returned 400
[00:22:16.508] Server  ERROR   Backend returned 400
[00:22:16.614] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:16.614] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.18148113851744746
[00:22:16.614] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:16.614] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9047547948202166
[00:22:16.614] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:16.614] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8101563494838601
[00:22:16.951] Server  ERROR   Backend returned 400
[00:22:17.028] Server  ERROR   Backend returned 400
[00:22:17.035] Server  ERROR   Backend returned 400
[00:22:17.155] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:17.156] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.13312242204128616
[00:22:17.156] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:17.156] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.022978357517612014
[00:22:17.156] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:17.156] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.28309320716989517
[00:22:17.441] Server  ERROR   Backend returned 400
[00:22:17.474] Server  ERROR   Backend returned 400
[00:22:17.490] Server  ERROR   Backend returned 400
[00:22:17.594] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:17.594] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3885243520352609
[00:22:17.594] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:17.594] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.08071772985059578
[00:22:17.594] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:17.594] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.782254421130362
[00:22:17.594] Browser LOG     Panic detected at: Parking Area Confidence: 0.782254421130362
[00:22:17.932] Server  ERROR   Backend returned 400
[00:22:17.971] Server  ERROR   Backend returned 400
[00:22:17.974] Server  ERROR   Backend returned 400
[00:22:18.083] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:18.083] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8495409536250262
[00:22:18.083] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:18.083] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.07079724776816326
[00:22:18.083] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:18.083] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.09946084201642763
[00:22:18.445] Server  ERROR   Backend returned 400
[00:22:18.486] Server  ERROR   Backend returned 400
[00:22:18.488] Server  ERROR   Backend returned 400
[00:22:18.596] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:18.596] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.041878912566404136
[00:22:18.596] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:18.596] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3511652410644754
[00:22:18.596] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:18.596] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3198656824135503
[00:22:18.941] Server  ERROR   Backend returned 400
[00:22:18.962] Server  ERROR   Backend returned 400
[00:22:18.986] Server  ERROR   Backend returned 400
[00:22:19.090] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:19.090] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7075040146177618
[00:22:19.090] Browser LOG     Panic detected at: Gate 1 Entrance Confidence: 0.7075040146177618
[00:22:19.090] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:19.090] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8932791059800325
[00:22:19.090] Browser LOG     Panic detected at: Main Hall Confidence: 0.8932791059800325
[00:22:19.090] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:19.090] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.050608373629570436
[00:22:19.432] Server  ERROR   Backend returned 400
[00:22:19.478] Server  ERROR   Backend returned 400
[00:22:19.479] Server  ERROR   Backend returned 400
[00:22:19.589] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:19.589] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.44790702778341307
[00:22:19.589] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:19.589] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7481075735313801
[00:22:19.589] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:19.589] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2902142656300077
[00:22:19.944] Server  ERROR   Backend returned 400
[00:22:19.974] Server  ERROR   Backend returned 400
[00:22:19.992] Server  ERROR   Backend returned 400
[00:22:20.106] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:20.106] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8131829016025423
[00:22:20.106] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:20.106] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.09130343206730701
[00:22:20.106] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:20.106] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1800014564100189
[00:22:20.451] Server  ERROR   Backend returned 400
[00:22:20.485] Server  ERROR   Backend returned 400
[00:22:20.499] Server  ERROR   Backend returned 400
[00:22:20.603] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:20.603] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.16898779954289633
[00:22:20.603] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:20.603] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.04401715757041236
[00:22:20.603] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:20.603] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9003832700163784
[00:22:20.603] Browser LOG     Panic detected at: Parking Area Confidence: 0.9003832700163784
[00:22:20.936] Server  ERROR   Backend returned 400
[00:22:20.953] Server  ERROR   Backend returned 400
[00:22:20.978] Server  ERROR   Backend returned 400
[00:22:21.084] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:21.084] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2158596456974498
[00:22:21.084] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:21.084] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.42774379699132437
[00:22:21.084] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:21.084] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2263921596412788
[00:22:21.448] Server  ERROR   Backend returned 400
[00:22:21.466] Server  ERROR   Backend returned 400
[00:22:21.506] Server  ERROR   Backend returned 400
[00:22:21.611] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:21.611] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.09404392213525958
[00:22:21.611] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:21.611] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4579525756001258
[00:22:21.611] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:21.611] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.003793429671069215
[00:22:21.951] Server  ERROR   Backend returned 400
[00:22:22.008] Server  ERROR   Backend returned 400
[00:22:22.011] Server  ERROR   Backend returned 400
[00:22:22.121] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:22.121] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4116714474040644
[00:22:22.121] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:22.121] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.40508603826594075
[00:22:22.121] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:22.121] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8312536920078537
[00:22:22.447] Server  ERROR   Backend returned 400
[00:22:22.467] Server  ERROR   Backend returned 400
[00:22:22.493] Server  ERROR   Backend returned 400
[00:22:22.609] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:22.609] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.43939794960222267
[00:22:22.609] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:22.609] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9416268596329336
[00:22:22.609] Browser LOG     Panic detected at: Main Hall Confidence: 0.9416268596329336
[00:22:22.609] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:22.609] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7172766407945286
[00:22:22.939] Server  ERROR   Backend returned 400
[00:22:22.978] Server  ERROR   Backend returned 400
[00:22:22.980] Server  ERROR   Backend returned 400
[00:22:23.069] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:23.069] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9886560139757092
[00:22:23.069] Browser LOG     Panic detected at: Gate 1 Entrance Confidence: 0.9886560139757092
[00:22:23.069] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:23.069] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2250748846508796
[00:22:23.069] Browser WARN    âš ï¸ Inference API failed, status: 400 using simulation
[00:22:23.069] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8003631855822603
[00:22:23.069] Browser LOG     Panic detected at: Parking Area Confidence: 0.8003631855822603
[00:22:23.968] Browser INFO    %cDownload the React DevTools for a better development experience: https://react.dev/link/react-devtools font-weight:bold
[00:22:24.263] Browser LOG     Starting inference for videos: [1,2,3]
[00:22:24.263] Browser LOG     Starting inference for video: 1 Gate 1 Entrance
[00:22:24.263] Browser LOG     Starting inference for video: 2 Main Hall
[00:22:24.263] Browser LOG     Starting inference for video: 3 Parking Area
[00:22:24.263] Browser LOG     %c[Vercel Web Analytics]%c Debug mode is enabled by default in development. No requests will be sent to the server. color: rgb(120, 120, 120) color: inherit
[00:22:24.263] Browser LOG     %c[Vercel Web Analytics]%c Running queued event color: rgb(120, 120, 120) color: inherit pageview {"path":"/admin/monitoring","route":"/admin/monitoring"}
[00:22:24.263] Browser LOG     %c[Vercel Web Analytics]%c [pageview] http://localhost:3000/admin/monitoring color: rgb(120, 120, 120) color: inherit {"dp":"/admin/monitoring","o":"http://localhost:3000/admin/monitoring","sdkn":"@vercel/analytics/next","sdkv":"1.5.0","sv":"0.1.3","ts":1763753215388}
[00:22:24.647] Server  ERROR   Backend returned 400
[00:22:24.700] Server  ERROR   Backend returned 400
[00:22:24.718] Server  ERROR   Backend returned 400
[00:22:24.823] Browser WARN    Inference API failed, using simulation 400
[00:22:24.823] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2103718423002655
[00:22:24.823] Browser WARN    Inference API failed, using simulation 400
[00:22:24.823] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4922473744752174
[00:22:24.823] Browser WARN    Inference API failed, using simulation 400
[00:22:24.823] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4822075778885942
[00:22:25.139] Server  ERROR   Backend returned 400
[00:22:25.159] Server  ERROR   Backend returned 400
[00:22:25.180] Server  ERROR   Backend returned 400
[00:22:25.286] Browser WARN    Inference API failed, using simulation 400
[00:22:25.286] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8061296019899532
[00:22:25.286] Browser LOG     Panic detected at: Gate 1 Entrance Count in last 15s: 1
[00:22:25.286] Browser WARN    Inference API failed, using simulation 400
[00:22:25.286] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9541564747866468
[00:22:25.286] Browser LOG     Panic detected at: Main Hall Count in last 15s: 2
[00:22:25.286] Browser WARN    Inference API failed, using simulation 400
[00:22:25.286] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7363912514777514
[00:22:25.286] Browser LOG     Panic detected at: Parking Area Count in last 15s: 3
[00:22:25.839] Server  ERROR   Backend returned 400
[00:22:25.867] Server  ERROR   Backend returned 400
[00:22:25.869] Server  ERROR   Backend returned 400
[00:22:25.982] Browser WARN    Inference API failed, using simulation 400
[00:22:25.982] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2535630795904333
[00:22:25.982] Browser WARN    Inference API failed, using simulation 400
[00:22:25.982] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.09422655428376325
[00:22:25.982] Browser WARN    Inference API failed, using simulation 400
[00:22:25.982] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9376507521457317
[00:22:26.144] Server  ERROR   Backend returned 400
[00:22:26.174] Server  ERROR   Backend returned 400
[00:22:26.176] Server  ERROR   Backend returned 400
[00:22:26.297] Browser WARN    Inference API failed, using simulation 400
[00:22:26.297] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7377431645938575
[00:22:26.297] Browser WARN    Inference API failed, using simulation 400
[00:22:26.297] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7815322153377758
[00:22:26.297] Browser WARN    Inference API failed, using simulation 400
[00:22:26.297] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.007090234146486185
[00:22:26.652] Server  ERROR   Backend returned 400
[00:22:26.672] Server  ERROR   Backend returned 400
[00:22:26.689] Server  ERROR   Backend returned 400
[00:22:26.795] Browser WARN    Inference API failed, using simulation 400
[00:22:26.795] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2202012375716096
[00:22:26.795] Browser WARN    Inference API failed, using simulation 400
[00:22:26.795] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.15954579498483418
[00:22:26.795] Browser WARN    Inference API failed, using simulation 400
[00:22:26.795] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9932356414352145
[00:22:27.198] Server  ERROR   Backend returned 400
[00:22:27.221] Server  ERROR   Backend returned 400
[00:22:27.251] Server  ERROR   Backend returned 400
[00:22:27.360] Browser WARN    Inference API failed, using simulation 400
[00:22:27.360] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3220935992759537
[00:22:27.360] Browser WARN    Inference API failed, using simulation 400
[00:22:27.360] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.772748241946364
[00:22:27.360] Browser LOG     Panic detected at: Main Hall Count in last 15s: 4
[00:22:27.360] Browser WARN    Inference API failed, using simulation 400
[00:22:27.360] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2329109879763141
[00:22:27.650] Server  ERROR   Backend returned 400
[00:22:27.668] Server  ERROR   Backend returned 400
[00:22:27.689] Server  ERROR   Backend returned 400
[00:22:27.795] Browser WARN    Inference API failed, using simulation 400
[00:22:27.795] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.11949185242794019
[00:22:27.795] Browser WARN    Inference API failed, using simulation 400
[00:22:27.795] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8521787736065983
[00:22:27.795] Browser WARN    Inference API failed, using simulation 400
[00:22:27.795] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9571977463165569
[00:22:27.795] Browser LOG     Panic detected at: Parking Area Count in last 15s: 5
[00:22:28.145] Server  ERROR   Backend returned 400
[00:22:28.161] Server  ERROR   Backend returned 400
[00:22:28.181] Server  ERROR   Backend returned 400
[00:22:28.289] Browser WARN    Inference API failed, using simulation 400
[00:22:28.290] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.14062211590660417
[00:22:28.290] Browser WARN    Inference API failed, using simulation 400
[00:22:28.290] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3297741962331356
[00:22:28.290] Browser WARN    Inference API failed, using simulation 400
[00:22:28.290] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4333137612424148
[00:22:28.650] Server  ERROR   Backend returned 400
[00:22:28.678] Server  ERROR   Backend returned 400
[00:22:28.699] Server  ERROR   Backend returned 400
[00:22:28.805] Browser WARN    Inference API failed, using simulation 400
[00:22:28.805] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.04982087427616122
[00:22:28.805] Browser WARN    Inference API failed, using simulation 400
[00:22:28.805] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.703712999372474
[00:22:28.805] Browser WARN    Inference API failed, using simulation 400
[00:22:28.805] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3652542089234913
[00:22:29.147] Server  ERROR   Backend returned 400
[00:22:29.164] Server  ERROR   Backend returned 400
[00:22:29.187] Server  ERROR   Backend returned 400
[00:22:29.320] Browser WARN    Inference API failed, using simulation 400
[00:22:29.320] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8973995124634031
[00:22:29.320] Browser LOG     Panic detected at: Gate 1 Entrance Count in last 15s: 6
[00:22:29.320] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Gate 1 Entrance","Main Hall","Parking Area","Main Hall","Parking Area","Gate 1 Entrance"]
[00:22:29.320] Browser WARN    Inference API failed, using simulation 400
[00:22:29.320] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.0859501536406514
[00:22:29.320] Browser WARN    Inference API failed, using simulation 400
[00:22:29.320] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.801679680622214
[00:22:29.661] Server  ERROR   Backend returned 400
[00:22:29.688] Server  ERROR   Backend returned 400
[00:22:29.719] Server  ERROR   Backend returned 400
[00:22:29.825] Browser WARN    Inference API failed, using simulation 400
[00:22:29.825] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2936447810138711
[00:22:29.825] Browser WARN    Inference API failed, using simulation 400
[00:22:29.825] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.23321885296231326
[00:22:29.825] Browser WARN    Inference API failed, using simulation 400
[00:22:29.825] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8020909391957181
[00:22:29.825] Browser LOG     Panic detected at: Parking Area Count in last 15s: 7
[00:22:29.825] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Gate 1 Entrance","Main Hall","Parking Area","Main Hall","Parking Area","Gate 1 Entrance","Parking Area"]
[00:22:30.153] Server  ERROR   Backend returned 400
[00:22:30.176] Server  ERROR   Backend returned 400
[00:22:30.190] Server  ERROR   Backend returned 400
[00:22:30.297] Browser WARN    Inference API failed, using simulation 400
[00:22:30.297] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.49691245228067943
[00:22:30.297] Browser WARN    Inference API failed, using simulation 400
[00:22:30.297] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.483235024856571
[00:22:30.297] Browser WARN    Inference API failed, using simulation 400
[00:22:30.297] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.11910088209561365
[00:22:30.665] Server  ERROR   Backend returned 400
[00:22:30.688] Server  ERROR   Backend returned 400
[00:22:30.713] Server  ERROR   Backend returned 400
[00:22:30.819] Browser WARN    Inference API failed, using simulation 400
[00:22:30.819] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.958696293097967
[00:22:30.819] Browser WARN    Inference API failed, using simulation 400
[00:22:30.819] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7554493630565335
[00:22:30.819] Browser LOG     Panic detected at: Main Hall Count in last 15s: 8
[00:22:30.819] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Gate 1 Entrance","Main Hall","Parking Area","Main Hall","Parking Area","Gate 1 Entrance","Parking Area","Main Hall"]
[00:22:30.819] Browser WARN    Inference API failed, using simulation 400
[00:22:30.819] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.09462011217567512
[00:22:31.149] Server  ERROR   Backend returned 400
[00:22:31.176] Server  ERROR   Backend returned 400
[00:22:31.201] Server  ERROR   Backend returned 400
[00:22:31.306] Browser WARN    Inference API failed, using simulation 400
[00:22:31.306] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.24931655403810715
[00:22:31.306] Browser WARN    Inference API failed, using simulation 400
[00:22:31.306] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.04588207211233353
[00:22:31.306] Browser WARN    Inference API failed, using simulation 400
[00:22:31.306] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.0557411673370376
[00:22:32.209] Server  ERROR   Backend returned 400
[00:22:32.225] Server  ERROR   Backend returned 400
[00:22:32.243] Server  ERROR   Backend returned 400
[00:22:33.176] Browser WARN    Inference API failed, using simulation 400
[00:22:33.176] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.07003698125396607
[00:22:33.176] Browser WARN    Inference API failed, using simulation 400
[00:22:33.176] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.09905314571503604
[00:22:33.176] Browser WARN    Inference API failed, using simulation 400
[00:22:33.176] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.19756760603756673
[00:22:33.228] Server  ERROR   Backend returned 400
[00:22:33.245] Server  ERROR   Backend returned 400
[00:22:33.291] Server  ERROR   Backend returned 400
[00:22:34.173] Browser WARN    Inference API failed, using simulation 400
[00:22:34.173] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.14117257195525823
[00:22:34.173] Browser WARN    Inference API failed, using simulation 400
[00:22:34.173] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.14593469722057179
[00:22:34.173] Browser WARN    Inference API failed, using simulation 400
[00:22:34.173] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.26107248925429977
[00:22:34.224] Server  ERROR   Backend returned 400
[00:22:34.247] Server  ERROR   Backend returned 400
[00:22:34.253] Server  ERROR   Backend returned 400
[00:22:35.167] Browser WARN    Inference API failed, using simulation 400
[00:22:35.167] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.22901056556538146
[00:22:35.167] Browser WARN    Inference API failed, using simulation 400
[00:22:35.167] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3867719834670146
[00:22:35.167] Browser WARN    Inference API failed, using simulation 400
[00:22:35.167] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.35172279241361115
[00:22:35.220] Server  ERROR   Backend returned 400
[00:22:35.266] Server  ERROR   Backend returned 400
[00:22:35.283] Server  ERROR   Backend returned 400
[00:22:36.172] Browser WARN    Inference API failed, using simulation 400
[00:22:36.172] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.33177800925522644
[00:22:36.172] Browser WARN    Inference API failed, using simulation 400
[00:22:36.172] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4488768200041746
[00:22:36.172] Browser WARN    Inference API failed, using simulation 400
[00:22:36.172] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2437638075175672
[00:22:36.227] Server  ERROR   Backend returned 400
[00:22:36.253] Server  ERROR   Backend returned 400
[00:22:36.272] Server  ERROR   Backend returned 400
[00:22:37.168] Browser WARN    Inference API failed, using simulation 400
[00:22:37.168] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.08939431815975935
[00:22:37.168] Browser WARN    Inference API failed, using simulation 400
[00:22:37.168] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.09617463455232839
[00:22:37.168] Browser WARN    Inference API failed, using simulation 400
[00:22:37.168] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.17253303433150546
[00:22:37.217] Server  ERROR   Backend returned 400
[00:22:37.252] Server  ERROR   Backend returned 400
[00:22:37.265] Server  ERROR   Backend returned 400
[00:22:38.166] Browser WARN    Inference API failed, using simulation 400
[00:22:38.166] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2244959485606951
[00:22:38.166] Browser WARN    Inference API failed, using simulation 400
[00:22:38.166] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.44732689721123664
[00:22:38.166] Browser WARN    Inference API failed, using simulation 400
[00:22:38.166] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4122198623323123
[00:22:38.222] Server  ERROR   Backend returned 400
[00:22:38.242] Server  ERROR   Backend returned 400
[00:22:38.262] Server  ERROR   Backend returned 400
[00:22:39.168] Browser WARN    Inference API failed, using simulation 400
[00:22:39.168] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7565763066049119
[00:22:39.168] Browser LOG     Panic detected at: Gate 1 Entrance Count in last 15s: 9
[00:22:39.168] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Gate 1 Entrance","Main Hall","Parking Area","Main Hall","Parking Area","Gate 1 Entrance","Parking Area","Main Hall","Gate 1 Entrance"]
[00:22:39.168] Browser WARN    Inference API failed, using simulation 400
[00:22:39.168] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3470918248402212
[00:22:39.168] Browser WARN    Inference API failed, using simulation 400
[00:22:39.168] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4728688368231663
[00:22:39.221] Server  ERROR   Backend returned 400
[00:22:39.244] Server  ERROR   Backend returned 400
[00:22:39.250] Server  ERROR   Backend returned 400
[00:22:40.164] Browser WARN    Inference API failed, using simulation 400
[00:22:40.164] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8405200203241239
[00:22:40.164] Browser WARN    Inference API failed, using simulation 400
[00:22:40.164] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.11122279218050168
[00:22:40.164] Browser WARN    Inference API failed, using simulation 400
[00:22:40.164] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7212851407019856
[00:22:40.164] Browser LOG     Panic detected at: Parking Area Count in last 15s: 10
[00:22:40.164] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Gate 1 Entrance","Main Hall","Parking Area","Main Hall","Parking Area","Gate 1 Entrance","Parking Area","Main Hall","Gate 1 Entrance","Parking Area"]
[00:22:40.218] Server  ERROR   Backend returned 400
[00:22:40.234] Server  ERROR   Backend returned 400
[00:22:40.244] Server  ERROR   Backend returned 400
[00:22:41.175] Browser WARN    Inference API failed, using simulation 400
[00:22:41.175] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.056744242731471606
[00:22:41.175] Browser WARN    Inference API failed, using simulation 400
[00:22:41.175] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.30014292937128
[00:22:41.175] Browser WARN    Inference API failed, using simulation 400
[00:22:41.175] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8808896143802076
[00:22:41.227] Server  ERROR   Backend returned 400
[00:22:41.245] Server  ERROR   Backend returned 400
[00:22:41.257] Server  ERROR   Backend returned 400
[00:22:42.175] Browser WARN    Inference API failed, using simulation 400
[00:22:42.175] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7649076078765027
[00:22:42.175] Browser LOG     Panic detected at: Gate 1 Entrance Count in last 15s: 8
[00:22:42.175] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Main Hall","Parking Area","Gate 1 Entrance","Parking Area","Main Hall","Gate 1 Entrance","Parking Area","Gate 1 Entrance"]
[00:22:42.175] Browser WARN    Inference API failed, using simulation 400
[00:22:42.175] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.014107573513182814
[00:22:42.175] Browser WARN    Inference API failed, using simulation 400
[00:22:42.175] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2076923898812732
[00:22:43.244] Server  ERROR   Backend returned 400
[00:22:43.272] Server  ERROR   Backend returned 400
[00:22:43.275] Server  ERROR   Backend returned 400
[00:22:44.167] Browser WARN    Inference API failed, using simulation 400
[00:22:44.167] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4235374226469506
[00:22:44.167] Browser WARN    Inference API failed, using simulation 400
[00:22:44.167] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.37565725238199293
[00:22:44.167] Browser WARN    Inference API failed, using simulation 400
[00:22:44.167] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.39286943495764226
[00:22:44.208] Server  ERROR   Backend returned 400
[00:22:44.224] Server  ERROR   Backend returned 400
[00:22:44.237] Server  ERROR   Backend returned 400
[00:22:45.163] Browser WARN    Inference API failed, using simulation 400
[00:22:45.163] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2618902192532059
[00:22:45.163] Browser WARN    Inference API failed, using simulation 400
[00:22:45.163] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3366178424976334
[00:22:45.163] Browser WARN    Inference API failed, using simulation 400
[00:22:45.163] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.24468939006541485
[00:22:45.222] Server  ERROR   Backend returned 400
[00:22:45.225] Server  ERROR   Backend returned 400
[00:22:45.241] Server  ERROR   Backend returned 400
[00:22:46.175] Browser WARN    Inference API failed, using simulation 400
[00:22:46.175] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8403794930026126
[00:22:46.175] Browser LOG     Panic detected at: Gate 1 Entrance Count in last 15s: 5
[00:22:46.175] Browser WARN    Inference API failed, using simulation 400
[00:22:46.175] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4237558877002656
[00:22:46.175] Browser WARN    Inference API failed, using simulation 400
[00:22:46.175] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.18053029664175774
[00:22:46.240] Server  ERROR   Backend returned 400
[00:22:46.242] Server  ERROR   Backend returned 400
[00:22:46.248] Server  ERROR   Backend returned 400
[00:22:47.169] Browser WARN    Inference API failed, using simulation 400
[00:22:47.169] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3968583815818433
[00:22:47.169] Browser WARN    Inference API failed, using simulation 400
[00:22:47.169] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.0005047856283373342
[00:22:47.169] Browser WARN    Inference API failed, using simulation 400
[00:22:47.169] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.014746726517950282
[00:22:47.224] Server  ERROR   Backend returned 400
[00:22:47.226] Server  ERROR   Backend returned 400
[00:22:47.240] Server  ERROR   Backend returned 400
[00:22:48.163] Browser WARN    Inference API failed, using simulation 400
[00:22:48.163] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3964604365219354
[00:22:48.163] Browser WARN    Inference API failed, using simulation 400
[00:22:48.163] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7616758106293744
[00:22:48.163] Browser LOG     Panic detected at: Main Hall Count in last 15s: 5
[00:22:48.163] Browser WARN    Inference API failed, using simulation 400
[00:22:48.163] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.015793102355200095
[00:22:48.208] Server  ERROR   Backend returned 400
[00:22:48.228] Server  ERROR   Backend returned 400
[00:22:48.241] Server  ERROR   Backend returned 400
[00:22:49.162] Browser WARN    Inference API failed, using simulation 400
[00:22:49.162] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8939910228881024
[00:22:49.162] Browser LOG     Panic detected at: Gate 1 Entrance Count in last 15s: 6
[00:22:49.162] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Gate 1 Entrance","Parking Area","Gate 1 Entrance","Gate 1 Entrance","Main Hall","Gate 1 Entrance"]
[00:22:49.162] Browser WARN    Inference API failed, using simulation 400
[00:22:49.162] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8787160135203572
[00:22:49.162] Browser WARN    Inference API failed, using simulation 400
[00:22:49.162] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7758569157472223
[00:22:49.162] Browser LOG     Panic detected at: Parking Area Count in last 15s: 7
[00:22:49.162] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Gate 1 Entrance","Parking Area","Gate 1 Entrance","Gate 1 Entrance","Main Hall","Gate 1 Entrance","Parking Area"]
[00:22:49.224] Server  ERROR   Backend returned 400
[00:22:49.242] Server  ERROR   Backend returned 400
[00:22:49.245] Server  ERROR   Backend returned 400
[00:22:50.166] Browser WARN    Inference API failed, using simulation 400
[00:22:50.166] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.213100768545876
[00:22:50.166] Browser WARN    Inference API failed, using simulation 400
[00:22:50.166] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7382253089932261
[00:22:50.166] Browser WARN    Inference API failed, using simulation 400
[00:22:50.166] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3980545542844071
[00:22:50.310] Server  ERROR   Backend returned 400
[00:22:50.345] Server  ERROR   Backend returned 400
[00:22:50.533] Server  ERROR   Backend returned 400
[00:22:51.192] Browser WARN    Inference API failed, using simulation 400
[00:22:51.192] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8191463672577367
[00:22:51.192] Browser WARN    Inference API failed, using simulation 400
[00:22:51.192] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.16630763946074278
[00:22:51.192] Browser WARN    Inference API failed, using simulation 400
[00:22:51.192] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8158168009458445
[00:22:51.261] Server  ERROR   Backend returned 400
[00:22:51.277] Server  ERROR   Backend returned 400
[00:22:51.280] Server  ERROR   Backend returned 400
[00:22:52.162] Browser WARN    Inference API failed, using simulation 400
[00:22:52.162] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9420217760614439
[00:22:52.162] Browser LOG     Panic detected at: Gate 1 Entrance Count in last 15s: 8
[00:22:52.162] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Gate 1 Entrance","Parking Area","Gate 1 Entrance","Gate 1 Entrance","Main Hall","Gate 1 Entrance","Parking Area","Gate 1 Entrance"]
[00:22:52.162] Browser WARN    Inference API failed, using simulation 400
[00:22:52.162] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7709857471452627
[00:22:52.162] Browser LOG     Panic detected at: Main Hall Count in last 15s: 9
[00:22:52.162] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Gate 1 Entrance","Parking Area","Gate 1 Entrance","Gate 1 Entrance","Main Hall","Gate 1 Entrance","Parking Area","Gate 1 Entrance","Main Hall"]
[00:22:52.162] Browser WARN    Inference API failed, using simulation 400
[00:22:52.162] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.41449233558706555
[00:22:52.211] Server  ERROR   Backend returned 400
[00:22:52.234] Server  ERROR   Backend returned 400
[00:22:52.237] Server  ERROR   Backend returned 400
[00:22:53.174] Browser WARN    Inference API failed, using simulation 400
[00:22:53.174] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4398229876006587
[00:22:53.174] Browser WARN    Inference API failed, using simulation 400
[00:22:53.174] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.16832389660382568
[00:22:53.174] Browser WARN    Inference API failed, using simulation 400
[00:22:53.174] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.02647972681271399
[00:22:53.235] Server  ERROR   Backend returned 400
[00:22:53.237] Server  ERROR   Backend returned 400
[00:22:53.251] Server  ERROR   Backend returned 400
[00:22:54.172] Browser WARN    Inference API failed, using simulation 400
[00:22:54.172] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.30480300770681423
[00:22:54.172] Browser WARN    Inference API failed, using simulation 400
[00:22:54.172] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.059894218334890725
[00:22:54.172] Browser WARN    Inference API failed, using simulation 400
[00:22:54.172] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3951613364255978
[00:22:54.226] Server  ERROR   Backend returned 400
[00:22:54.229] Server  ERROR   Backend returned 400
[00:22:54.242] Server  ERROR   Backend returned 400
[00:22:55.171] Browser WARN    Inference API failed, using simulation 400
[00:22:55.171] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3656979606526994
[00:22:55.171] Browser WARN    Inference API failed, using simulation 400
[00:22:55.171] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.012933273490342623
[00:22:55.171] Browser WARN    Inference API failed, using simulation 400
[00:22:55.171] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.29016039993410747
[00:22:55.224] Server  ERROR   Backend returned 400
[00:22:55.229] Server  ERROR   Backend returned 400
[00:22:55.243] Server  ERROR   Backend returned 400
[00:22:56.169] Browser WARN    Inference API failed, using simulation 400
[00:22:56.169] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.36639396041569783
[00:22:56.169] Browser WARN    Inference API failed, using simulation 400
[00:22:56.169] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.14469371765389588
[00:22:56.169] Browser WARN    Inference API failed, using simulation 400
[00:22:56.169] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3243392887995623
[00:22:56.217] Server  ERROR   Backend returned 400
[00:22:56.243] Server  ERROR   Backend returned 400
[00:22:56.245] Server  ERROR   Backend returned 400
[00:22:57.170] Browser WARN    Inference API failed, using simulation 400
[00:22:57.170] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2097090655884718
[00:22:57.170] Browser WARN    Inference API failed, using simulation 400
[00:22:57.170] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.0006833747674331314
[00:22:57.170] Browser WARN    Inference API failed, using simulation 400
[00:22:57.170] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.12373758718735384
[00:22:57.213] Server  ERROR   Backend returned 400
[00:22:57.244] Server  ERROR   Backend returned 400
[00:22:57.247] Server  ERROR   Backend returned 400
[00:22:58.164] Browser WARN    Inference API failed, using simulation 400
[00:22:58.164] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1779832803751702
[00:22:58.164] Browser WARN    Inference API failed, using simulation 400
[00:22:58.164] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4549749215452025
[00:22:58.164] Browser WARN    Inference API failed, using simulation 400
[00:22:58.164] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3543073908547465
[00:22:58.224] Server  ERROR   Backend returned 400
[00:22:58.235] Server  ERROR   Backend returned 400
[00:22:58.248] Server  ERROR   Backend returned 400
[00:22:59.174] Browser WARN    Inference API failed, using simulation 400
[00:22:59.174] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.011922540558190087
[00:22:59.174] Browser WARN    Inference API failed, using simulation 400
[00:22:59.174] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.26722192222395025
[00:22:59.174] Browser WARN    Inference API failed, using simulation 400
[00:22:59.174] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.20215449297684923
[00:22:59.249] Server  ERROR   Backend returned 400
[00:22:59.251] Server  ERROR   Backend returned 400
[00:22:59.257] Server  ERROR   Backend returned 400
[00:23:00.170] Browser WARN    Inference API failed, using simulation 400
[00:23:00.170] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.25484617395941456
[00:23:00.170] Browser WARN    Inference API failed, using simulation 400
[00:23:00.170] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1476633967787082
[00:23:00.170] Browser WARN    Inference API failed, using simulation 400
[00:23:00.170] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.10865539199842172
[00:23:00.227] Server  ERROR   Backend returned 400
[00:23:00.233] Server  ERROR   Backend returned 400
[00:23:00.249] Server  ERROR   Backend returned 400
[00:23:01.171] Browser WARN    Inference API failed, using simulation 400
[00:23:01.171] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.10532021689892479
[00:23:01.171] Browser WARN    Inference API failed, using simulation 400
[00:23:01.171] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.13485785224926888
[00:23:01.171] Browser WARN    Inference API failed, using simulation 400
[00:23:01.171] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8434116347408751
[00:23:01.171] Browser LOG     Panic detected at: Parking Area Count in last 15s: 6
[00:23:01.171] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Main Hall","Gate 1 Entrance","Parking Area","Gate 1 Entrance","Main Hall","Parking Area"]
[00:23:01.230] Server  ERROR   Backend returned 400
[00:23:01.243] Server  ERROR   Backend returned 400
[00:23:01.247] Server  ERROR   Backend returned 400
[00:23:02.166] Browser WARN    Inference API failed, using simulation 400
[00:23:02.166] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2927335730799072
[00:23:02.166] Browser WARN    Inference API failed, using simulation 400
[00:23:02.166] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7874841667906523
[00:23:02.166] Browser LOG     Panic detected at: Main Hall Count in last 15s: 7
[00:23:02.166] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Main Hall","Gate 1 Entrance","Parking Area","Gate 1 Entrance","Main Hall","Parking Area","Main Hall"]
[00:23:02.166] Browser WARN    Inference API failed, using simulation 400
[00:23:02.166] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8552880108438646
[00:23:02.204] Server  ERROR   Backend returned 400
[00:23:02.225] Server  ERROR   Backend returned 400
[00:23:02.237] Server  ERROR   Backend returned 400
[00:23:03.163] Browser WARN    Inference API failed, using simulation 400
[00:23:03.163] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8194479781387272
[00:23:03.163] Browser LOG     Panic detected at: Gate 1 Entrance Count in last 15s: 8
[00:23:03.163] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Main Hall","Gate 1 Entrance","Parking Area","Gate 1 Entrance","Main Hall","Parking Area","Main Hall","Gate 1 Entrance"]
[00:23:03.163] Browser WARN    Inference API failed, using simulation 400
[00:23:03.163] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9540587448831774
[00:23:03.163] Browser WARN    Inference API failed, using simulation 400
[00:23:03.163] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.12817060145246262
[00:23:03.229] Server  ERROR   Backend returned 400
[00:23:03.232] Server  ERROR   Backend returned 400
[00:23:03.244] Server  ERROR   Backend returned 400
[00:23:04.175] Browser WARN    Inference API failed, using simulation 400
[00:23:04.175] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.047175457841703294
[00:23:04.175] Browser WARN    Inference API failed, using simulation 400
[00:23:04.175] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9761207174646434
[00:23:04.175] Browser WARN    Inference API failed, using simulation 400
[00:23:04.175] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.076297773252465
[00:23:04.229] Server  ERROR   Backend returned 400
[00:23:04.242] Server  ERROR   Backend returned 400
[00:23:04.258] Server  ERROR   Backend returned 400
[00:23:05.171] Browser WARN    Inference API failed, using simulation 400
[00:23:05.171] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9729877790334169
[00:23:05.171] Browser WARN    Inference API failed, using simulation 400
[00:23:05.171] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1204319355540634
[00:23:05.171] Browser WARN    Inference API failed, using simulation 400
[00:23:05.171] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.49949480677490216
[00:23:05.236] Server  ERROR   Backend returned 400
[00:23:05.249] Server  ERROR   Backend returned 400
[00:23:05.258] Server  ERROR   Backend returned 400
[00:23:06.166] Browser WARN    Inference API failed, using simulation 400
[00:23:06.166] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2815163644018132
[00:23:06.166] Browser WARN    Inference API failed, using simulation 400
[00:23:06.166] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1322071541330701
[00:23:06.166] Browser WARN    Inference API failed, using simulation 400
[00:23:06.166] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.49328122391953855
[00:23:06.212] Server  ERROR   Backend returned 400
[00:23:06.231] Server  ERROR   Backend returned 400
[00:23:06.247] Server  ERROR   Backend returned 400
[00:23:07.164] Browser WARN    Inference API failed, using simulation 400
[00:23:07.164] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.0009079917914657787
[00:23:07.164] Browser WARN    Inference API failed, using simulation 400
[00:23:07.164] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4443077521505878
[00:23:07.164] Browser WARN    Inference API failed, using simulation 400
[00:23:07.164] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9727768473828089
[00:23:07.164] Browser LOG     Panic detected at: Parking Area Count in last 15s: 6
[00:23:07.164] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Gate 1 Entrance","Main Hall","Parking Area","Main Hall","Gate 1 Entrance","Parking Area"]
[00:23:07.219] Server  ERROR   Backend returned 400
[00:23:07.223] Server  ERROR   Backend returned 400
[00:23:07.237] Server  ERROR   Backend returned 400
[00:23:08.161] Browser WARN    Inference API failed, using simulation 400
[00:23:08.161] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.13127073510002008
[00:23:08.162] Browser WARN    Inference API failed, using simulation 400
[00:23:08.162] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.28793446576154047
[00:23:08.162] Browser WARN    Inference API failed, using simulation 400
[00:23:08.162] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3904887300799322
[00:23:08.215] Server  ERROR   Backend returned 400
[00:23:08.218] Server  ERROR   Backend returned 400
[00:23:08.232] Server  ERROR   Backend returned 400
[00:23:09.171] Browser WARN    Inference API failed, using simulation 400
[00:23:09.171] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.12312594713495323
[00:23:09.171] Browser WARN    Inference API failed, using simulation 400
[00:23:09.171] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7650373674153026
[00:23:09.171] Browser LOG     Panic detected at: Main Hall Count in last 15s: 5
[00:23:09.171] Browser WARN    Inference API failed, using simulation 400
[00:23:09.171] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9976074151102166
[00:23:09.230] Server  ERROR   Backend returned 400
[00:23:09.241] Server  ERROR   Backend returned 400
[00:23:09.247] Server  ERROR   Backend returned 400
[00:23:10.166] Browser WARN    Inference API failed, using simulation 400
[00:23:10.166] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.49400489037725775
[00:23:10.166] Browser WARN    Inference API failed, using simulation 400
[00:23:10.166] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.43418199102143606
[00:23:10.166] Browser WARN    Inference API failed, using simulation 400
[00:23:10.166] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.0013219546360055956
[00:23:10.225] Server  ERROR   Backend returned 400
[00:23:10.250] Server  ERROR   Backend returned 400
[00:23:10.252] Server  ERROR   Backend returned 400
[00:23:11.165] Browser WARN    Inference API failed, using simulation 400
[00:23:11.165] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.06783316986497256
[00:23:11.165] Browser WARN    Inference API failed, using simulation 400
[00:23:11.165] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.46451133133301514
[00:23:11.165] Browser WARN    Inference API failed, using simulation 400
[00:23:11.165] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9675143763150187
[00:23:11.165] Browser LOG     Panic detected at: Parking Area Count in last 15s: 6
[00:23:11.165] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Parking Area","Main Hall","Gate 1 Entrance","Parking Area","Main Hall","Parking Area"]
[00:23:11.229] Server  ERROR   Backend returned 400
[00:23:11.231] Server  ERROR   Backend returned 400
[00:23:11.250] Server  ERROR   Backend returned 400
[00:23:12.175] Browser WARN    Inference API failed, using simulation 400
[00:23:12.175] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.27198258366251826
[00:23:12.175] Browser WARN    Inference API failed, using simulation 400
[00:23:12.175] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.07653201102872176
[00:23:12.175] Browser WARN    Inference API failed, using simulation 400
[00:23:12.175] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.29460746368475105
[00:23:12.251] Server  ERROR   Backend returned 400
[00:23:12.254] Server  ERROR   Backend returned 400
[00:23:12.266] Server  ERROR   Backend returned 400
[00:23:13.172] Browser WARN    Inference API failed, using simulation 400
[00:23:13.172] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9938610997195605
[00:23:13.172] Browser LOG     Panic detected at: Gate 1 Entrance Count in last 15s: 7
[00:23:13.172] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Parking Area","Main Hall","Gate 1 Entrance","Parking Area","Main Hall","Parking Area","Gate 1 Entrance"]
[00:23:13.172] Browser WARN    Inference API failed, using simulation 400
[00:23:13.172] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8343870389654036
[00:23:13.172] Browser LOG     Panic detected at: Main Hall Count in last 15s: 8
[00:23:13.172] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Parking Area","Main Hall","Gate 1 Entrance","Parking Area","Main Hall","Parking Area","Gate 1 Entrance","Main Hall"]
[00:23:13.172] Browser WARN    Inference API failed, using simulation 400
[00:23:13.172] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3021792037969891
[00:23:13.230] Server  ERROR   Backend returned 400
[00:23:13.247] Server  ERROR   Backend returned 400
[00:23:13.263] Server  ERROR   Backend returned 400
[00:23:14.176] Browser WARN    Inference API failed, using simulation 400
[00:23:14.176] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.195254573443291
[00:23:14.176] Browser WARN    Inference API failed, using simulation 400
[00:23:14.176] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2626220821628238
[00:23:14.176] Browser WARN    Inference API failed, using simulation 400
[00:23:14.176] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9114999889930179
[00:23:14.176] Browser LOG     Panic detected at: Parking Area Count in last 15s: 9
[00:23:14.176] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Parking Area","Main Hall","Gate 1 Entrance","Parking Area","Main Hall","Parking Area","Gate 1 Entrance","Main Hall","Parking Area"]
[00:23:14.263] Server  ERROR   Backend returned 400
[00:23:14.265] Server  ERROR   Backend returned 400
[00:23:14.284] Server  ERROR   Backend returned 400
[00:23:15.173] Browser WARN    Inference API failed, using simulation 400
[00:23:15.173] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3624876213456377
[00:23:15.173] Browser WARN    Inference API failed, using simulation 400
[00:23:15.173] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.30460459196518036
[00:23:15.173] Browser WARN    Inference API failed, using simulation 400
[00:23:15.173] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.38319781959016064
[00:23:15.212] Server  ERROR   Backend returned 400
[00:23:15.233] Server  ERROR   Backend returned 400
[00:23:15.254] Server  ERROR   Backend returned 400
[00:23:16.172] Browser WARN    Inference API failed, using simulation 400
[00:23:16.172] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.18203175203437977
[00:23:16.172] Browser WARN    Inference API failed, using simulation 400
[00:23:16.172] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2648066473254335
[00:23:16.172] Browser WARN    Inference API failed, using simulation 400
[00:23:16.172] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9581524479102004
[00:23:16.255] Server  ERROR   Backend returned 400
[00:23:16.258] Server  ERROR   Backend returned 400
[00:23:16.274] Server  ERROR   Backend returned 400
[00:23:17.332] Browser WARN    Inference API failed, using simulation 400
[00:23:17.332] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.24580929739471213
[00:23:17.332] Browser WARN    Inference API failed, using simulation 400
[00:23:17.332] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.11665786844965559
[00:23:17.332] Browser WARN    Inference API failed, using simulation 400
[00:23:17.332] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3809085672845239
[00:23:17.463] Server  ERROR   Backend returned 400
[00:23:17.466] Server  ERROR   Backend returned 400
[00:23:17.483] Server  ERROR   Backend returned 400
[00:23:18.190] Browser WARN    Inference API failed, using simulation 400
[00:23:18.190] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2478202369062235
[00:23:18.190] Browser WARN    Inference API failed, using simulation 400
[00:23:18.190] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.40645080442734804
[00:23:18.190] Browser WARN    Inference API failed, using simulation 400
[00:23:18.190] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.14992109824789523
[00:23:18.411] Server  ERROR   Backend returned 400
[00:23:18.416] Server  ERROR   Backend returned 400
[00:23:18.430] Server  ERROR   Backend returned 400
[00:23:19.167] Browser WARN    Inference API failed, using simulation 400
[00:23:19.167] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.08977736457723134
[00:23:19.167] Browser WARN    Inference API failed, using simulation 400
[00:23:19.167] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3315027405273162
[00:23:19.167] Browser WARN    Inference API failed, using simulation 400
[00:23:19.167] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.42314257708455927
[00:23:19.227] Server  ERROR   Backend returned 400
[00:23:19.229] Server  ERROR   Backend returned 400
[00:23:19.244] Server  ERROR   Backend returned 400
[00:23:20.162] Browser WARN    Inference API failed, using simulation 400
[00:23:20.162] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4617838145370502
[00:23:20.162] Browser WARN    Inference API failed, using simulation 400
[00:23:20.162] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9897430216467946
[00:23:20.162] Browser LOG     Panic detected at: Main Hall Count in last 15s: 7
[00:23:20.162] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Parking Area","Main Hall","Parking Area","Gate 1 Entrance","Main Hall","Parking Area","Main Hall"]
[00:23:20.162] Browser WARN    Inference API failed, using simulation 400
[00:23:20.162] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.06093583377095463
[00:23:20.202] Server  ERROR   Backend returned 400
[00:23:20.223] Server  ERROR   Backend returned 400
[00:23:20.235] Server  ERROR   Backend returned 400
[00:23:21.169] Browser WARN    Inference API failed, using simulation 400
[00:23:21.169] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.208976578356937
[00:23:21.169] Browser WARN    Inference API failed, using simulation 400
[00:23:21.169] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.27089419732642916
[00:23:21.169] Browser WARN    Inference API failed, using simulation 400
[00:23:21.169] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7238495054808151
[00:23:21.169] Browser LOG     Panic detected at: Parking Area Count in last 15s: 8
[00:23:21.169] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Parking Area","Main Hall","Parking Area","Gate 1 Entrance","Main Hall","Parking Area","Main Hall","Parking Area"]
[00:23:21.226] Server  ERROR   Backend returned 400
[00:23:21.235] Server  ERROR   Backend returned 400
[00:23:21.241] Server  ERROR   Backend returned 400
[00:23:22.166] Browser WARN    Inference API failed, using simulation 400
[00:23:22.166] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.15359676475923806
[00:23:22.166] Browser WARN    Inference API failed, using simulation 400
[00:23:22.166] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8465635845056819
[00:23:22.166] Browser WARN    Inference API failed, using simulation 400
[00:23:22.166] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2269617262913199
[00:23:22.227] Server  ERROR   Backend returned 400
[00:23:22.229] Server  ERROR   Backend returned 400
[00:23:22.241] Server  ERROR   Backend returned 400
[00:23:23.163] Browser WARN    Inference API failed, using simulation 400
[00:23:23.163] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8746557349660157
[00:23:23.163] Browser LOG     Panic detected at: Gate 1 Entrance Count in last 15s: 8
[00:23:23.163] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Main Hall","Parking Area","Gate 1 Entrance","Main Hall","Parking Area","Main Hall","Parking Area","Gate 1 Entrance"]
[00:23:23.163] Browser WARN    Inference API failed, using simulation 400
[00:23:23.163] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.41727748613915716
[00:23:23.163] Browser WARN    Inference API failed, using simulation 400
[00:23:23.163] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.14241059526217992
[00:23:23.226] Server  ERROR   Backend returned 400
[00:23:23.233] Server  ERROR   Backend returned 400
[00:23:23.246] Server  ERROR   Backend returned 400
[00:23:24.174] Browser WARN    Inference API failed, using simulation 400
[00:23:24.174] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.42328064056531134
[00:23:24.174] Browser WARN    Inference API failed, using simulation 400
[00:23:24.174] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4850637426838056
[00:23:24.174] Browser WARN    Inference API failed, using simulation 400
[00:23:24.174] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.03748947910214845
[00:23:24.245] Server  ERROR   Backend returned 400
[00:23:24.247] Server  ERROR   Backend returned 400
[00:23:24.249] Server  ERROR   Backend returned 400
[00:23:25.168] Browser WARN    Inference API failed, using simulation 400
[00:23:25.168] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4994850780278464
[00:23:25.168] Browser WARN    Inference API failed, using simulation 400
[00:23:25.168] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.45756237745572265
[00:23:25.168] Browser WARN    Inference API failed, using simulation 400
[00:23:25.168] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7207734161437754
[00:23:25.168] Browser LOG     Panic detected at: Parking Area Count in last 15s: 8
[00:23:25.168] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Parking Area","Gate 1 Entrance","Main Hall","Parking Area","Main Hall","Parking Area","Gate 1 Entrance","Parking Area"]
[00:23:25.219] Server  ERROR   Backend returned 400
[00:23:25.221] Server  ERROR   Backend returned 400
[00:23:25.236] Server  ERROR   Backend returned 400
[00:23:26.164] Browser WARN    Inference API failed, using simulation 400
[00:23:26.164] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.02532311804976456
[00:23:26.164] Browser WARN    Inference API failed, using simulation 400
[00:23:26.164] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.005392901940730888
[00:23:26.164] Browser WARN    Inference API failed, using simulation 400
[00:23:26.164] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.18853751608685104
[00:23:26.211] Server  ERROR   Backend returned 400
[00:23:26.215] Server  ERROR   Backend returned 400
[00:23:26.231] Server  ERROR   Backend returned 400
[00:23:27.162] Browser WARN    Inference API failed, using simulation 400
[00:23:27.162] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9989773525692768
[00:23:27.162] Browser LOG     Panic detected at: Gate 1 Entrance Count in last 15s: 8
[00:23:27.162] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Gate 1 Entrance","Main Hall","Parking Area","Main Hall","Parking Area","Gate 1 Entrance","Parking Area","Gate 1 Entrance"]
[00:23:27.162] Browser WARN    Inference API failed, using simulation 400
[00:23:27.162] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.10408918910752296
[00:23:27.162] Browser WARN    Inference API failed, using simulation 400
[00:23:27.162] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.783560817870406
[00:23:27.236] Server  ERROR   Backend returned 400
[00:23:27.245] Server  ERROR   Backend returned 400
[00:23:27.248] Server  ERROR   Backend returned 400
[00:23:28.174] Browser WARN    Inference API failed, using simulation 400
[00:23:28.174] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.038887738282985695
[00:23:28.174] Browser WARN    Inference API failed, using simulation 400
[00:23:28.174] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2966587101977037
[00:23:28.174] Browser WARN    Inference API failed, using simulation 400
[00:23:28.174] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3378347394246214
[00:23:28.228] Server  ERROR   Backend returned 400
[00:23:28.254] Server  ERROR   Backend returned 400
[00:23:28.256] Server  ERROR   Backend returned 400
[00:23:29.170] Browser WARN    Inference API failed, using simulation 400
[00:23:29.170] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8881288105480825
[00:23:29.170] Browser WARN    Inference API failed, using simulation 400
[00:23:29.170] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8323434808430167
[00:23:29.170] Browser LOG     Panic detected at: Main Hall Count in last 15s: 7
[00:23:29.170] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Parking Area","Main Hall","Parking Area","Gate 1 Entrance","Parking Area","Gate 1 Entrance","Main Hall"]
[00:23:29.170] Browser WARN    Inference API failed, using simulation 400
[00:23:29.170] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7845834169832333
[00:23:29.170] Browser LOG     Panic detected at: Parking Area Count in last 15s: 8
[00:23:29.170] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Parking Area","Main Hall","Parking Area","Gate 1 Entrance","Parking Area","Gate 1 Entrance","Main Hall","Parking Area"]
[00:23:29.218] Server  ERROR   Backend returned 400
[00:23:29.249] Server  ERROR   Backend returned 400
[00:23:29.270] Server  ERROR   Backend returned 400
[00:23:30.166] Browser WARN    Inference API failed, using simulation 400
[00:23:30.166] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.907616325179772
[00:23:30.166] Browser LOG     Panic detected at: Gate 1 Entrance Count in last 15s: 8
[00:23:30.166] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Main Hall","Parking Area","Gate 1 Entrance","Parking Area","Gate 1 Entrance","Main Hall","Parking Area","Gate 1 Entrance"]
[00:23:30.166] Browser WARN    Inference API failed, using simulation 400
[00:23:30.166] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9451807301542828
[00:23:30.166] Browser WARN    Inference API failed, using simulation 400
[00:23:30.166] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.478480356769338
[00:23:30.220] Server  ERROR   Backend returned 400
[00:23:30.223] Server  ERROR   Backend returned 400
[00:23:30.238] Server  ERROR   Backend returned 400
[00:23:31.162] Browser WARN    Inference API failed, using simulation 400
[00:23:31.162] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.21018280364224246
[00:23:31.162] Browser WARN    Inference API failed, using simulation 400
[00:23:31.162] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7077741145829644
[00:23:31.162] Browser WARN    Inference API failed, using simulation 400
[00:23:31.162] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1403153657404917
[00:23:31.216] Server  ERROR   Backend returned 400
[00:23:31.219] Server  ERROR   Backend returned 400
[00:23:31.234] Server  ERROR   Backend returned 400
[00:23:32.172] Browser WARN    Inference API failed, using simulation 400
[00:23:32.172] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.20116388736301705
[00:23:32.172] Browser WARN    Inference API failed, using simulation 400
[00:23:32.172] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4841680407045631
[00:23:32.172] Browser WARN    Inference API failed, using simulation 400
[00:23:32.172] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.18928435111703656
[00:23:32.198] Server  ERROR   Backend returned 400
[00:23:32.216] Server  ERROR   Backend returned 400
[00:23:32.229] Server  ERROR   Backend returned 400
[00:23:33.167] Browser WARN    Inference API failed, using simulation 400
[00:23:33.167] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.36454747362875295
[00:23:33.167] Browser WARN    Inference API failed, using simulation 400
[00:23:33.167] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8132480429006435
[00:23:33.167] Browser LOG     Panic detected at: Main Hall Count in last 15s: 9
[00:23:33.167] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Main Hall","Parking Area","Gate 1 Entrance","Parking Area","Gate 1 Entrance","Main Hall","Parking Area","Gate 1 Entrance","Main Hall"]
[00:23:33.167] Browser WARN    Inference API failed, using simulation 400
[00:23:33.167] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.28094960516126377
[00:23:34.978] Server  ERROR   Inference proxy error: "fetch failed"
[00:23:35.035] Server  ERROR   Inference proxy error: "fetch failed"
[00:23:35.050] Server  ERROR   Inference proxy error: "fetch failed"
[00:23:35.185] Browser WARN    Inference API failed, using simulation 500
[00:23:35.185] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4219543441392855
[00:23:35.185] Browser WARN    Inference API failed, using simulation 500
[00:23:35.185] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.04999320149766023
[00:23:35.185] Browser WARN    Inference API failed, using simulation 500
[00:23:35.185] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.17606971317809172
[00:23:35.214] Server  ERROR   Inference proxy error: "fetch failed"
[00:23:35.250] Server  ERROR   Inference proxy error: "fetch failed"
[00:23:35.270] Server  ERROR   Inference proxy error: "fetch failed"
[00:23:35.382] Browser WARN    Inference API failed, using simulation 500
[00:23:35.382] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.17921991746885974
[00:23:35.382] Browser WARN    Inference API failed, using simulation 500
[00:23:35.382] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.39008638837270143
[00:23:35.382] Browser WARN    Inference API failed, using simulation 500
[00:23:35.382] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8109951101471738
[00:23:35.382] Browser LOG     Panic detected at: Parking Area Count in last 15s: 8
[00:23:35.382] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Gate 1 Entrance","Parking Area","Gate 1 Entrance","Main Hall","Parking Area","Gate 1 Entrance","Main Hall","Parking Area"]
[00:23:35.679] Server  ERROR   Inference proxy error: "fetch failed"
[00:23:35.713] Server  ERROR   Inference proxy error: "fetch failed"
[00:23:35.721] Server  ERROR   Inference proxy error: "fetch failed"
[00:23:35.835] Browser WARN    Inference API failed, using simulation 500
[00:23:35.835] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.24529649522916097
[00:23:35.835] Browser WARN    Inference API failed, using simulation 500
[00:23:35.835] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.49121747851707553
[00:23:35.835] Browser WARN    Inference API failed, using simulation 500
[00:23:35.835] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.07831701137843616
[00:23:36.185] Server  ERROR   Inference proxy error: "fetch failed"
[00:23:36.216] Server  ERROR   Inference proxy error: "fetch failed"
[00:23:36.230] Server  ERROR   Inference proxy error: "fetch failed"
[00:23:36.346] Browser WARN    Inference API failed, using simulation 500
[00:23:36.346] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.23624853932093343
[00:23:36.346] Browser WARN    Inference API failed, using simulation 500
[00:23:36.346] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3649315197745018
[00:23:36.346] Browser WARN    Inference API failed, using simulation 500
[00:23:36.346] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.252994468576951
[00:23:36.687] Server  ERROR   Inference proxy error: "fetch failed"
[00:23:36.724] Server  ERROR   Inference proxy error: "fetch failed"
[00:23:36.729] Server  ERROR   Inference proxy error: "fetch failed"
[00:23:36.844] Browser WARN    Inference API failed, using simulation 500
[00:23:36.844] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.04404529452505096
[00:23:36.844] Browser WARN    Inference API failed, using simulation 500
[00:23:36.844] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.11380393756984675
[00:23:36.844] Browser WARN    Inference API failed, using simulation 500
[00:23:36.844] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3707970722218146
[00:23:37.215] Server  ERROR   Backend returned 400
[00:23:37.324] Browser WARN    Inference API failed, using simulation 400
[00:23:37.324] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9652006188971363
[00:23:37.324] Browser LOG     Panic detected at: Gate 1 Entrance Count in last 15s: 9
[00:23:37.324] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Gate 1 Entrance","Parking Area","Gate 1 Entrance","Main Hall","Parking Area","Gate 1 Entrance","Main Hall","Parking Area","Gate 1 Entrance"]
[00:23:37.341] Server  ERROR   Backend returned 400
[00:23:37.347] Server  ERROR   Backend returned 400
[00:23:37.466] Browser WARN    Inference API failed, using simulation 400
[00:23:37.466] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.25161875324793676
[00:23:37.466] Browser WARN    Inference API failed, using simulation 400
[00:23:37.466] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9310446915498132
[00:23:37.466] Browser LOG     Panic detected at: Parking Area Count in last 15s: 9
[00:23:37.466] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Parking Area","Gate 1 Entrance","Main Hall","Parking Area","Gate 1 Entrance","Main Hall","Parking Area","Gate 1 Entrance","Parking Area"]
[00:23:37.661] Server  ERROR   Backend returned 400
[00:23:37.702] Server  ERROR   Backend returned 400
[00:23:37.735] Server  ERROR   Backend returned 400
[00:23:37.839] Browser WARN    Inference API failed, using simulation 400
[00:23:37.839] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.30816633695083845
[00:23:37.839] Browser WARN    Inference API failed, using simulation 400
[00:23:37.839] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3245552075101926
[00:23:37.839] Browser WARN    Inference API failed, using simulation 400
[00:23:37.839] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3703454841863771
[00:23:38.158] Server  ERROR   Backend returned 400
[00:23:38.200] Server  ERROR   Backend returned 400
[00:23:38.237] Server  ERROR   Backend returned 400
[00:23:38.346] Browser WARN    Inference API failed, using simulation 400
[00:23:38.346] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4604699124969745
[00:23:38.346] Browser WARN    Inference API failed, using simulation 400
[00:23:38.346] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.30947015460330474
[00:23:38.346] Browser WARN    Inference API failed, using simulation 400
[00:23:38.346] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.24844065613686478
[00:23:38.671] Server  ERROR   Backend returned 400
[00:23:38.695] Server  ERROR   Backend returned 400
[00:23:38.728] Server  ERROR   Backend returned 400
[00:23:38.837] Browser WARN    Inference API failed, using simulation 400
[00:23:38.837] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7655540077804954
[00:23:38.837] Browser WARN    Inference API failed, using simulation 400
[00:23:38.837] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.35060590984703993
[00:23:38.837] Browser WARN    Inference API failed, using simulation 400
[00:23:38.837] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9346627933996731
[00:23:39.159] Server  ERROR   Backend returned 400
[00:23:39.194] Server  ERROR   Backend returned 400
[00:23:39.211] Server  ERROR   Backend returned 400
[00:23:39.319] Browser WARN    Inference API failed, using simulation 400
[00:23:39.319] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.27188047630881906
[00:23:39.319] Browser WARN    Inference API failed, using simulation 400
[00:23:39.319] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3395246672723286
[00:23:39.319] Browser WARN    Inference API failed, using simulation 400
[00:23:39.319] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9943242582116467
[00:23:39.665] Server  ERROR   Backend returned 400
[00:23:39.721] Server  ERROR   Backend returned 400
[00:23:39.742] Server  ERROR   Backend returned 400
[00:23:39.851] Browser WARN    Inference API failed, using simulation 400
[00:23:39.851] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3705366240905749
[00:23:39.851] Browser WARN    Inference API failed, using simulation 400
[00:23:39.851] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3650408650122788
[00:23:39.851] Browser WARN    Inference API failed, using simulation 400
[00:23:39.851] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9026070783217442
[00:23:39.851] Browser LOG     Panic detected at: Parking Area Count in last 15s: 9
[00:23:39.851] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Gate 1 Entrance","Main Hall","Parking Area","Gate 1 Entrance","Main Hall","Parking Area","Gate 1 Entrance","Parking Area","Parking Area"]
[00:23:40.208] Server  ERROR   Backend returned 400
[00:23:40.248] Server  ERROR   Backend returned 400
[00:23:40.319] Server  ERROR   Backend returned 400
[00:23:40.424] Browser WARN    Inference API failed, using simulation 400
[00:23:40.424] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7704653143372265
[00:23:40.424] Browser LOG     Panic detected at: Gate 1 Entrance Count in last 15s: 10
[00:23:40.424] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Gate 1 Entrance","Main Hall","Parking Area","Gate 1 Entrance","Main Hall","Parking Area","Gate 1 Entrance","Parking Area","Parking Area","Gate 1 Entrance"]
[00:23:40.424] Browser WARN    Inference API failed, using simulation 400
[00:23:40.424] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.40035574344823155
[00:23:40.424] Browser WARN    Inference API failed, using simulation 400
[00:23:40.424] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3815250930841203
[00:23:40.691] Server  ERROR   Backend returned 400
[00:23:40.751] Server  ERROR   Backend returned 400
[00:23:40.810] Server  ERROR   Backend returned 400
[00:23:40.917] Browser WARN    Inference API failed, using simulation 400
[00:23:40.917] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.09506958371214574
[00:23:40.917] Browser WARN    Inference API failed, using simulation 400
[00:23:40.917] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9255844842948604
[00:23:40.917] Browser LOG     Panic detected at: Main Hall Count in last 15s: 11
[00:23:40.917] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Gate 1 Entrance","Main Hall","Parking Area","Gate 1 Entrance","Main Hall","Parking Area","Gate 1 Entrance","Parking Area","Parking Area","Gate 1 Entrance","Main Hall"]
[00:23:40.917] Browser WARN    Inference API failed, using simulation 400
[00:23:40.917] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.26310817112171236
[00:23:41.195] Server  ERROR   Backend returned 400
[00:23:41.290] Server  ERROR   Backend returned 400
[00:23:41.360] Server  ERROR   Backend returned 400
[00:23:41.472] Browser WARN    Inference API failed, using simulation 400
[00:23:41.472] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.45212163441879244
[00:23:41.472] Browser WARN    Inference API failed, using simulation 400
[00:23:41.472] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.883339378648199
[00:23:41.472] Browser WARN    Inference API failed, using simulation 400
[00:23:41.472] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8426506139626895
[00:23:41.689] Server  ERROR   Backend returned 400
[00:23:41.751] Server  ERROR   Backend returned 400
[00:23:41.835] Server  ERROR   Backend returned 400
[00:23:41.954] Browser WARN    Inference API failed, using simulation 400
[00:23:41.954] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.41937745128991244
[00:23:41.954] Browser WARN    Inference API failed, using simulation 400
[00:23:41.954] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.860695903241211
[00:23:41.954] Browser WARN    Inference API failed, using simulation 400
[00:23:41.954] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8012197573284664
[00:23:41.954] Browser LOG     Panic detected at: Parking Area Count in last 15s: 11
[00:23:41.954] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Main Hall","Parking Area","Gate 1 Entrance","Main Hall","Parking Area","Gate 1 Entrance","Parking Area","Parking Area","Gate 1 Entrance","Main Hall","Parking Area"]
[00:23:42.497] Server  ERROR   Backend returned 400
[00:23:42.586] Server  ERROR   Backend returned 400
[00:23:42.604] Server  ERROR   Backend returned 400
[00:23:42.769] Browser WARN    Inference API failed, using simulation 400
[00:23:42.769] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.18853935814464862
[00:23:42.769] Browser WARN    Inference API failed, using simulation 400
[00:23:42.769] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2106789836993565
[00:23:42.769] Browser WARN    Inference API failed, using simulation 400
[00:23:42.769] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.23505054389684765
[00:23:42.847] Server  ERROR   Backend returned 400
[00:23:42.916] Server  ERROR   Backend returned 400
[00:23:42.935] Server  ERROR   Backend returned 400
[00:23:43.046] Browser WARN    Inference API failed, using simulation 400
[00:23:43.046] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1716415750722038
[00:23:43.046] Browser WARN    Inference API failed, using simulation 400
[00:23:43.046] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.05638480779282695
[00:23:43.046] Browser WARN    Inference API failed, using simulation 400
[00:23:43.046] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.04844360258814101
[00:23:43.230] Server  ERROR   Backend returned 400
[00:23:43.305] Server  ERROR   Backend returned 400
[00:23:43.328] Server  ERROR   Backend returned 400
[00:23:43.434] Browser WARN    Inference API failed, using simulation 400
[00:23:43.434] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.17004087572282162
[00:23:43.434] Browser WARN    Inference API failed, using simulation 400
[00:23:43.434] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9534261027056328
[00:23:43.434] Browser LOG     Panic detected at: Main Hall Count in last 15s: 10
[00:23:43.434] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Gate 1 Entrance","Main Hall","Parking Area","Gate 1 Entrance","Parking Area","Parking Area","Gate 1 Entrance","Main Hall","Parking Area","Main Hall"]
[00:23:43.434] Browser WARN    Inference API failed, using simulation 400
[00:23:43.434] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.19630830064892157
[00:23:43.739] Server  ERROR   Backend returned 400
[00:23:43.781] Server  ERROR   Backend returned 400
[00:23:44.023] Server  ERROR   Backend returned 400
[00:23:44.028] Browser WARN    Inference API failed, using simulation 400
[00:23:44.028] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9170702293620874
[00:23:44.028] Browser LOG     Panic detected at: Gate 1 Entrance Count in last 15s: 11
[00:23:44.028] Browser LOG     ðŸš¨ CRITICAL ALERT: 6 panic detections in 15 seconds at: ["Gate 1 Entrance","Main Hall","Parking Area","Gate 1 Entrance","Parking Area","Parking Area","Gate 1 Entrance","Main Hall","Parking Area","Main Hall","Gate 1 Entrance"]
[00:23:44.028] Browser WARN    Inference API failed, using simulation 400
[00:23:44.028] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.08560725180648637
[00:23:44.155] Browser WARN    Inference API failed, using simulation 400
[00:23:44.155] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.16285433517196451
[00:23:44.412] Browser LOG     Cleanup: clearing intervals
[00:23:44.412] Browser LOG     %c[Vercel Web Analytics]%c [pageview] http://localhost:3000/admin/dashboard color: rgb(120, 120, 120) color: inherit {"dp":"/admin/dashboard","o":"http://localhost:3000/admin/dashboard","sdkn":"@vercel/analytics/next","sdkv":"1.5.0","sv":"0.1.3","ts":1763753295473}
[00:23:44.475] Server  ERROR   Backend returned 400
[00:23:44.509] Server  ERROR   Backend returned 400
[00:23:44.515] Server  ERROR   Backend returned 400
[00:23:44.620] Browser WARN    Inference API failed, using simulation 400
[00:23:44.620] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.21208970134927402
[00:23:44.620] Browser WARN    Inference API failed, using simulation 400
[00:23:44.620] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.07714767573463294
[00:23:44.620] Browser WARN    Inference API failed, using simulation 400
[00:23:44.620] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.09836208549934594
[00:26:19.111] Server  LOG      âœ“ Compiled in 535ms
[00:26:27.830] Server  LOG      âœ“ Compiled in 296ms
[00:26:32.206] Browser LOG     Starting inference for videos: [1,2,3]
[00:26:32.206] Browser LOG     Starting inference for video: 1 Gate 1 Entrance
[00:26:32.206] Browser LOG     Starting inference for video: 2 Main Hall
[00:26:32.206] Browser LOG     Starting inference for video: 3 Parking Area
[00:26:32.206] Browser LOG     Cleanup: clearing intervals
[00:26:32.206] Browser LOG     Starting inference for videos: [1,2,3]
[00:26:32.206] Browser LOG     Starting inference for video: 1 Gate 1 Entrance
[00:26:32.206] Browser LOG     Starting inference for video: 2 Main Hall
[00:26:32.206] Browser LOG     Starting inference for video: 3 Parking Area
[00:26:32.206] Browser LOG     %c[Vercel Web Analytics]%c [pageview] http://localhost:3000/admin/monitoring color: rgb(120, 120, 120) color: inherit {"dp":"/admin/monitoring","o":"http://localhost:3000/admin/monitoring","sdkn":"@vercel/analytics/next","sdkv":"1.5.0","sv":"0.1.3","ts":1763753463327}
[00:26:32.772] Server  ERROR   Backend returned 400
[00:26:32.812] Server  ERROR   Backend returned 400
[00:26:32.833] Server  ERROR   Backend returned 400
[00:26:32.950] Browser WARN    Inference API failed, using simulation 400
[00:26:32.950] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9893031020080093
[00:26:32.950] Browser WARN    Inference API failed, using simulation 400
[00:26:32.951] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.26963129611680137
[00:26:32.951] Browser WARN    Inference API failed, using simulation 400
[00:26:32.951] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.43002243833236564
[00:26:33.204] Server  ERROR   Backend returned 400
[00:26:33.233] Server  ERROR   Backend returned 400
[00:26:33.261] Server  ERROR   Backend returned 400
[00:26:33.365] Browser WARN    Inference API failed, using simulation 400
[00:26:33.365] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.005509935395828891
[00:26:33.365] Browser WARN    Inference API failed, using simulation 400
[00:26:33.365] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.79219380428291
[00:26:33.365] Browser WARN    Inference API failed, using simulation 400
[00:26:33.365] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8778654873392491
[00:26:33.689] Server  ERROR   Backend returned 400
[00:26:33.721] Server  ERROR   Backend returned 400
[00:26:33.745] Server  ERROR   Backend returned 400
[00:26:33.850] Browser WARN    Inference API failed, using simulation 400
[00:26:33.850] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2837612615507813
[00:26:33.850] Browser WARN    Inference API failed, using simulation 400
[00:26:33.850] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.039212570583766204
[00:26:33.850] Browser WARN    Inference API failed, using simulation 400
[00:26:33.850] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.04872045591143803
[00:26:34.165] Server  ERROR   Backend returned 400
[00:26:34.186] Server  ERROR   Backend returned 400
[00:26:34.214] Server  ERROR   Backend returned 400
[00:26:34.320] Browser WARN    Inference API failed, using simulation 400
[00:26:34.320] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4849008953518675
[00:26:34.320] Browser WARN    Inference API failed, using simulation 400
[00:26:34.320] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.34727814422893466
[00:26:34.320] Browser WARN    Inference API failed, using simulation 400
[00:26:34.320] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2331468170646236
[00:26:34.665] Server  ERROR   Backend returned 400
[00:26:34.701] Server  ERROR   Backend returned 400
[00:26:34.716] Server  ERROR   Backend returned 400
[00:26:34.801] Server  LOG      âœ“ Compiled in 281ms
[00:26:34.927] Browser WARN    Inference API failed, using simulation 400
[00:26:34.927] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.33430985259136214
[00:26:34.927] Browser WARN    Inference API failed, using simulation 400
[00:26:34.927] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7597320562351849
[00:26:34.927] Browser WARN    Inference API failed, using simulation 400
[00:26:34.927] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.022229629280870744
[00:26:34.927] Browser ERROR   uncaughtError: ReferenceError: panicAlert is not defined
[00:26:34.927] Browser LOG     Cleanup: clearing intervals
[00:26:52.391] Server  WARN     âš  Fast Refresh had to perform a full reload due to a runtime error.
[00:26:52.412] Server  LOG      âœ“ Compiled in 194ms
[00:26:53.345] Browser LOG     Starting inference for videos: [1,2,3]
[00:26:53.345] Browser LOG     Starting inference for video: 1 Gate 1 Entrance
[00:26:53.345] Browser LOG     Starting inference for video: 2 Main Hall
[00:26:53.345] Browser LOG     Starting inference for video: 3 Parking Area
[00:26:53.345] Browser LOG     %c[Vercel Web Analytics]%c [pageview] http://localhost:3000/admin/monitoring color: rgb(120, 120, 120) color: inherit {"dp":"/admin/monitoring","o":"http://localhost:3000/admin/monitoring","sdkn":"@vercel/analytics/next","sdkv":"1.5.0","sv":"0.1.3","ts":1763753483714}
[00:26:54.172] Browser INFO    %cDownload the React DevTools for a better development experience: https://react.dev/link/react-devtools font-weight:bold
[00:26:55.178] Browser LOG     Starting inference for videos: [1,2,3]
[00:26:55.179] Browser LOG     Starting inference for video: 1 Gate 1 Entrance
[00:26:55.179] Browser LOG     Starting inference for video: 2 Main Hall
[00:26:55.179] Browser LOG     Starting inference for video: 3 Parking Area
[00:26:55.179] Browser LOG     %c[Vercel Web Analytics]%c Debug mode is enabled by default in development. No requests will be sent to the server. color: rgb(120, 120, 120) color: inherit
[00:26:55.179] Browser LOG     %c[Vercel Web Analytics]%c Running queued event color: rgb(120, 120, 120) color: inherit pageview {"path":"/admin/monitoring","route":"/admin/monitoring"}
[00:26:55.179] Browser LOG     %c[Vercel Web Analytics]%c [pageview] http://localhost:3000/admin/monitoring color: rgb(120, 120, 120) color: inherit {"dp":"/admin/monitoring","o":"http://localhost:3000/admin/monitoring","sdkn":"@vercel/analytics/next","sdkv":"1.5.0","sv":"0.1.3","ts":1763753485753}
[00:26:55.305] Server  ERROR   Backend returned 400
[00:26:55.350] Server  ERROR   Backend returned 400
[00:26:55.378] Server  ERROR   Backend returned 400
[00:26:56.198] Browser WARN    Inference API failed, using simulation 400
[00:26:56.198] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8262465006177423
[00:26:56.198] Browser WARN    Inference API failed, using simulation 400
[00:26:56.198] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.04767220862391547
[00:26:56.198] Browser WARN    Inference API failed, using simulation 400
[00:26:56.198] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7655571848949587
[00:26:56.265] Server  ERROR   Backend returned 400
[00:26:56.280] Server  ERROR   Backend returned 400
[00:26:56.295] Server  ERROR   Backend returned 400
[00:26:57.172] Browser WARN    Inference API failed, using simulation 400
[00:26:57.172] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7195215882116526
[00:26:57.172] Browser WARN    Inference API failed, using simulation 400
[00:26:57.172] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2849111005305755
[00:26:57.172] Browser WARN    Inference API failed, using simulation 400
[00:26:57.172] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.042562883259776085
[00:26:57.229] Server  ERROR   Backend returned 400
[00:26:57.252] Server  ERROR   Backend returned 400
[00:26:57.270] Server  ERROR   Backend returned 400
[00:26:58.144] Browser WARN    Inference API failed, using simulation 400
[00:26:58.144] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.24617788014503067
[00:26:58.144] Browser WARN    Inference API failed, using simulation 400
[00:26:58.144] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4262465662831489
[00:26:58.144] Browser WARN    Inference API failed, using simulation 400
[00:26:58.144] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.02248361019824613
[00:26:58.241] Server  ERROR   Backend returned 400
[00:26:58.268] Server  ERROR   Backend returned 400
[00:26:58.290] Server  ERROR   Backend returned 400
[00:26:58.395] Browser WARN    Inference API failed, using simulation 400
[00:26:58.395] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9507712448127661
[00:26:58.395] Browser WARN    Inference API failed, using simulation 400
[00:26:58.395] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.0003962619536336809
[00:26:58.395] Browser WARN    Inference API failed, using simulation 400
[00:26:58.395] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8525296170558345
[00:26:58.543] Server  ERROR   Backend returned 400
[00:26:58.559] Server  ERROR   Backend returned 400
[00:26:58.584] Server  ERROR   Backend returned 400
[00:26:58.688] Browser WARN    Inference API failed, using simulation 400
[00:26:58.688] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.40686691363730443
[00:26:58.688] Browser WARN    Inference API failed, using simulation 400
[00:26:58.688] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4658444802050169
[00:26:58.688] Browser WARN    Inference API failed, using simulation 400
[00:26:58.688] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7524234461629169
[00:26:59.045] Server  ERROR   Backend returned 400
[00:26:59.076] Server  ERROR   Backend returned 400
[00:26:59.095] Server  ERROR   Backend returned 400
[00:26:59.202] Browser WARN    Inference API failed, using simulation 400
[00:26:59.202] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8810545923358117
[00:26:59.202] Browser WARN    Inference API failed, using simulation 400
[00:26:59.202] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4131558092886349
[00:26:59.202] Browser WARN    Inference API failed, using simulation 400
[00:26:59.202] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8467047405339413
[00:26:59.562] Server  ERROR   Backend returned 400
[00:26:59.581] Server  ERROR   Backend returned 400
[00:26:59.601] Server  ERROR   Backend returned 400
[00:26:59.704] Browser WARN    Inference API failed, using simulation 400
[00:26:59.704] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.11163091142272541
[00:26:59.704] Browser WARN    Inference API failed, using simulation 400
[00:26:59.704] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4099746069940889
[00:26:59.704] Browser WARN    Inference API failed, using simulation 400
[00:26:59.704] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.01491970505611434
[00:27:00.046] Server  ERROR   Backend returned 400
[00:27:00.069] Server  ERROR   Backend returned 400
[00:27:00.088] Server  ERROR   Backend returned 400
[00:27:00.192] Browser WARN    Inference API failed, using simulation 400
[00:27:00.192] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2126886313895736
[00:27:00.192] Browser WARN    Inference API failed, using simulation 400
[00:27:00.192] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3563607176076785
[00:27:00.192] Browser WARN    Inference API failed, using simulation 400
[00:27:00.192] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8134581628765877
[00:27:00.549] Server  ERROR   Backend returned 400
[00:27:00.580] Server  ERROR   Backend returned 400
[00:27:00.583] Server  ERROR   Backend returned 400
[00:27:00.683] Browser WARN    Inference API failed, using simulation 400
[00:27:00.683] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3336699006843877
[00:27:00.683] Browser WARN    Inference API failed, using simulation 400
[00:27:00.683] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.28058645564584644
[00:27:00.683] Browser WARN    Inference API failed, using simulation 400
[00:27:00.683] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7390571676840295
[00:27:01.268] Browser INFO    %cDownload the React DevTools for a better development experience: https://react.dev/link/react-devtools font-weight:bold
[00:27:01.580] Browser LOG     Starting inference for videos: [1,2,3]
[00:27:01.580] Browser LOG     Starting inference for video: 1 Gate 1 Entrance
[00:27:01.580] Browser LOG     Starting inference for video: 2 Main Hall
[00:27:01.580] Browser LOG     Starting inference for video: 3 Parking Area
[00:27:01.580] Browser LOG     %c[Vercel Web Analytics]%c Debug mode is enabled by default in development. No requests will be sent to the server. color: rgb(120, 120, 120) color: inherit
[00:27:01.580] Browser LOG     %c[Vercel Web Analytics]%c Running queued event color: rgb(120, 120, 120) color: inherit pageview {"path":"/admin/monitoring","route":"/admin/monitoring"}
[00:27:01.580] Browser LOG     %c[Vercel Web Analytics]%c [pageview] http://localhost:3000/admin/monitoring color: rgb(120, 120, 120) color: inherit {"dp":"/admin/monitoring","o":"http://localhost:3000/admin/monitoring","sdkn":"@vercel/analytics/next","sdkv":"1.5.0","sv":"0.1.3","ts":1763753492705}
[00:27:01.962] Server  ERROR   Backend returned 400
[00:27:01.991] Server  ERROR   Backend returned 400
[00:27:02.047] Server  ERROR   Backend returned 400
[00:27:02.151] Browser WARN    Inference API failed, using simulation 400
[00:27:02.151] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8760072537524254
[00:27:02.151] Browser WARN    Inference API failed, using simulation 400
[00:27:02.151] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.17378352869561442
[00:27:02.151] Browser WARN    Inference API failed, using simulation 400
[00:27:02.151] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.70723764669432
[00:27:02.468] Server  ERROR   Backend returned 400
[00:27:02.498] Server  ERROR   Backend returned 400
[00:27:02.543] Server  ERROR   Backend returned 400
[00:27:02.660] Browser WARN    Inference API failed, using simulation 400
[00:27:02.660] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.014082921102111956
[00:27:02.660] Browser WARN    Inference API failed, using simulation 400
[00:27:02.660] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7164841633619216
[00:27:02.660] Browser WARN    Inference API failed, using simulation 400
[00:27:02.660] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.22077631552110744
[00:27:02.975] Server  ERROR   Backend returned 400
[00:27:03.029] Server  ERROR   Backend returned 400
[00:27:03.054] Server  ERROR   Backend returned 400
[00:27:03.159] Browser WARN    Inference API failed, using simulation 400
[00:27:03.159] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9689529441040337
[00:27:03.159] Browser WARN    Inference API failed, using simulation 400
[00:27:03.159] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.957391309710748
[00:27:03.159] Browser WARN    Inference API failed, using simulation 400
[00:27:03.159] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.09216036505110531
[00:27:03.493] Server  ERROR   Backend returned 400
[00:27:03.523] Server  ERROR   Backend returned 400
[00:27:03.538] Server  ERROR   Backend returned 400
[00:27:03.642] Browser WARN    Inference API failed, using simulation 400
[00:27:03.642] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8101524401747512
[00:27:03.642] Browser WARN    Inference API failed, using simulation 400
[00:27:03.642] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.11313807092445927
[00:27:03.642] Browser WARN    Inference API failed, using simulation 400
[00:27:03.642] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.46697325953232344
[00:27:03.976] Server  ERROR   Backend returned 400
[00:27:04.009] Server  ERROR   Backend returned 400
[00:27:04.011] Server  ERROR   Backend returned 400
[00:27:04.124] Browser WARN    Inference API failed, using simulation 400
[00:27:04.124] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9538088365100428
[00:27:04.124] Browser WARN    Inference API failed, using simulation 400
[00:27:04.124] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1936216458542243
[00:27:04.124] Browser WARN    Inference API failed, using simulation 400
[00:27:04.124] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9230113282022809
[00:27:04.460] Server  ERROR   Backend returned 400
[00:27:04.495] Server  ERROR   Backend returned 400
[00:27:04.498] Server  ERROR   Backend returned 400
[00:27:04.620] Browser WARN    Inference API failed, using simulation 400
[00:27:04.620] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8565565725889683
[00:27:04.620] Browser WARN    Inference API failed, using simulation 400
[00:27:04.620] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.08167892195681997
[00:27:04.620] Browser WARN    Inference API failed, using simulation 400
[00:27:04.620] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.38455083256025907
[00:27:04.954] Server  ERROR   Backend returned 400
[00:27:04.987] Server  ERROR   Backend returned 400
[00:27:04.990] Server  ERROR   Backend returned 400
[00:27:05.098] Browser WARN    Inference API failed, using simulation 400
[00:27:05.098] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7262449764789425
[00:27:05.098] Browser WARN    Inference API failed, using simulation 400
[00:27:05.098] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4002073597454286
[00:27:05.098] Browser WARN    Inference API failed, using simulation 400
[00:27:05.098] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8419170315279665
[00:27:05.457] Server  ERROR   Backend returned 400
[00:27:05.493] Server  ERROR   Backend returned 400
[00:27:05.497] Server  ERROR   Backend returned 400
[00:27:05.606] Browser WARN    Inference API failed, using simulation 400
[00:27:05.606] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.03846165979452104
[00:27:05.606] Browser WARN    Inference API failed, using simulation 400
[00:27:05.606] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9223758028199529
[00:27:05.606] Browser WARN    Inference API failed, using simulation 400
[00:27:05.606] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7246571385827111
[00:27:05.957] Server  ERROR   Backend returned 400
[00:27:05.976] Server  ERROR   Backend returned 400
[00:27:05.995] Server  ERROR   Backend returned 400
[00:27:06.106] Browser WARN    Inference API failed, using simulation 400
[00:27:06.106] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.037546302565396816
[00:27:06.106] Browser WARN    Inference API failed, using simulation 400
[00:27:06.106] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.17867857299367446
[00:27:06.106] Browser WARN    Inference API failed, using simulation 400
[00:27:06.106] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.15335766752449836
[00:27:06.449] Server  ERROR   Backend returned 400
[00:27:06.484] Server  ERROR   Backend returned 400
[00:27:06.490] Server  ERROR   Backend returned 400
[00:27:06.599] Browser WARN    Inference API failed, using simulation 400
[00:27:06.599] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3254211991712217
[00:27:06.599] Browser WARN    Inference API failed, using simulation 400
[00:27:06.599] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7002827227786774
[00:27:06.599] Browser WARN    Inference API failed, using simulation 400
[00:27:06.599] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.21638480428997992
[00:27:06.965] Server  ERROR   Backend returned 400
[00:27:07.003] Server  ERROR   Backend returned 400
[00:27:07.030] Server  ERROR   Backend returned 400
[00:27:07.134] Browser WARN    Inference API failed, using simulation 400
[00:27:07.134] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3338235278731516
[00:27:07.134] Browser WARN    Inference API failed, using simulation 400
[00:27:07.134] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.40665394638884883
[00:27:07.134] Browser WARN    Inference API failed, using simulation 400
[00:27:07.134] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7951486228127932
[00:27:07.478] Server  ERROR   Backend returned 400
[00:27:07.505] Server  ERROR   Backend returned 400
[00:27:07.538] Server  ERROR   Backend returned 400
[00:27:07.646] Browser WARN    Inference API failed, using simulation 400
[00:27:07.646] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.14491483724004944
[00:27:07.646] Browser WARN    Inference API failed, using simulation 400
[00:27:07.646] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.006048942818007441
[00:27:07.646] Browser WARN    Inference API failed, using simulation 400
[00:27:07.646] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9219574783350825
[00:27:07.963] Server  ERROR   Backend returned 400
[00:27:07.988] Server  ERROR   Backend returned 400
[00:27:08.016] Server  ERROR   Backend returned 400
[00:27:08.122] Browser WARN    Inference API failed, using simulation 400
[00:27:08.122] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.40787622294258696
[00:27:08.122] Browser WARN    Inference API failed, using simulation 400
[00:27:08.122] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7197767641627038
[00:27:08.122] Browser WARN    Inference API failed, using simulation 400
[00:27:08.122] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.22502405492064553
[00:27:08.583] Server  ERROR   Backend returned 400
[00:27:08.649] Server  ERROR   Backend returned 400
[00:27:08.669] Server  ERROR   Backend returned 400
[00:27:09.193] Browser WARN    Inference API failed, using simulation 400
[00:27:09.193] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8774453427511831
[00:27:09.193] Browser WARN    Inference API failed, using simulation 400
[00:27:09.193] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4898257082162805
[00:27:09.193] Browser WARN    Inference API failed, using simulation 400
[00:27:09.193] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.17052830693842086
[00:27:09.328] Server  ERROR   Backend returned 400
[00:27:09.358] Server  ERROR   Backend returned 400
[00:27:09.442] Server  ERROR   Backend returned 400
[00:27:10.197] Browser WARN    Inference API failed, using simulation 400
[00:27:10.197] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3756236916331059
[00:27:10.197] Browser WARN    Inference API failed, using simulation 400
[00:27:10.197] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1890511856604824
[00:27:10.197] Browser WARN    Inference API failed, using simulation 400
[00:27:10.197] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7416117028156778
[00:27:10.219] Server  ERROR   Backend returned 400
[00:27:10.293] Server  ERROR   Backend returned 400
[00:27:10.309] Server  ERROR   Backend returned 400
[00:27:11.190] Browser WARN    Inference API failed, using simulation 400
[00:27:11.190] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.24835352447440556
[00:27:11.190] Browser WARN    Inference API failed, using simulation 400
[00:27:11.190] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8500411295595862
[00:27:11.190] Browser WARN    Inference API failed, using simulation 400
[00:27:11.190] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7469189926069207
[00:27:11.213] Server  ERROR   Backend returned 400
[00:27:11.283] Server  ERROR   Backend returned 400
[00:27:11.298] Server  ERROR   Backend returned 400
[00:27:12.054] Browser WARN    Inference API failed, using simulation 400
[00:27:12.055] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3866818685576011
[00:27:12.055] Browser WARN    Inference API failed, using simulation 400
[00:27:12.055] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.49506247561266836
[00:27:12.055] Browser WARN    Inference API failed, using simulation 400
[00:27:12.055] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.47482339218800684
[00:27:12.172] Server  ERROR   Backend returned 400
[00:27:12.223] Server  ERROR   Backend returned 400
[00:27:12.242] Server  ERROR   Backend returned 400
[00:27:12.348] Browser WARN    Inference API failed, using simulation 400
[00:27:12.348] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3750859822498191
[00:27:12.348] Browser WARN    Inference API failed, using simulation 400
[00:27:12.348] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.03312660531075795
[00:27:12.348] Browser WARN    Inference API failed, using simulation 400
[00:27:12.348] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3219154430462163
[00:27:12.456] Server  ERROR   Backend returned 400
[00:27:12.496] Server  ERROR   Backend returned 400
[00:27:12.517] Server  ERROR   Backend returned 400
[00:27:12.625] Browser WARN    Inference API failed, using simulation 400
[00:27:12.625] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.16363029389189793
[00:27:12.625] Browser WARN    Inference API failed, using simulation 400
[00:27:12.625] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4355065409247712
[00:27:12.625] Browser WARN    Inference API failed, using simulation 400
[00:27:12.625] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.13571107196495125
[00:27:12.955] Server  ERROR   Backend returned 400
[00:27:12.980] Server  ERROR   Backend returned 400
[00:27:12.997] Server  ERROR   Backend returned 400
[00:27:13.105] Browser WARN    Inference API failed, using simulation 400
[00:27:13.105] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4082519276354357
[00:27:13.105] Browser WARN    Inference API failed, using simulation 400
[00:27:13.105] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8203799060138887
[00:27:13.105] Browser WARN    Inference API failed, using simulation 400
[00:27:13.105] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.24735940848966936
[00:27:13.459] Server  ERROR   Backend returned 400
[00:27:13.485] Server  ERROR   Backend returned 400
[00:27:13.505] Server  ERROR   Backend returned 400
[00:27:13.609] Browser WARN    Inference API failed, using simulation 400
[00:27:13.609] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.40695830543874006
[00:27:13.609] Browser WARN    Inference API failed, using simulation 400
[00:27:13.609] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7794918877315477
[00:27:13.609] Browser WARN    Inference API failed, using simulation 400
[00:27:13.609] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.19246150333085127
[00:27:13.955] Server  ERROR   Backend returned 400
[00:27:13.995] Server  ERROR   Backend returned 400
[00:27:14.023] Server  ERROR   Backend returned 400
[00:27:14.177] Browser WARN    Inference API failed, using simulation 400
[00:27:14.177] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.18388711972713767
[00:27:14.177] Browser WARN    Inference API failed, using simulation 400
[00:27:14.177] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.033104469582536156
[00:27:14.177] Browser WARN    Inference API failed, using simulation 400
[00:27:14.177] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8447653052437425
[00:27:14.462] Server  ERROR   Backend returned 400
[00:27:14.485] Server  ERROR   Backend returned 400
[00:27:14.497] Server  ERROR   Backend returned 400
[00:27:14.601] Browser WARN    Inference API failed, using simulation 400
[00:27:14.601] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.25634954551615396
[00:27:14.601] Browser WARN    Inference API failed, using simulation 400
[00:27:14.601] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3569284071924936
[00:27:14.601] Browser WARN    Inference API failed, using simulation 400
[00:27:14.601] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.39156511792223997
[00:27:14.940] Server  ERROR   Backend returned 400
[00:27:14.981] Server  ERROR   Backend returned 400
[00:27:14.984] Server  ERROR   Backend returned 400
[00:27:15.097] Browser WARN    Inference API failed, using simulation 400
[00:27:15.097] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.19831821585026976
[00:27:15.097] Browser WARN    Inference API failed, using simulation 400
[00:27:15.097] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9744638962050887
[00:27:15.097] Browser WARN    Inference API failed, using simulation 400
[00:27:15.097] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8448805907376808
[00:27:15.441] Server  ERROR   Backend returned 400
[00:27:15.458] Server  ERROR   Backend returned 400
[00:27:15.480] Server  ERROR   Backend returned 400
[00:27:15.585] Browser WARN    Inference API failed, using simulation 400
[00:27:15.585] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.23204642215107307
[00:27:15.585] Browser WARN    Inference API failed, using simulation 400
[00:27:15.585] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.751009183336766
[00:27:15.585] Browser WARN    Inference API failed, using simulation 400
[00:27:15.585] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.20089210543014435
[00:27:15.953] Server  ERROR   Backend returned 400
[00:27:15.975] Browser WARN    Inference API failed, using simulation 400
[00:27:15.975] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4563983976108686
[00:27:15.976] Server  ERROR   Backend returned 400
[00:27:15.989] Server  ERROR   Backend returned 400
[00:27:16.474] Browser INFO    %cDownload the React DevTools for a better development experience: https://react.dev/link/react-devtools font-weight:bold
[00:27:16.692] Browser LOG     Starting inference for videos: [1,2,3]
[00:27:16.692] Browser LOG     Starting inference for video: 1 Gate 1 Entrance
[00:27:16.692] Browser LOG     Starting inference for video: 2 Main Hall
[00:27:16.692] Browser LOG     Starting inference for video: 3 Parking Area
[00:27:16.692] Browser LOG     %c[Vercel Web Analytics]%c Debug mode is enabled by default in development. No requests will be sent to the server. color: rgb(120, 120, 120) color: inherit
[00:27:16.692] Browser LOG     %c[Vercel Web Analytics]%c Running queued event color: rgb(120, 120, 120) color: inherit pageview {"path":"/admin/monitoring","route":"/admin/monitoring"}
[00:27:16.692] Browser LOG     %c[Vercel Web Analytics]%c [pageview] http://localhost:3000/admin/monitoring color: rgb(120, 120, 120) color: inherit {"dp":"/admin/monitoring","o":"http://localhost:3000/admin/monitoring","sdkn":"@vercel/analytics/next","sdkv":"1.5.0","sv":"0.1.3","ts":1763753507818}
[00:27:17.133] Server  ERROR   Backend returned 400
[00:27:17.152] Server  ERROR   Backend returned 400
[00:27:17.166] Server  ERROR   Backend returned 400
[00:27:17.271] Browser WARN    Inference API failed, using simulation 400
[00:27:17.271] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.28425245331502486
[00:27:17.271] Browser WARN    Inference API failed, using simulation 400
[00:27:17.271] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2774178517396443
[00:27:17.271] Browser WARN    Inference API failed, using simulation 400
[00:27:17.272] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.14515217215373355
[00:27:17.624] Server  ERROR   Backend returned 400
[00:27:17.641] Server  ERROR   Backend returned 400
[00:27:17.658] Server  ERROR   Backend returned 400
[00:27:17.762] Browser WARN    Inference API failed, using simulation 400
[00:27:17.762] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3279731804808506
[00:27:17.762] Browser WARN    Inference API failed, using simulation 400
[00:27:17.762] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7844683748501016
[00:27:17.762] Browser WARN    Inference API failed, using simulation 400
[00:27:17.762] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9001859257126833
[00:27:18.134] Server  ERROR   Backend returned 400
[00:27:18.162] Server  ERROR   Backend returned 400
[00:27:18.188] Server  ERROR   Backend returned 400
[00:27:18.293] Browser WARN    Inference API failed, using simulation 400
[00:27:18.293] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.01382863021788322
[00:27:18.293] Browser WARN    Inference API failed, using simulation 400
[00:27:18.293] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.025492671938401656
[00:27:18.293] Browser WARN    Inference API failed, using simulation 400
[00:27:18.293] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.753764231207962
[00:27:18.901] Browser INFO    %cDownload the React DevTools for a better development experience: https://react.dev/link/react-devtools font-weight:bold
[00:27:19.248] Browser LOG     Starting inference for videos: [1,2,3]
[00:27:19.248] Browser LOG     Starting inference for video: 1 Gate 1 Entrance
[00:27:19.248] Browser LOG     Starting inference for video: 2 Main Hall
[00:27:19.248] Browser LOG     Starting inference for video: 3 Parking Area
[00:27:19.248] Browser LOG     %c[Vercel Web Analytics]%c Debug mode is enabled by default in development. No requests will be sent to the server. color: rgb(120, 120, 120) color: inherit
[00:27:19.248] Browser LOG     %c[Vercel Web Analytics]%c Running queued event color: rgb(120, 120, 120) color: inherit pageview {"path":"/admin/monitoring","route":"/admin/monitoring"}
[00:27:19.248] Browser LOG     %c[Vercel Web Analytics]%c [pageview] http://localhost:3000/admin/monitoring color: rgb(120, 120, 120) color: inherit {"dp":"/admin/monitoring","o":"http://localhost:3000/admin/monitoring","sdkn":"@vercel/analytics/next","sdkv":"1.5.0","sv":"0.1.3","ts":1763753510373}
[00:27:19.613] Server  ERROR   Backend returned 400
[00:27:19.634] Server  ERROR   Backend returned 400
[00:27:19.657] Server  ERROR   Backend returned 400
[00:27:19.768] Browser WARN    Inference API failed, using simulation 400
[00:27:19.768] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3783249465085781
[00:27:19.768] Browser WARN    Inference API failed, using simulation 400
[00:27:19.768] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.11360787547909112
[00:27:19.768] Browser WARN    Inference API failed, using simulation 400
[00:27:19.768] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2683514903960272
[00:27:20.078] Server  ERROR   Backend returned 400
[00:27:20.108] Server  ERROR   Backend returned 400
[00:27:20.124] Server  ERROR   Backend returned 400
[00:27:20.231] Browser WARN    Inference API failed, using simulation 400
[00:27:20.231] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.022084660375623844
[00:27:20.231] Browser WARN    Inference API failed, using simulation 400
[00:27:20.231] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8253063376245455
[00:27:20.231] Browser WARN    Inference API failed, using simulation 400
[00:27:20.231] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3659474271567057
[00:27:20.580] Server  ERROR   Backend returned 400
[00:27:20.598] Server  ERROR   Backend returned 400
[00:27:20.619] Server  ERROR   Backend returned 400
[00:27:20.724] Browser WARN    Inference API failed, using simulation 400
[00:27:20.724] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.33664495553523177
[00:27:20.724] Browser WARN    Inference API failed, using simulation 400
[00:27:20.724] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.01749599533380558
[00:27:20.724] Browser WARN    Inference API failed, using simulation 400
[00:27:20.724] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.045938959190125084
[00:27:21.085] Server  ERROR   Backend returned 400
[00:27:21.113] Server  ERROR   Backend returned 400
[00:27:21.118] Server  ERROR   Backend returned 400
[00:27:21.237] Browser WARN    Inference API failed, using simulation 400
[00:27:21.237] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.404090256130641
[00:27:21.237] Browser WARN    Inference API failed, using simulation 400
[00:27:21.237] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.037491258721427
[00:27:21.237] Browser WARN    Inference API failed, using simulation 400
[00:27:21.237] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.28461167916417895
[00:27:21.581] Server  ERROR   Backend returned 400
[00:27:21.601] Server  ERROR   Backend returned 400
[00:27:21.622] Server  ERROR   Backend returned 400
[00:27:21.726] Browser WARN    Inference API failed, using simulation 400
[00:27:21.726] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.26405716071456586
[00:27:21.726] Browser WARN    Inference API failed, using simulation 400
[00:27:21.726] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7567494544946867
[00:27:21.726] Browser WARN    Inference API failed, using simulation 400
[00:27:21.726] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2720754139639046
[00:27:22.082] Server  ERROR   Backend returned 400
[00:27:22.101] Server  ERROR   Backend returned 400
[00:27:22.116] Server  ERROR   Backend returned 400
[00:27:22.220] Browser WARN    Inference API failed, using simulation 400
[00:27:22.220] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.10816351926918455
[00:27:22.220] Browser WARN    Inference API failed, using simulation 400
[00:27:22.220] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9141372034545217
[00:27:22.220] Browser WARN    Inference API failed, using simulation 400
[00:27:22.220] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.19454749917505987
[00:27:22.582] Server  ERROR   Backend returned 400
[00:27:22.601] Server  ERROR   Backend returned 400
[00:27:22.615] Server  ERROR   Backend returned 400
[00:27:22.719] Browser WARN    Inference API failed, using simulation 400
[00:27:22.719] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.249106893948427
[00:27:22.719] Browser WARN    Inference API failed, using simulation 400
[00:27:22.719] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.38530801203432563
[00:27:22.719] Browser WARN    Inference API failed, using simulation 400
[00:27:22.719] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.07684022971311921
[00:27:23.080] Server  ERROR   Backend returned 400
[00:27:23.117] Server  ERROR   Backend returned 400
[00:27:23.120] Server  ERROR   Backend returned 400
[00:27:23.231] Browser WARN    Inference API failed, using simulation 400
[00:27:23.231] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.19699518482535522
[00:27:23.231] Browser WARN    Inference API failed, using simulation 400
[00:27:23.231] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.33900008783162267
[00:27:23.231] Browser WARN    Inference API failed, using simulation 400
[00:27:23.231] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7585724042935922
[00:27:23.585] Server  ERROR   Backend returned 400
[00:27:23.600] Server  ERROR   Backend returned 400
[00:27:23.617] Server  ERROR   Backend returned 400
[00:27:23.720] Browser WARN    Inference API failed, using simulation 400
[00:27:23.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.05406975332432479
[00:27:23.720] Browser WARN    Inference API failed, using simulation 400
[00:27:23.720] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.190258794238155
[00:27:23.721] Browser WARN    Inference API failed, using simulation 400
[00:27:23.721] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7318688238415318
[00:27:24.075] Server  ERROR   Backend returned 400
[00:27:24.107] Server  ERROR   Backend returned 400
[00:27:24.121] Server  ERROR   Backend returned 400
[00:27:24.225] Browser WARN    Inference API failed, using simulation 400
[00:27:24.225] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.021387238740671277
[00:27:24.225] Browser WARN    Inference API failed, using simulation 400
[00:27:24.225] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.19350288735211363
[00:27:24.225] Browser WARN    Inference API failed, using simulation 400
[00:27:24.225] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.06926108288058419
[00:27:24.581] Server  ERROR   Backend returned 400
[00:27:24.612] Server  ERROR   Backend returned 400
[00:27:24.626] Server  ERROR   Backend returned 400
[00:27:24.730] Browser WARN    Inference API failed, using simulation 400
[00:27:24.730] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4895953342722653
[00:27:24.730] Browser WARN    Inference API failed, using simulation 400
[00:27:24.730] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7960761427162799
[00:27:24.730] Browser WARN    Inference API failed, using simulation 400
[00:27:24.730] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2666724366298418
[00:27:25.079] Server  ERROR   Backend returned 400
[00:27:25.107] Server  ERROR   Backend returned 400
[00:27:25.121] Server  ERROR   Backend returned 400
[00:27:25.225] Browser WARN    Inference API failed, using simulation 400
[00:27:25.225] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.710265562144292
[00:27:25.225] Browser WARN    Inference API failed, using simulation 400
[00:27:25.225] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8872026381891821
[00:27:25.225] Browser WARN    Inference API failed, using simulation 400
[00:27:25.225] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.14885127536773884
[00:27:25.578] Server  ERROR   Backend returned 400
[00:27:25.596] Server  ERROR   Backend returned 400
[00:27:25.613] Server  ERROR   Backend returned 400
[00:27:25.718] Browser WARN    Inference API failed, using simulation 400
[00:27:25.718] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9744672761406019
[00:27:25.718] Browser WARN    Inference API failed, using simulation 400
[00:27:25.718] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.12649609041639454
[00:27:25.718] Browser WARN    Inference API failed, using simulation 400
[00:27:25.718] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7377365264730779
[00:27:26.082] Server  ERROR   Backend returned 400
[00:27:26.113] Server  ERROR   Backend returned 400
[00:27:26.116] Server  ERROR   Backend returned 400
[00:27:26.228] Browser WARN    Inference API failed, using simulation 400
[00:27:26.228] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.04595562313690327
[00:27:26.228] Browser WARN    Inference API failed, using simulation 400
[00:27:26.228] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.013206654185160038
[00:27:26.228] Browser WARN    Inference API failed, using simulation 400
[00:27:26.228] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9815668216911371
[00:27:26.579] Server  ERROR   Backend returned 400
[00:27:26.607] Server  ERROR   Backend returned 400
[00:27:26.624] Server  ERROR   Backend returned 400
[00:27:26.729] Browser WARN    Inference API failed, using simulation 400
[00:27:26.729] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2846663326513222
[00:27:26.729] Browser WARN    Inference API failed, using simulation 400
[00:27:26.729] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4714310084846853
[00:27:26.729] Browser WARN    Inference API failed, using simulation 400
[00:27:26.729] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.08546368795621362
[00:27:27.102] Server  ERROR   Backend returned 400
[00:27:27.141] Server  ERROR   Backend returned 400
[00:27:27.161] Server  ERROR   Backend returned 400
[00:27:27.267] Browser WARN    Inference API failed, using simulation 400
[00:27:27.267] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.33550644881208413
[00:27:27.267] Browser WARN    Inference API failed, using simulation 400
[00:27:27.267] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8696950749634781
[00:27:27.267] Browser WARN    Inference API failed, using simulation 400
[00:27:27.267] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1390252158350378
[00:27:27.593] Server  ERROR   Backend returned 400
[00:27:27.611] Server  ERROR   Backend returned 400
[00:27:27.623] Server  ERROR   Backend returned 400
[00:27:27.727] Browser WARN    Inference API failed, using simulation 400
[00:27:27.727] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4902699317735449
[00:27:27.727] Browser WARN    Inference API failed, using simulation 400
[00:27:27.727] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8691418023590447
[00:27:27.727] Browser WARN    Inference API failed, using simulation 400
[00:27:27.727] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.38326603131716536
[00:27:28.078] Server  ERROR   Backend returned 400
[00:27:28.110] Server  ERROR   Backend returned 400
[00:27:28.113] Server  ERROR   Backend returned 400
[00:27:28.223] Browser WARN    Inference API failed, using simulation 400
[00:27:28.223] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8094391742867947
[00:27:28.223] Browser WARN    Inference API failed, using simulation 400
[00:27:28.223] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8500015580827223
[00:27:28.223] Browser WARN    Inference API failed, using simulation 400
[00:27:28.223] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7767348529317677
[00:27:28.573] Server  ERROR   Backend returned 400
[00:27:28.602] Server  ERROR   Backend returned 400
[00:27:28.622] Server  ERROR   Backend returned 400
[00:27:28.728] Browser WARN    Inference API failed, using simulation 400
[00:27:28.728] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.02993255811435075
[00:27:28.728] Browser WARN    Inference API failed, using simulation 400
[00:27:28.728] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.43373175072957254
[00:27:28.728] Browser WARN    Inference API failed, using simulation 400
[00:27:28.728] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.08843103542594904
[00:27:29.078] Server  ERROR   Backend returned 400
[00:27:29.112] Server  ERROR   Backend returned 400
[00:27:29.115] Server  ERROR   Backend returned 400
[00:27:29.247] Browser WARN    Inference API failed, using simulation 400
[00:27:29.247] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.036361757825537966
[00:27:29.247] Browser WARN    Inference API failed, using simulation 400
[00:27:29.247] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9068697490903178
[00:27:29.247] Browser WARN    Inference API failed, using simulation 400
[00:27:29.247] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2272314927204634
[00:27:29.575] Server  ERROR   Backend returned 400
[00:27:29.611] Server  ERROR   Backend returned 400
[00:27:29.614] Server  ERROR   Backend returned 400
[00:27:29.723] Browser WARN    Inference API failed, using simulation 400
[00:27:29.723] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.08786262705436976
[00:27:29.723] Browser WARN    Inference API failed, using simulation 400
[00:27:29.723] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.457315919398026
[00:27:29.723] Browser WARN    Inference API failed, using simulation 400
[00:27:29.723] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.06485653638925448
[00:27:30.083] Server  ERROR   Backend returned 400
[00:27:30.107] Server  ERROR   Backend returned 400
[00:27:30.110] Server  ERROR   Backend returned 400
[00:27:30.221] Browser WARN    Inference API failed, using simulation 400
[00:27:30.221] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.42048100498279467
[00:27:30.221] Browser WARN    Inference API failed, using simulation 400
[00:27:30.221] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.42214160372838777
[00:27:30.221] Browser WARN    Inference API failed, using simulation 400
[00:27:30.221] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.11192043191390194
[00:27:30.581] Server  ERROR   Backend returned 400
[00:27:30.611] Server  ERROR   Backend returned 400
[00:27:30.613] Server  ERROR   Backend returned 400
[00:27:30.728] Browser WARN    Inference API failed, using simulation 400
[00:27:30.728] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.09063927524932919
[00:27:30.728] Browser WARN    Inference API failed, using simulation 400
[00:27:30.728] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8635273005313828
[00:27:30.728] Browser WARN    Inference API failed, using simulation 400
[00:27:30.728] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3807516482921509
[00:27:31.075] Server  ERROR   Backend returned 400
[00:27:31.105] Server  ERROR   Backend returned 400
[00:27:31.119] Server  ERROR   Backend returned 400
[00:27:31.223] Browser WARN    Inference API failed, using simulation 400
[00:27:31.223] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8962217837759502
[00:27:31.223] Browser WARN    Inference API failed, using simulation 400
[00:27:31.223] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.45290472150898536
[00:27:31.223] Browser WARN    Inference API failed, using simulation 400
[00:27:31.223] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.39591435092091254
[00:27:31.581] Server  ERROR   Backend returned 400
[00:27:31.606] Server  ERROR   Backend returned 400
[00:27:31.618] Server  ERROR   Backend returned 400
[00:27:31.723] Browser WARN    Inference API failed, using simulation 400
[00:27:31.723] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9164771520546798
[00:27:31.723] Browser WARN    Inference API failed, using simulation 400
[00:27:31.723] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7975457729461741
[00:27:31.723] Browser WARN    Inference API failed, using simulation 400
[00:27:31.723] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.09513235098281342
[00:27:32.077] Server  ERROR   Backend returned 400
[00:27:32.109] Server  ERROR   Backend returned 400
[00:27:32.114] Server  ERROR   Backend returned 400
[00:27:32.222] Browser WARN    Inference API failed, using simulation 400
[00:27:32.222] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.04032604730211686
[00:27:32.222] Browser WARN    Inference API failed, using simulation 400
[00:27:32.222] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3652690633207246
[00:27:32.222] Browser WARN    Inference API failed, using simulation 400
[00:27:32.222] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9468762350639073
[00:27:32.575] Server  ERROR   Backend returned 400
[00:27:32.614] Server  ERROR   Backend returned 400
[00:27:32.617] Server  ERROR   Backend returned 400
[00:27:32.728] Browser WARN    Inference API failed, using simulation 400
[00:27:32.728] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8586021078455377
[00:27:32.728] Browser WARN    Inference API failed, using simulation 400
[00:27:32.728] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.06631162800012469
[00:27:32.728] Browser WARN    Inference API failed, using simulation 400
[00:27:32.728] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.10769343373483536
[00:27:33.077] Server  ERROR   Backend returned 400
[00:27:33.107] Server  ERROR   Backend returned 400
[00:27:33.119] Server  ERROR   Backend returned 400
[00:27:33.224] Browser WARN    Inference API failed, using simulation 400
[00:27:33.224] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8986097632830732
[00:27:33.224] Browser WARN    Inference API failed, using simulation 400
[00:27:33.224] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.054309368007788295
[00:27:33.224] Browser WARN    Inference API failed, using simulation 400
[00:27:33.224] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.41640029631945724
[00:27:33.576] Server  ERROR   Backend returned 400
[00:27:33.603] Server  ERROR   Backend returned 400
[00:27:33.606] Server  ERROR   Backend returned 400
[00:27:33.736] Browser WARN    Inference API failed, using simulation 400
[00:27:33.736] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.810659357194701
[00:27:33.736] Browser WARN    Inference API failed, using simulation 400
[00:27:33.736] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.775101727775685
[00:27:33.736] Browser WARN    Inference API failed, using simulation 400
[00:27:33.736] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9416128073654484
[00:27:34.082] Server  ERROR   Backend returned 400
[00:27:34.109] Server  ERROR   Backend returned 400
[00:27:34.112] Server  ERROR   Backend returned 400
[00:27:34.223] Browser WARN    Inference API failed, using simulation 400
[00:27:34.223] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.019246874853908413
[00:27:34.223] Browser WARN    Inference API failed, using simulation 400
[00:27:34.223] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3881827787362255
[00:27:34.223] Browser WARN    Inference API failed, using simulation 400
[00:27:34.223] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.479295953606879
[00:27:34.575] Server  ERROR   Backend returned 400
[00:27:34.613] Server  ERROR   Backend returned 400
[00:27:34.616] Server  ERROR   Backend returned 400
[00:27:34.727] Browser WARN    Inference API failed, using simulation 400
[00:27:34.727] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8405016101509178
[00:27:34.727] Browser WARN    Inference API failed, using simulation 400
[00:27:34.727] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.993891367730807
[00:27:34.727] Browser WARN    Inference API failed, using simulation 400
[00:27:34.727] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.34100448236040415
[00:27:35.080] Server  ERROR   Backend returned 400
[00:27:35.102] Server  ERROR   Backend returned 400
[00:27:35.116] Server  ERROR   Backend returned 400
[00:27:35.219] Browser WARN    Inference API failed, using simulation 400
[00:27:35.219] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9039498318077752
[00:27:35.219] Browser WARN    Inference API failed, using simulation 400
[00:27:35.219] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.31368768262338725
[00:27:35.219] Browser WARN    Inference API failed, using simulation 400
[00:27:35.219] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4676479293373412
[00:27:35.578] Server  ERROR   Backend returned 400
[00:27:35.602] Server  ERROR   Backend returned 400
[00:27:35.607] Server  ERROR   Backend returned 400
[00:27:35.728] Browser WARN    Inference API failed, using simulation 400
[00:27:35.728] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8809042579229078
[00:27:35.728] Browser WARN    Inference API failed, using simulation 400
[00:27:35.728] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3447178116115841
[00:27:35.728] Browser WARN    Inference API failed, using simulation 400
[00:27:35.728] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.20605787913554352
[00:27:36.085] Server  ERROR   Backend returned 400
[00:27:36.099] Server  ERROR   Backend returned 400
[00:27:36.116] Server  ERROR   Backend returned 400
[00:27:36.222] Browser WARN    Inference API failed, using simulation 400
[00:27:36.222] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.22979228580109712
[00:27:36.222] Browser WARN    Inference API failed, using simulation 400
[00:27:36.222] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.823927590054289
[00:27:36.222] Browser WARN    Inference API failed, using simulation 400
[00:27:36.222] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.17501598386979655
[00:27:36.584] Server  ERROR   Backend returned 400
[00:27:36.622] Server  ERROR   Backend returned 400
[00:27:36.638] Server  ERROR   Backend returned 400
[00:27:36.745] Browser WARN    Inference API failed, using simulation 400
[00:27:36.745] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8781834021895203
[00:27:36.745] Browser WARN    Inference API failed, using simulation 400
[00:27:36.745] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3771430168848016
[00:27:36.745] Browser WARN    Inference API failed, using simulation 400
[00:27:36.745] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7834270231048186
[00:27:37.096] Server  ERROR   Backend returned 400
[00:27:37.112] Server  ERROR   Backend returned 400
[00:27:37.125] Server  ERROR   Backend returned 400
[00:27:37.229] Browser WARN    Inference API failed, using simulation 400
[00:27:37.230] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7774463003973263
[00:27:37.230] Browser WARN    Inference API failed, using simulation 400
[00:27:37.230] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.04123571069157339
[00:27:37.230] Browser WARN    Inference API failed, using simulation 400
[00:27:37.230] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8619656579857394
[00:27:37.589] Server  ERROR   Backend returned 400
[00:27:37.616] Server  ERROR   Backend returned 400
[00:27:37.630] Server  ERROR   Backend returned 400
[00:27:37.736] Browser WARN    Inference API failed, using simulation 400
[00:27:37.736] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.30491013796691574
[00:27:37.736] Browser WARN    Inference API failed, using simulation 400
[00:27:37.736] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.262832422008278
[00:27:37.736] Browser WARN    Inference API failed, using simulation 400
[00:27:37.736] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2962328474990791
[00:27:38.104] Server  ERROR   Backend returned 400
[00:27:38.138] Server  ERROR   Backend returned 400
[00:27:38.170] Server  ERROR   Backend returned 400
[00:27:38.275] Browser WARN    Inference API failed, using simulation 400
[00:27:38.275] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.03441807058691587
[00:27:38.275] Browser WARN    Inference API failed, using simulation 400
[00:27:38.275] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9722099477058691
[00:27:38.275] Browser WARN    Inference API failed, using simulation 400
[00:27:38.275] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.24761121679218445
[00:27:38.596] Server  ERROR   Backend returned 400
[00:27:38.616] Server  ERROR   Backend returned 400
[00:27:38.632] Server  ERROR   Backend returned 400
[00:27:38.736] Browser WARN    Inference API failed, using simulation 400
[00:27:38.736] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.04994789803755739
[00:27:38.736] Browser WARN    Inference API failed, using simulation 400
[00:27:38.736] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.44663252136183823
[00:27:38.736] Browser WARN    Inference API failed, using simulation 400
[00:27:38.736] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.712313661305945
[00:27:39.114] Server  ERROR   Backend returned 400
[00:27:39.132] Server  ERROR   Backend returned 400
[00:27:39.145] Server  ERROR   Backend returned 400
[00:27:39.250] Browser WARN    Inference API failed, using simulation 400
[00:27:39.250] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4116505423173786
[00:27:39.250] Browser WARN    Inference API failed, using simulation 400
[00:27:39.250] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.36246090025905
[00:27:39.250] Browser WARN    Inference API failed, using simulation 400
[00:27:39.250] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2116750948318044
[00:27:39.593] Server  ERROR   Backend returned 400
[00:27:39.625] Server  ERROR   Backend returned 400
[00:27:39.638] Server  ERROR   Backend returned 400
[00:27:39.744] Browser WARN    Inference API failed, using simulation 400
[00:27:39.744] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4959957045593348
[00:27:39.744] Browser WARN    Inference API failed, using simulation 400
[00:27:39.744] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.11853526353839605
[00:27:39.744] Browser WARN    Inference API failed, using simulation 400
[00:27:39.744] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.32992599445031606
[00:27:40.080] Server  ERROR   Backend returned 400
[00:27:40.112] Server  ERROR   Backend returned 400
[00:27:40.120] Server  ERROR   Backend returned 400
[00:27:40.227] Browser WARN    Inference API failed, using simulation 400
[00:27:40.227] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.41486725367479904
[00:27:40.227] Browser WARN    Inference API failed, using simulation 400
[00:27:40.227] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.05927416918981998
[00:27:40.227] Browser WARN    Inference API failed, using simulation 400
[00:27:40.227] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4390605030702008
[00:27:40.587] Server  ERROR   Backend returned 400
[00:27:40.614] Server  ERROR   Backend returned 400
[00:27:40.617] Server  ERROR   Backend returned 400
[00:27:40.727] Browser WARN    Inference API failed, using simulation 400
[00:27:40.727] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3666719507917164
[00:27:40.727] Browser WARN    Inference API failed, using simulation 400
[00:27:40.727] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1436830117108211
[00:27:40.727] Browser WARN    Inference API failed, using simulation 400
[00:27:40.727] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.08245130039640564
[00:27:41.089] Server  ERROR   Backend returned 400
[00:27:41.106] Server  ERROR   Backend returned 400
[00:27:41.123] Server  ERROR   Backend returned 400
[00:27:41.234] Browser WARN    Inference API failed, using simulation 400
[00:27:41.234] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.27805361393225064
[00:27:41.234] Browser WARN    Inference API failed, using simulation 400
[00:27:41.234] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.020097138783525204
[00:27:41.234] Browser WARN    Inference API failed, using simulation 400
[00:27:41.234] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.07880745594266325
[00:27:41.621] Server  ERROR   Backend returned 400
[00:27:41.703] Server  ERROR   Backend returned 400
[00:27:41.720] Server  ERROR   Backend returned 400
[00:27:41.827] Browser WARN    Inference API failed, using simulation 400
[00:27:41.827] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9040104235071456
[00:27:41.827] Browser WARN    Inference API failed, using simulation 400
[00:27:41.827] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.37500670492034405
[00:27:41.827] Browser WARN    Inference API failed, using simulation 400
[00:27:41.827] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7917796198574754
[00:27:42.208] Server  ERROR   Backend returned 400
[00:27:42.231] Server  ERROR   Backend returned 400
[00:27:42.242] Server  ERROR   Backend returned 400
[00:27:43.176] Browser WARN    Inference API failed, using simulation 400
[00:27:43.176] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7532571390636974
[00:27:43.176] Browser WARN    Inference API failed, using simulation 400
[00:27:43.176] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8718024579781073
[00:27:43.176] Browser WARN    Inference API failed, using simulation 400
[00:27:43.176] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4963751523158367
[00:27:43.214] Server  ERROR   Backend returned 400
[00:27:43.235] Server  ERROR   Backend returned 400
[00:27:43.249] Server  ERROR   Backend returned 400
[00:27:44.171] Browser WARN    Inference API failed, using simulation 400
[00:27:44.171] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.497206637179943
[00:27:44.171] Browser WARN    Inference API failed, using simulation 400
[00:27:44.171] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4607557801075008
[00:27:44.171] Browser WARN    Inference API failed, using simulation 400
[00:27:44.171] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.29789041044119213
[00:27:44.236] Server  ERROR   Backend returned 400
[00:27:44.254] Server  ERROR   Backend returned 400
[00:27:44.269] Server  ERROR   Backend returned 400
[00:27:45.167] Browser WARN    Inference API failed, using simulation 400
[00:27:45.167] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.06972872745640962
[00:27:45.167] Browser WARN    Inference API failed, using simulation 400
[00:27:45.167] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.19517662995551982
[00:27:45.167] Browser WARN    Inference API failed, using simulation 400
[00:27:45.167] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.17715170714072925
[00:27:45.241] Server  ERROR   Backend returned 400
[00:27:45.268] Server  ERROR   Backend returned 400
[00:27:45.286] Server  ERROR   Backend returned 400
[00:27:46.164] Browser WARN    Inference API failed, using simulation 400
[00:27:46.164] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3868956411300923
[00:27:46.164] Browser WARN    Inference API failed, using simulation 400
[00:27:46.164] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.16658588059792906
[00:27:46.164] Browser WARN    Inference API failed, using simulation 400
[00:27:46.164] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9306705912390139
[00:27:46.231] Server  ERROR   Backend returned 400
[00:27:46.255] Server  ERROR   Backend returned 400
[00:27:46.262] Server  ERROR   Backend returned 400
[00:27:47.175] Browser WARN    Inference API failed, using simulation 400
[00:27:47.175] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3238671699076383
[00:27:47.175] Browser WARN    Inference API failed, using simulation 400
[00:27:47.175] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4729974476471767
[00:27:47.175] Browser WARN    Inference API failed, using simulation 400
[00:27:47.175] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3228724884898066
[00:27:47.244] Server  ERROR   Backend returned 400
[00:27:47.274] Server  ERROR   Backend returned 400
[00:27:47.276] Server  ERROR   Backend returned 400
[00:27:48.176] Browser WARN    Inference API failed, using simulation 400
[00:27:48.176] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.21070383264074066
[00:27:48.176] Browser WARN    Inference API failed, using simulation 400
[00:27:48.176] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8207082821034289
[00:27:48.176] Browser WARN    Inference API failed, using simulation 400
[00:27:48.176] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9150321101593442
[00:27:48.218] Server  ERROR   Backend returned 400
[00:27:48.232] Server  ERROR   Backend returned 400
[00:27:48.243] Server  ERROR   Backend returned 400
[00:27:49.173] Browser WARN    Inference API failed, using simulation 400
[00:27:49.174] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4779581190267895
[00:27:49.174] Browser WARN    Inference API failed, using simulation 400
[00:27:49.174] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.44201250898504835
[00:27:49.174] Browser WARN    Inference API failed, using simulation 400
[00:27:49.174] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.23875611137325148
[00:27:49.211] Server  ERROR   Backend returned 400
[00:27:49.234] Server  ERROR   Backend returned 400
[00:27:49.246] Server  ERROR   Backend returned 400
[00:27:49.684] Browser WARN    Inference API failed, using simulation 400
[00:27:49.684] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3625896350330471
[00:27:49.684] Browser WARN    Inference API failed, using simulation 400
[00:27:49.684] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7183000776454248
[00:27:49.684] Browser WARN    Inference API failed, using simulation 400
[00:27:49.684] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4417919159426753
[00:27:49.816] Server  ERROR   Backend returned 400
[00:27:49.846] Server  ERROR   Backend returned 400
[00:27:49.867] Server  ERROR   Backend returned 400
[00:27:49.972] Browser WARN    Inference API failed, using simulation 400
[00:27:49.972] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7316662788241098
[00:27:49.972] Browser WARN    Inference API failed, using simulation 400
[00:27:49.972] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1668351649972405
[00:27:49.972] Browser WARN    Inference API failed, using simulation 400
[00:27:49.972] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.46664647917010565
[00:27:50.085] Server  ERROR   Backend returned 400
[00:27:50.123] Server  ERROR   Backend returned 400
[00:27:50.145] Server  ERROR   Backend returned 400
[00:27:50.249] Browser WARN    Inference API failed, using simulation 400
[00:27:50.249] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.32313981238974837
[00:27:50.249] Browser WARN    Inference API failed, using simulation 400
[00:27:50.249] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4935304010269445
[00:27:50.249] Browser WARN    Inference API failed, using simulation 400
[00:27:50.249] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9737829355247032
[00:27:50.576] Server  ERROR   Backend returned 400
[00:27:50.601] Server  ERROR   Backend returned 400
[00:27:50.608] Server  ERROR   Backend returned 400
[00:27:50.718] Browser WARN    Inference API failed, using simulation 400
[00:27:50.718] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4921052304180285
[00:27:50.718] Browser WARN    Inference API failed, using simulation 400
[00:27:50.718] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8356897791533962
[00:27:50.718] Browser WARN    Inference API failed, using simulation 400
[00:27:50.718] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7156474165399316
[00:27:51.076] Server  ERROR   Backend returned 400
[00:27:51.119] Server  ERROR   Backend returned 400
[00:27:51.123] Server  ERROR   Backend returned 400
[00:27:51.234] Browser WARN    Inference API failed, using simulation 400
[00:27:51.235] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8878737536448196
[00:27:51.235] Browser WARN    Inference API failed, using simulation 400
[00:27:51.235] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4857497010776708
[00:27:51.235] Browser WARN    Inference API failed, using simulation 400
[00:27:51.235] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8754789131056501
[00:27:51.580] Server  ERROR   Backend returned 400
[00:27:51.609] Server  ERROR   Backend returned 400
[00:27:51.612] Server  ERROR   Backend returned 400
[00:27:51.722] Browser WARN    Inference API failed, using simulation 400
[00:27:51.722] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4781493113397799
[00:27:51.722] Browser WARN    Inference API failed, using simulation 400
[00:27:51.722] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.18998642904720764
[00:27:51.722] Browser WARN    Inference API failed, using simulation 400
[00:27:51.722] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.06715599212304763
[00:27:52.084] Server  ERROR   Backend returned 400
[00:27:52.109] Server  ERROR   Backend returned 400
[00:27:52.126] Server  ERROR   Backend returned 400
[00:27:52.231] Browser WARN    Inference API failed, using simulation 400
[00:27:52.231] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9480621098859036
[00:27:52.231] Browser WARN    Inference API failed, using simulation 400
[00:27:52.231] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.24814502807028826
[00:27:52.231] Browser WARN    Inference API failed, using simulation 400
[00:27:52.231] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3235026463195114
[00:27:52.591] Server  ERROR   Backend returned 400
[00:27:52.618] Server  ERROR   Backend returned 400
[00:27:52.650] Server  ERROR   Backend returned 400
[00:27:52.774] Browser WARN    Inference API failed, using simulation 400
[00:27:52.774] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.03840538138373911
[00:27:52.774] Browser WARN    Inference API failed, using simulation 400
[00:27:52.774] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.726068818741936
[00:27:52.774] Browser WARN    Inference API failed, using simulation 400
[00:27:52.774] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3487791624197746
[00:27:53.106] Server  ERROR   Backend returned 400
[00:27:53.164] Server  ERROR   Backend returned 400
[00:27:53.198] Server  ERROR   Backend returned 400
[00:27:53.302] Browser WARN    Inference API failed, using simulation 400
[00:27:53.302] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4030781841328613
[00:27:53.302] Browser WARN    Inference API failed, using simulation 400
[00:27:53.302] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2593575605506018
[00:27:53.302] Browser WARN    Inference API failed, using simulation 400
[00:27:53.302] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.961118959749155
[00:27:53.588] Server  ERROR   Backend returned 400
[00:27:53.621] Server  ERROR   Backend returned 400
[00:27:53.638] Server  ERROR   Backend returned 400
[00:27:53.743] Browser WARN    Inference API failed, using simulation 400
[00:27:53.743] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.20439784732569843
[00:27:53.743] Browser WARN    Inference API failed, using simulation 400
[00:27:53.743] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3814355139454661
[00:27:53.743] Browser WARN    Inference API failed, using simulation 400
[00:27:53.743] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.14977530333375483
[00:27:54.208] Server  ERROR   Backend returned 400
[00:27:54.226] Server  ERROR   Backend returned 400
[00:27:54.278] Server  ERROR   Backend returned 400
[00:27:55.167] Browser WARN    Inference API failed, using simulation 400
[00:27:55.167] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7491721652413317
[00:27:55.167] Browser WARN    Inference API failed, using simulation 400
[00:27:55.167] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.06278460031457783
[00:27:55.167] Browser WARN    Inference API failed, using simulation 400
[00:27:55.167] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7331446935344783
[00:27:55.210] Server  ERROR   Backend returned 400
[00:27:55.227] Server  ERROR   Backend returned 400
[00:27:55.243] Server  ERROR   Backend returned 400
[00:27:56.175] Browser WARN    Inference API failed, using simulation 400
[00:27:56.175] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4603939946817252
[00:27:56.175] Browser WARN    Inference API failed, using simulation 400
[00:27:56.175] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.36856005191925567
[00:27:56.175] Browser WARN    Inference API failed, using simulation 400
[00:27:56.175] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8682367129872702
[00:27:56.274] Server  ERROR   Backend returned 400
[00:27:56.295] Server  ERROR   Backend returned 400
[00:27:56.308] Server  ERROR   Backend returned 400
[00:27:57.170] Browser WARN    Inference API failed, using simulation 400
[00:27:57.170] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.070734167335051
[00:27:57.170] Browser WARN    Inference API failed, using simulation 400
[00:27:57.170] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.10039254673749043
[00:27:57.170] Browser WARN    Inference API failed, using simulation 400
[00:27:57.170] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.43339087518059477
[00:27:57.215] Server  ERROR   Backend returned 400
[00:27:57.238] Server  ERROR   Backend returned 400
[00:27:57.260] Server  ERROR   Backend returned 400
[00:27:58.165] Browser WARN    Inference API failed, using simulation 400
[00:27:58.165] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3558673338238
[00:27:58.165] Browser WARN    Inference API failed, using simulation 400
[00:27:58.165] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2635243255715391
[00:27:58.165] Browser WARN    Inference API failed, using simulation 400
[00:27:58.165] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2148555538701128
[00:27:58.223] Server  ERROR   Backend returned 400
[00:27:58.246] Server  ERROR   Backend returned 400
[00:27:58.256] Server  ERROR   Backend returned 400
[00:27:59.166] Browser WARN    Inference API failed, using simulation 400
[00:27:59.166] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.20616909487103136
[00:27:59.166] Browser WARN    Inference API failed, using simulation 400
[00:27:59.166] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8754569971302424
[00:27:59.166] Browser WARN    Inference API failed, using simulation 400
[00:27:59.166] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7209097589266621
[00:27:59.200] Server  ERROR   Backend returned 400
[00:27:59.217] Server  ERROR   Backend returned 400
[00:27:59.233] Server  ERROR   Backend returned 400
[00:28:00.162] Browser WARN    Inference API failed, using simulation 400
[00:28:00.162] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7032378019528964
[00:28:00.162] Browser WARN    Inference API failed, using simulation 400
[00:28:00.162] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.20745813110810857
[00:28:00.162] Browser WARN    Inference API failed, using simulation 400
[00:28:00.162] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7433343255325389
[00:28:00.207] Server  ERROR   Backend returned 400
[00:28:00.225] Server  ERROR   Backend returned 400
[00:28:00.238] Server  ERROR   Backend returned 400
[00:28:01.176] Browser WARN    Inference API failed, using simulation 400
[00:28:01.176] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9869311363642199
[00:28:01.176] Browser WARN    Inference API failed, using simulation 400
[00:28:01.176] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4486354539182674
[00:28:01.176] Browser WARN    Inference API failed, using simulation 400
[00:28:01.176] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7260036306042751
[00:28:01.220] Server  ERROR   Backend returned 400
[00:28:01.234] Server  ERROR   Backend returned 400
[00:28:01.248] Server  ERROR   Backend returned 400
[00:28:02.171] Browser WARN    Inference API failed, using simulation 400
[00:28:02.171] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.47543648509448466
[00:28:02.171] Browser WARN    Inference API failed, using simulation 400
[00:28:02.171] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1323026358368129
[00:28:02.171] Browser WARN    Inference API failed, using simulation 400
[00:28:02.171] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7171915968641027
[00:28:02.239] Server  ERROR   Backend returned 400
[00:28:02.266] Server  ERROR   Backend returned 400
[00:28:02.278] Server  ERROR   Backend returned 400
[00:28:03.161] Browser WARN    Inference API failed, using simulation 400
[00:28:03.161] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.21978652979035784
[00:28:03.161] Browser WARN    Inference API failed, using simulation 400
[00:28:03.161] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.48631132084718515
[00:28:03.161] Browser WARN    Inference API failed, using simulation 400
[00:28:03.161] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.952102158020087
[00:28:03.221] Server  ERROR   Backend returned 400
[00:28:03.235] Server  ERROR   Backend returned 400
[00:28:03.247] Server  ERROR   Backend returned 400
[00:28:04.177] Browser WARN    Inference API failed, using simulation 400
[00:28:04.177] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8933523430411762
[00:28:04.177] Browser WARN    Inference API failed, using simulation 400
[00:28:04.177] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.20715129820506234
[00:28:04.177] Browser WARN    Inference API failed, using simulation 400
[00:28:04.177] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9824394972314416
[00:28:05.231] Server  ERROR   Backend returned 400
[00:28:05.246] Server  ERROR   Backend returned 400
[00:28:05.249] Server  ERROR   Backend returned 400
[00:28:06.169] Browser WARN    Inference API failed, using simulation 400
[00:28:06.169] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3151783901382951
[00:28:06.170] Browser WARN    Inference API failed, using simulation 400
[00:28:06.170] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.02690480675766521
[00:28:06.170] Browser WARN    Inference API failed, using simulation 400
[00:28:06.170] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.0055860699434865935
[00:28:06.244] Server  ERROR   Backend returned 400
[00:28:06.246] Server  ERROR   Backend returned 400
[00:28:06.252] Server  ERROR   Backend returned 400
[00:28:07.165] Browser WARN    Inference API failed, using simulation 400
[00:28:07.165] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.32884057038472586
[00:28:07.165] Browser WARN    Inference API failed, using simulation 400
[00:28:07.165] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9769463200938749
[00:28:07.165] Browser WARN    Inference API failed, using simulation 400
[00:28:07.165] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.15828590608149162
[00:28:07.217] Server  ERROR   Backend returned 400
[00:28:07.240] Server  ERROR   Backend returned 400
[00:28:07.253] Server  ERROR   Backend returned 400
[00:28:08.168] Browser WARN    Inference API failed, using simulation 400
[00:28:08.168] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4129391691368856
[00:28:08.168] Browser WARN    Inference API failed, using simulation 400
[00:28:08.168] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.16512988390343286
[00:28:08.168] Browser WARN    Inference API failed, using simulation 400
[00:28:08.168] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2734405978024875
[00:28:08.260] Server  ERROR   Backend returned 400
[00:28:08.267] Server  ERROR   Backend returned 400
[00:28:08.281] Server  ERROR   Backend returned 400
[00:28:09.169] Browser WARN    Inference API failed, using simulation 400
[00:28:09.169] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.40000830630700074
[00:28:09.169] Browser WARN    Inference API failed, using simulation 400
[00:28:09.169] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.04688810222208345
[00:28:09.169] Browser WARN    Inference API failed, using simulation 400
[00:28:09.169] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.06215285495707995
[00:28:09.231] Server  ERROR   Backend returned 400
[00:28:09.234] Server  ERROR   Backend returned 400
[00:28:09.245] Server  ERROR   Backend returned 400
[00:28:10.176] Browser WARN    Inference API failed, using simulation 400
[00:28:10.176] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.36920315358382805
[00:28:10.176] Browser WARN    Inference API failed, using simulation 400
[00:28:10.176] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3768991182012302
[00:28:10.176] Browser WARN    Inference API failed, using simulation 400
[00:28:10.176] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1285656228994475
[00:28:10.255] Server  ERROR   Backend returned 400
[00:28:10.257] Server  ERROR   Backend returned 400
[00:28:10.263] Server  ERROR   Backend returned 400
[00:28:11.172] Browser WARN    Inference API failed, using simulation 400
[00:28:11.172] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.11546630284798415
[00:28:11.172] Browser WARN    Inference API failed, using simulation 400
[00:28:11.172] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.06513625264992623
[00:28:11.172] Browser WARN    Inference API failed, using simulation 400
[00:28:11.172] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.06219815610125917
[00:28:11.223] Server  ERROR   Backend returned 400
[00:28:11.227] Server  ERROR   Backend returned 400
[00:28:11.243] Server  ERROR   Backend returned 400
[00:28:12.165] Browser WARN    Inference API failed, using simulation 400
[00:28:12.165] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.0582153303975701
[00:28:12.165] Browser WARN    Inference API failed, using simulation 400
[00:28:12.165] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4502559577698064
[00:28:12.165] Browser WARN    Inference API failed, using simulation 400
[00:28:12.165] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.39523613390626056
[00:28:12.204] Server  ERROR   Backend returned 400
[00:28:12.218] Server  ERROR   Backend returned 400
[00:28:12.229] Server  ERROR   Backend returned 400
[00:28:13.162] Browser WARN    Inference API failed, using simulation 400
[00:28:13.162] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.0704235947954095
[00:28:13.162] Browser WARN    Inference API failed, using simulation 400
[00:28:13.162] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7542386343781667
[00:28:13.162] Browser WARN    Inference API failed, using simulation 400
[00:28:13.162] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7979932552849868
[00:28:13.337] Server  ERROR   Backend returned 400
[00:28:13.348] Server  ERROR   Backend returned 400
[00:28:13.367] Server  ERROR   Backend returned 400
[00:28:14.162] Browser WARN    Inference API failed, using simulation 400
[00:28:14.162] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.03432397778761964
[00:28:14.162] Browser WARN    Inference API failed, using simulation 400
[00:28:14.162] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.36396761227480084
[00:28:14.162] Browser WARN    Inference API failed, using simulation 400
[00:28:14.162] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.10893613356172765
[00:28:14.235] Server  ERROR   Backend returned 400
[00:28:14.255] Server  ERROR   Backend returned 400
[00:28:14.356] Server  ERROR   Backend returned 400
[00:28:15.162] Browser WARN    Inference API failed, using simulation 400
[00:28:15.162] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.29734490570433936
[00:28:15.162] Browser WARN    Inference API failed, using simulation 400
[00:28:15.162] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.11051796487114607
[00:28:15.162] Browser WARN    Inference API failed, using simulation 400
[00:28:15.162] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8583937999902518
[00:28:15.244] Server  ERROR   Backend returned 400
[00:28:15.295] Server  ERROR   Backend returned 400
[00:28:15.298] Server  ERROR   Backend returned 400
[00:28:16.161] Browser WARN    Inference API failed, using simulation 400
[00:28:16.161] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7695655694304119
[00:28:16.161] Browser WARN    Inference API failed, using simulation 400
[00:28:16.161] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8761987455489667
[00:28:16.161] Browser WARN    Inference API failed, using simulation 400
[00:28:16.161] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.27171521019805744
[00:28:16.217] Server  ERROR   Backend returned 400
[00:28:16.219] Server  ERROR   Backend returned 400
[00:28:16.239] Server  ERROR   Backend returned 400
[00:28:17.162] Browser WARN    Inference API failed, using simulation 400
[00:28:17.162] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8997178532772198
[00:28:17.162] Browser WARN    Inference API failed, using simulation 400
[00:28:17.162] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.49063692887515625
[00:28:17.162] Browser WARN    Inference API failed, using simulation 400
[00:28:17.162] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8437116073894021
[00:28:17.233] Server  ERROR   Backend returned 400
[00:28:17.237] Server  ERROR   Backend returned 400
[00:28:17.244] Server  ERROR   Backend returned 400
[00:28:18.162] Browser WARN    Inference API failed, using simulation 400
[00:28:18.162] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.030162514719185296
[00:28:18.162] Browser WARN    Inference API failed, using simulation 400
[00:28:18.162] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.12084970626652153
[00:28:18.162] Browser WARN    Inference API failed, using simulation 400
[00:28:18.162] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4470968960133393
[00:28:18.254] Server  ERROR   Backend returned 400
[00:28:18.261] Server  ERROR   Backend returned 400
[00:28:18.289] Server  ERROR   Backend returned 400
[00:28:19.162] Browser WARN    Inference API failed, using simulation 400
[00:28:19.162] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7666448871586393
[00:28:19.162] Browser WARN    Inference API failed, using simulation 400
[00:28:19.162] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7454999195299319
[00:28:19.162] Browser WARN    Inference API failed, using simulation 400
[00:28:19.162] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.37168527830310133
[00:28:19.375] Server  ERROR   Backend returned 400
[00:28:19.382] Server  ERROR   Backend returned 400
[00:28:19.399] Server  ERROR   Backend returned 400
[00:28:20.162] Browser WARN    Inference API failed, using simulation 400
[00:28:20.162] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.44188235743630055
[00:28:20.162] Browser WARN    Inference API failed, using simulation 400
[00:28:20.162] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.06303014123387751
[00:28:20.162] Browser WARN    Inference API failed, using simulation 400
[00:28:20.162] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7423075141515589
[00:28:20.260] Server  ERROR   Backend returned 400
[00:28:20.296] Server  ERROR   Backend returned 400
[00:28:20.329] Server  ERROR   Backend returned 400
[00:28:21.162] Browser WARN    Inference API failed, using simulation 400
[00:28:21.162] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.22373521255959739
[00:28:21.162] Browser WARN    Inference API failed, using simulation 400
[00:28:21.162] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.19735858055462135
[00:28:21.162] Browser WARN    Inference API failed, using simulation 400
[00:28:21.162] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.49994386179456296
[00:28:21.245] Server  ERROR   Backend returned 400
[00:28:21.249] Server  ERROR   Backend returned 400
[00:28:21.286] Server  ERROR   Backend returned 400
[00:28:22.170] Browser WARN    Inference API failed, using simulation 400
[00:28:22.170] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8840504829874208
[00:28:22.170] Browser WARN    Inference API failed, using simulation 400
[00:28:22.170] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8913730464644135
[00:28:22.170] Browser WARN    Inference API failed, using simulation 400
[00:28:22.170] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9207075717179001
[00:28:22.249] Server  ERROR   Backend returned 400
[00:28:22.269] Server  ERROR   Backend returned 400
[00:28:22.272] Server  ERROR   Backend returned 400
[00:28:23.161] Browser WARN    Inference API failed, using simulation 400
[00:28:23.161] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.08526356123615852
[00:28:23.161] Browser WARN    Inference API failed, using simulation 400
[00:28:23.161] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3075485123118893
[00:28:23.161] Browser WARN    Inference API failed, using simulation 400
[00:28:23.161] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.08964476752838463
[00:28:23.214] Server  ERROR   Backend returned 400
[00:28:23.237] Server  ERROR   Backend returned 400
[00:28:23.240] Server  ERROR   Backend returned 400
[00:28:24.171] Browser WARN    Inference API failed, using simulation 400
[00:28:24.171] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.07355235087668349
[00:28:24.171] Browser WARN    Inference API failed, using simulation 400
[00:28:24.171] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8400415903735261
[00:28:24.171] Browser WARN    Inference API failed, using simulation 400
[00:28:24.171] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.19127785845645756
[00:28:24.229] Server  ERROR   Backend returned 400
[00:28:24.251] Server  ERROR   Backend returned 400
[00:28:24.261] Server  ERROR   Backend returned 400
[00:28:25.169] Browser WARN    Inference API failed, using simulation 400
[00:28:25.169] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.40887913688758387
[00:28:25.169] Browser WARN    Inference API failed, using simulation 400
[00:28:25.169] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.041609223242620785
[00:28:25.169] Browser WARN    Inference API failed, using simulation 400
[00:28:25.169] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8536701608005037
[00:28:25.225] Server  ERROR   Backend returned 400
[00:28:25.227] Server  ERROR   Backend returned 400
[00:28:25.245] Server  ERROR   Backend returned 400
[00:28:26.176] Browser WARN    Inference API failed, using simulation 400
[00:28:26.177] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7029533821022834
[00:28:26.177] Browser WARN    Inference API failed, using simulation 400
[00:28:26.177] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8394562039042844
[00:28:26.177] Browser WARN    Inference API failed, using simulation 400
[00:28:26.177] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7225569046458954
[00:28:26.250] Server  ERROR   Backend returned 400
[00:28:26.253] Server  ERROR   Backend returned 400
[00:28:26.260] Server  ERROR   Backend returned 400
[00:28:27.161] Browser WARN    Inference API failed, using simulation 400
[00:28:27.161] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.14922124939224407
[00:28:27.161] Browser WARN    Inference API failed, using simulation 400
[00:28:27.161] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4553170955372778
[00:28:27.161] Browser WARN    Inference API failed, using simulation 400
[00:28:27.161] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8347108023688548
[00:28:27.224] Server  ERROR   Backend returned 400
[00:28:27.228] Server  ERROR   Backend returned 400
[00:28:27.244] Server  ERROR   Backend returned 400
[00:28:28.174] Browser WARN    Inference API failed, using simulation 400
[00:28:28.174] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.22690051178501325
[00:28:28.174] Browser WARN    Inference API failed, using simulation 400
[00:28:28.174] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.07864317208027355
[00:28:28.174] Browser WARN    Inference API failed, using simulation 400
[00:28:28.174] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.34640442740802363
[00:28:28.261] Server  ERROR   Backend returned 400
[00:28:28.284] Server  ERROR   Backend returned 400
[00:28:28.292] Server  ERROR   Backend returned 400
[00:28:29.161] Browser WARN    Inference API failed, using simulation 400
[00:28:29.161] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7791956388775234
[00:28:29.161] Browser WARN    Inference API failed, using simulation 400
[00:28:29.161] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9379315475588166
[00:28:29.161] Browser WARN    Inference API failed, using simulation 400
[00:28:29.161] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8854197131938847
[00:28:29.210] Server  ERROR   Backend returned 400
[00:28:29.226] Server  ERROR   Backend returned 400
[00:28:29.240] Server  ERROR   Backend returned 400
[00:28:30.161] Browser WARN    Inference API failed, using simulation 400
[00:28:30.161] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9704011089185997
[00:28:30.161] Browser WARN    Inference API failed, using simulation 400
[00:28:30.162] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4553599863541616
[00:28:30.162] Browser WARN    Inference API failed, using simulation 400
[00:28:30.162] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.27089005269952887
[00:28:30.221] Server  ERROR   Backend returned 400
[00:28:30.244] Server  ERROR   Backend returned 400
[00:28:30.247] Server  ERROR   Backend returned 400
[00:28:31.162] Browser WARN    Inference API failed, using simulation 400
[00:28:31.162] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.11826975899181341
[00:28:31.162] Browser WARN    Inference API failed, using simulation 400
[00:28:31.162] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.38067910083422674
[00:28:31.162] Browser WARN    Inference API failed, using simulation 400
[00:28:31.162] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.08478749776972311
[00:28:31.212] Server  ERROR   Backend returned 400
[00:28:31.216] Server  ERROR   Backend returned 400
[00:28:31.234] Server  ERROR   Backend returned 400
[00:28:32.161] Browser WARN    Inference API failed, using simulation 400
[00:28:32.161] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7635273126004128
[00:28:32.161] Browser WARN    Inference API failed, using simulation 400
[00:28:32.161] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7371608994437343
[00:28:32.161] Browser WARN    Inference API failed, using simulation 400
[00:28:32.161] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.22621012326413703
[00:28:32.210] Server  ERROR   Backend returned 400
[00:28:32.212] Server  ERROR   Backend returned 400
[00:28:32.227] Server  ERROR   Backend returned 400
[00:28:33.164] Browser WARN    Inference API failed, using simulation 400
[00:28:33.164] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8869778412720899
[00:28:33.164] Browser WARN    Inference API failed, using simulation 400
[00:28:33.164] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9951706585305871
[00:28:33.164] Browser WARN    Inference API failed, using simulation 400
[00:28:33.164] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9934229918656167
[00:28:33.213] Server  ERROR   Backend returned 400
[00:28:33.230] Server  ERROR   Backend returned 400
[00:28:33.234] Server  ERROR   Backend returned 400
[00:28:34.170] Browser WARN    Inference API failed, using simulation 400
[00:28:34.170] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8836073858886013
[00:28:34.170] Browser WARN    Inference API failed, using simulation 400
[00:28:34.170] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3358994452702191
[00:28:34.170] Browser WARN    Inference API failed, using simulation 400
[00:28:34.170] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.15671563221788615
[00:28:34.332] Server  ERROR   Backend returned 400
[00:28:34.393] Server  ERROR   Backend returned 400
[00:28:34.413] Server  ERROR   Backend returned 400
[00:28:35.199] Browser WARN    Inference API failed, using simulation 400
[00:28:35.199] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.486280910752006
[00:28:35.199] Browser WARN    Inference API failed, using simulation 400
[00:28:35.199] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3607584128179162
[00:28:35.199] Browser WARN    Inference API failed, using simulation 400
[00:28:35.199] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.08200491425488887
[00:28:35.289] Server  ERROR   Backend returned 400
[00:28:35.343] Server  ERROR   Backend returned 400
[00:28:35.351] Server  ERROR   Backend returned 400
[00:28:36.164] Browser WARN    Inference API failed, using simulation 400
[00:28:36.164] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8899917924785151
[00:28:36.164] Browser WARN    Inference API failed, using simulation 400
[00:28:36.164] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4490236341356466
[00:28:36.164] Browser WARN    Inference API failed, using simulation 400
[00:28:36.164] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9504573054407239
[00:28:36.267] Server  ERROR   Backend returned 400
[00:28:36.271] Server  ERROR   Backend returned 400
[00:28:36.291] Server  ERROR   Backend returned 400
[00:28:37.163] Browser WARN    Inference API failed, using simulation 400
[00:28:37.163] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.35014735886289217
[00:28:37.163] Browser WARN    Inference API failed, using simulation 400
[00:28:37.163] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8117113942202842
[00:28:37.163] Browser WARN    Inference API failed, using simulation 400
[00:28:37.163] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9091874729270262
[00:28:37.251] Server  ERROR   Backend returned 400
[00:28:37.267] Server  ERROR   Backend returned 400
[00:28:37.281] Server  ERROR   Backend returned 400
[00:28:38.161] Browser WARN    Inference API failed, using simulation 400
[00:28:38.162] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.23254061674860355
[00:28:38.162] Browser WARN    Inference API failed, using simulation 400
[00:28:38.162] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2750296911126583
[00:28:38.162] Browser WARN    Inference API failed, using simulation 400
[00:28:38.162] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8318182277341083
[00:28:38.277] Server  ERROR   Backend returned 400
[00:28:38.299] Server  ERROR   Backend returned 400
[00:28:38.313] Server  ERROR   Backend returned 400
[00:28:39.165] Browser WARN    Inference API failed, using simulation 400
[00:28:39.165] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3052102372704609
[00:28:39.165] Browser WARN    Inference API failed, using simulation 400
[00:28:39.165] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.18187960809724957
[00:28:39.165] Browser WARN    Inference API failed, using simulation 400
[00:28:39.165] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4974363336845858
[00:28:39.232] Server  ERROR   Backend returned 400
[00:28:39.235] Server  ERROR   Backend returned 400
[00:28:39.252] Server  ERROR   Backend returned 400
[00:28:40.173] Browser WARN    Inference API failed, using simulation 400
[00:28:40.173] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3888228042294774
[00:28:40.173] Browser WARN    Inference API failed, using simulation 400
[00:28:40.173] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4735675681108586
[00:28:40.173] Browser WARN    Inference API failed, using simulation 400
[00:28:40.173] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.738721781862653
[00:28:40.247] Server  ERROR   Backend returned 400
[00:28:40.278] Server  ERROR   Backend returned 400
[00:28:40.282] Server  ERROR   Backend returned 400
[00:28:41.169] Browser WARN    Inference API failed, using simulation 400
[00:28:41.169] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.030367992354551232
[00:28:41.169] Browser WARN    Inference API failed, using simulation 400
[00:28:41.169] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.036560469388430994
[00:28:41.169] Browser WARN    Inference API failed, using simulation 400
[00:28:41.169] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1130035354346145
[00:28:41.212] Server  ERROR   Backend returned 400
[00:28:41.225] Server  ERROR   Backend returned 400
[00:28:41.242] Server  ERROR   Backend returned 400
[00:28:42.165] Browser WARN    Inference API failed, using simulation 400
[00:28:42.165] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7788631302142686
[00:28:42.165] Browser WARN    Inference API failed, using simulation 400
[00:28:42.165] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.237657915095482
[00:28:42.165] Browser WARN    Inference API failed, using simulation 400
[00:28:42.165] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.37177119922428525
[00:28:42.210] Server  ERROR   Backend returned 400
[00:28:42.230] Server  ERROR   Backend returned 400
[00:28:42.232] Server  ERROR   Backend returned 400
[00:28:43.166] Browser WARN    Inference API failed, using simulation 400
[00:28:43.166] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7863093241161173
[00:28:43.166] Browser WARN    Inference API failed, using simulation 400
[00:28:43.166] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.32739978827123756
[00:28:43.166] Browser WARN    Inference API failed, using simulation 400
[00:28:43.166] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2923921828080345
[00:28:43.223] Server  ERROR   Backend returned 400
[00:28:43.232] Server  ERROR   Backend returned 400
[00:28:43.239] Server  ERROR   Backend returned 400
[00:28:44.162] Browser WARN    Inference API failed, using simulation 400
[00:28:44.162] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.783427797403789
[00:28:44.162] Browser WARN    Inference API failed, using simulation 400
[00:28:44.162] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7588452503021289
[00:28:44.162] Browser WARN    Inference API failed, using simulation 400
[00:28:44.162] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.03730207715063355
[00:28:44.232] Server  ERROR   Backend returned 400
[00:28:44.242] Server  ERROR   Backend returned 400
[00:28:44.244] Server  ERROR   Backend returned 400
[00:28:45.176] Browser WARN    Inference API failed, using simulation 400
[00:28:45.176] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.32317831783578155
[00:28:45.176] Browser WARN    Inference API failed, using simulation 400
[00:28:45.176] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.45548519380366814
[00:28:45.176] Browser WARN    Inference API failed, using simulation 400
[00:28:45.176] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.305655932884937
[00:28:45.227] Server  ERROR   Backend returned 400
[00:28:45.229] Server  ERROR   Backend returned 400
[00:28:45.244] Server  ERROR   Backend returned 400
[00:28:46.167] Browser WARN    Inference API failed, using simulation 400
[00:28:46.167] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.40187007009067177
[00:28:46.167] Browser WARN    Inference API failed, using simulation 400
[00:28:46.167] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9573947281909367
[00:28:46.167] Browser WARN    Inference API failed, using simulation 400
[00:28:46.167] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4667065715516682
[00:28:46.215] Server  ERROR   Backend returned 400
[00:28:46.240] Server  ERROR   Backend returned 400
[00:28:46.243] Server  ERROR   Backend returned 400
[00:28:47.161] Browser WARN    Inference API failed, using simulation 400
[00:28:47.161] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.14901119926726458
[00:28:47.161] Browser WARN    Inference API failed, using simulation 400
[00:28:47.161] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.321584491314074
[00:28:47.161] Browser WARN    Inference API failed, using simulation 400
[00:28:47.161] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.498098666097569
[00:28:47.200] Server  ERROR   Backend returned 400
[00:28:47.213] Server  ERROR   Backend returned 400
[00:28:47.224] Server  ERROR   Backend returned 400
[00:28:48.172] Browser WARN    Inference API failed, using simulation 400
[00:28:48.172] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.49443534472087997
[00:28:48.172] Browser WARN    Inference API failed, using simulation 400
[00:28:48.172] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.925717906869562
[00:28:48.172] Browser WARN    Inference API failed, using simulation 400
[00:28:48.172] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1833193429123992
[00:28:48.232] Server  ERROR   Backend returned 400
[00:28:48.259] Server  ERROR   Backend returned 400
[00:28:48.261] Server  ERROR   Backend returned 400
[00:28:49.169] Browser WARN    Inference API failed, using simulation 400
[00:28:49.169] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.08316361286754426
[00:28:49.169] Browser WARN    Inference API failed, using simulation 400
[00:28:49.169] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4750681256166801
[00:28:49.169] Browser WARN    Inference API failed, using simulation 400
[00:28:49.169] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8722969583206609
[00:28:49.232] Server  ERROR   Backend returned 400
[00:28:49.260] Server  ERROR   Backend returned 400
[00:28:49.265] Server  ERROR   Backend returned 400
[00:28:50.212] Browser WARN    Inference API failed, using simulation 400
[00:28:50.212] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.44084152220039663
[00:28:50.212] Browser WARN    Inference API failed, using simulation 400
[00:28:50.212] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.024673488163428126
[00:28:50.212] Browser WARN    Inference API failed, using simulation 400
[00:28:50.212] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8606011247806
[00:28:50.293] Server  ERROR   Backend returned 400
[00:28:50.320] Server  ERROR   Backend returned 400
[00:28:50.323] Server  ERROR   Backend returned 400
[00:28:51.163] Browser WARN    Inference API failed, using simulation 400
[00:28:51.163] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3930324598610865
[00:28:51.163] Browser WARN    Inference API failed, using simulation 400
[00:28:51.163] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2555987999048652
[00:28:51.163] Browser WARN    Inference API failed, using simulation 400
[00:28:51.163] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.23119811072532753
[00:28:51.266] Server  ERROR   Backend returned 400
[00:28:51.312] Server  ERROR   Backend returned 400
[00:28:51.316] Server  ERROR   Backend returned 400
[00:28:52.162] Browser WARN    Inference API failed, using simulation 400
[00:28:52.163] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.48988926078834194
[00:28:52.163] Browser WARN    Inference API failed, using simulation 400
[00:28:52.163] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.46926268562850976
[00:28:52.163] Browser WARN    Inference API failed, using simulation 400
[00:28:52.163] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9968404211314024
[00:28:52.230] Server  ERROR   Backend returned 400
[00:28:52.233] Server  ERROR   Backend returned 400
[00:28:52.250] Server  ERROR   Backend returned 400
[00:28:53.164] Browser WARN    Inference API failed, using simulation 400
[00:28:53.164] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.08326888854414988
[00:28:53.164] Browser WARN    Inference API failed, using simulation 400
[00:28:53.164] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3297658499944146
[00:28:53.164] Browser WARN    Inference API failed, using simulation 400
[00:28:53.164] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.21357084928224762
[00:28:53.233] Server  ERROR   Backend returned 400
[00:28:53.237] Server  ERROR   Backend returned 400
[00:28:53.247] Server  ERROR   Backend returned 400
[00:28:54.162] Browser WARN    Inference API failed, using simulation 400
[00:28:54.162] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4752542863368044
[00:28:54.162] Browser WARN    Inference API failed, using simulation 400
[00:28:54.162] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.26041022564254546
[00:28:54.162] Browser WARN    Inference API failed, using simulation 400
[00:28:54.162] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7623765680275538
[00:28:54.196] Server  ERROR   Backend returned 400
[00:28:54.219] Server  ERROR   Backend returned 400
[00:28:54.222] Server  ERROR   Backend returned 400
[00:28:55.162] Browser WARN    Inference API failed, using simulation 400
[00:28:55.162] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7990447399786639
[00:28:55.162] Browser WARN    Inference API failed, using simulation 400
[00:28:55.162] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.01679513142456107
[00:28:55.162] Browser WARN    Inference API failed, using simulation 400
[00:28:55.162] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1342101354903722
[00:29:06.199] Server  ERROR   Backend returned 400
[00:29:06.211] Server  ERROR   Backend returned 400
[00:29:06.222] Server  ERROR   Backend returned 400
[00:29:07.162] Browser WARN    Inference API failed, using simulation 400
[00:29:07.162] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.40670731093311785
[00:29:07.162] Browser WARN    Inference API failed, using simulation 400
[00:29:07.162] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4482656474322897
[00:29:07.162] Browser WARN    Inference API failed, using simulation 400
[00:29:07.162] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4108756989448342
[00:30:06.233] Server  ERROR   Backend returned 400
[00:30:06.236] Server  ERROR   Backend returned 400
[00:30:06.246] Server  ERROR   Backend returned 400
[00:30:07.175] Browser WARN    Inference API failed, using simulation 400
[00:30:07.175] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9880817598834002
[00:30:07.175] Browser WARN    Inference API failed, using simulation 400
[00:30:07.175] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4261429580545641
[00:30:07.175] Browser WARN    Inference API failed, using simulation 400
[00:30:07.175] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.35426184655197984
[00:30:49.866] Server  ERROR   Backend returned 400
[00:30:49.899] Server  ERROR   Backend returned 400
[00:30:49.922] Server  ERROR   Backend returned 400
[00:30:50.027] Browser WARN    Inference API failed, using simulation 400
[00:30:50.027] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1524521558467225
[00:30:50.027] Browser WARN    Inference API failed, using simulation 400
[00:30:50.027] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8942831440550072
[00:30:50.027] Browser WARN    Inference API failed, using simulation 400
[00:30:50.027] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.49055331614941594
[00:30:50.082] Server  ERROR   Backend returned 400
[00:30:50.110] Server  ERROR   Backend returned 400
[00:30:50.127] Server  ERROR   Backend returned 400
[00:30:50.231] Browser WARN    Inference API failed, using simulation 400
[00:30:50.231] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.36497943246818865
[00:30:50.231] Browser WARN    Inference API failed, using simulation 400
[00:30:50.231] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.19582856814446137
[00:30:50.231] Browser WARN    Inference API failed, using simulation 400
[00:30:50.231] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3724559186720378
[00:30:50.579] Server  ERROR   Backend returned 400
[00:30:50.601] Server  ERROR   Backend returned 400
[00:30:50.617] Server  ERROR   Backend returned 400
[00:30:50.720] Browser WARN    Inference API failed, using simulation 400
[00:30:50.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.33444917170852284
[00:30:50.720] Browser WARN    Inference API failed, using simulation 400
[00:30:50.720] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1996283198843929
[00:30:50.720] Browser WARN    Inference API failed, using simulation 400
[00:30:50.720] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4233550502797529
[00:30:51.089] Server  ERROR   Backend returned 400
[00:30:51.112] Server  ERROR   Backend returned 400
[00:30:51.125] Server  ERROR   Backend returned 400
[00:30:51.241] Browser WARN    Inference API failed, using simulation 400
[00:30:51.241] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7704574200017597
[00:30:51.241] Browser WARN    Inference API failed, using simulation 400
[00:30:51.241] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.24058682270067033
[00:30:51.241] Browser WARN    Inference API failed, using simulation 400
[00:30:51.241] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.47060503037754026
[00:30:51.576] Server  ERROR   Backend returned 400
[00:30:51.610] Server  ERROR   Backend returned 400
[00:30:51.615] Server  ERROR   Backend returned 400
[00:30:51.721] Browser WARN    Inference API failed, using simulation 400
[00:30:51.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1067915754124249
[00:30:51.722] Browser WARN    Inference API failed, using simulation 400
[00:30:51.722] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.20926922013847676
[00:30:51.722] Browser WARN    Inference API failed, using simulation 400
[00:30:51.722] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1696071725589987
[00:30:52.077] Server  ERROR   Backend returned 400
[00:30:52.102] Server  ERROR   Backend returned 400
[00:30:52.113] Server  ERROR   Backend returned 400
[00:30:52.221] Browser WARN    Inference API failed, using simulation 400
[00:30:52.221] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.11645505256541278
[00:30:52.221] Browser WARN    Inference API failed, using simulation 400
[00:30:52.221] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.24767575821276244
[00:30:52.221] Browser WARN    Inference API failed, using simulation 400
[00:30:52.221] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3123717656415551
[00:30:52.578] Server  ERROR   Backend returned 400
[00:30:52.609] Server  ERROR   Backend returned 400
[00:30:52.612] Server  ERROR   Backend returned 400
[00:30:52.720] Browser WARN    Inference API failed, using simulation 400
[00:30:52.720] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7695170916178152
[00:30:52.720] Browser WARN    Inference API failed, using simulation 400
[00:30:52.720] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8564481027370979
[00:30:52.720] Browser WARN    Inference API failed, using simulation 400
[00:30:52.720] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.09599067568999736
[00:30:53.087] Server  ERROR   Backend returned 400
[00:30:53.106] Server  ERROR   Backend returned 400
[00:30:53.119] Server  ERROR   Backend returned 400
[00:30:53.231] Browser WARN    Inference API failed, using simulation 400
[00:30:53.231] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7837630071119929
[00:30:53.231] Browser WARN    Inference API failed, using simulation 400
[00:30:53.231] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.24617826423108158
[00:30:53.231] Browser WARN    Inference API failed, using simulation 400
[00:30:53.231] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.049743250103627346
[00:30:53.586] Server  ERROR   Backend returned 400
[00:30:53.608] Server  ERROR   Backend returned 400
[00:30:53.621] Server  ERROR   Backend returned 400
[00:30:53.731] Browser WARN    Inference API failed, using simulation 400
[00:30:53.731] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8410300356425315
[00:30:53.731] Browser WARN    Inference API failed, using simulation 400
[00:30:53.731] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1245325025371597
[00:30:53.731] Browser WARN    Inference API failed, using simulation 400
[00:30:53.731] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.35849883139797895
[00:30:54.087] Server  ERROR   Backend returned 400
[00:30:54.104] Server  ERROR   Backend returned 400
[00:30:54.116] Server  ERROR   Backend returned 400
[00:30:54.221] Browser WARN    Inference API failed, using simulation 400
[00:30:54.221] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9182351570630553
[00:30:54.221] Browser WARN    Inference API failed, using simulation 400
[00:30:54.221] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.138641071021462
[00:30:54.221] Browser WARN    Inference API failed, using simulation 400
[00:30:54.221] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1707034193993111
[00:30:54.585] Server  ERROR   Backend returned 400
[00:30:54.600] Server  ERROR   Backend returned 400
[00:30:54.616] Server  ERROR   Backend returned 400
[00:30:54.720] Browser WARN    Inference API failed, using simulation 400
[00:30:54.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4446742419177632
[00:30:54.720] Browser WARN    Inference API failed, using simulation 400
[00:30:54.720] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.025591547148330007
[00:30:54.720] Browser WARN    Inference API failed, using simulation 400
[00:30:54.720] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8815530477299314
[00:30:55.083] Server  ERROR   Backend returned 400
[00:30:55.111] Server  ERROR   Backend returned 400
[00:30:55.126] Server  ERROR   Backend returned 400
[00:30:55.231] Browser WARN    Inference API failed, using simulation 400
[00:30:55.231] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.33462209667170245
[00:30:55.231] Browser WARN    Inference API failed, using simulation 400
[00:30:55.231] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.41032063575006805
[00:30:55.231] Browser WARN    Inference API failed, using simulation 400
[00:30:55.231] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7751989211492065
[00:30:55.598] Server  ERROR   Backend returned 400
[00:30:55.616] Server  ERROR   Backend returned 400
[00:30:55.630] Server  ERROR   Backend returned 400
[00:30:55.734] Browser WARN    Inference API failed, using simulation 400
[00:30:55.734] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.08690518692755739
[00:30:55.734] Browser WARN    Inference API failed, using simulation 400
[00:30:55.734] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8886062068923557
[00:30:55.734] Browser WARN    Inference API failed, using simulation 400
[00:30:55.734] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9287815957339463
[00:30:56.076] Server  ERROR   Backend returned 400
[00:30:56.103] Server  ERROR   Backend returned 400
[00:30:56.114] Server  ERROR   Backend returned 400
[00:30:56.218] Browser WARN    Inference API failed, using simulation 400
[00:30:56.218] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.16251727886018041
[00:30:56.218] Browser WARN    Inference API failed, using simulation 400
[00:30:56.218] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3520867197395991
[00:30:56.218] Browser WARN    Inference API failed, using simulation 400
[00:30:56.218] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8893194419420509
[00:30:56.579] Server  ERROR   Backend returned 400
[00:30:56.609] Server  ERROR   Backend returned 400
[00:30:56.618] Server  ERROR   Backend returned 400
[00:30:56.735] Browser WARN    Inference API failed, using simulation 400
[00:30:56.735] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.24349924275084034
[00:30:56.735] Browser WARN    Inference API failed, using simulation 400
[00:30:56.735] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.34796374109072487
[00:30:56.735] Browser WARN    Inference API failed, using simulation 400
[00:30:56.735] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2942380727555887
[00:30:57.084] Server  ERROR   Backend returned 400
[00:30:57.101] Server  ERROR   Backend returned 400
[00:30:57.112] Server  ERROR   Backend returned 400
[00:30:57.216] Browser WARN    Inference API failed, using simulation 400
[00:30:57.216] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8092289174667587
[00:30:57.216] Browser WARN    Inference API failed, using simulation 400
[00:30:57.216] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.030095050649561417
[00:30:57.216] Browser WARN    Inference API failed, using simulation 400
[00:30:57.216] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.958952336824973
[00:30:57.584] Server  ERROR   Backend returned 400
[00:30:57.599] Server  ERROR   Backend returned 400
[00:30:57.610] Server  ERROR   Backend returned 400
[00:30:57.715] Browser WARN    Inference API failed, using simulation 400
[00:30:57.715] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7365247277802673
[00:30:57.715] Browser WARN    Inference API failed, using simulation 400
[00:30:57.715] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2849193352667238
[00:30:57.715] Browser WARN    Inference API failed, using simulation 400
[00:30:57.715] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.863950253699481
[00:30:58.079] Server  ERROR   Backend returned 400
[00:30:58.100] Server  ERROR   Backend returned 400
[00:30:58.113] Server  ERROR   Backend returned 400
[00:30:58.216] Browser WARN    Inference API failed, using simulation 400
[00:30:58.216] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.004623137163887625
[00:30:58.216] Browser WARN    Inference API failed, using simulation 400
[00:30:58.216] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.00994065509763703
[00:30:58.216] Browser WARN    Inference API failed, using simulation 400
[00:30:58.216] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9977318650387879
[00:30:58.580] Server  ERROR   Backend returned 400
[00:30:58.612] Server  ERROR   Backend returned 400
[00:30:58.617] Server  ERROR   Backend returned 400
[00:30:58.725] Browser WARN    Inference API failed, using simulation 400
[00:30:58.725] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.06261179653798576
[00:30:58.725] Browser WARN    Inference API failed, using simulation 400
[00:30:58.725] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7315179380441154
[00:30:58.725] Browser WARN    Inference API failed, using simulation 400
[00:30:58.725] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4368973124633
[00:30:59.085] Server  ERROR   Backend returned 400
[00:30:59.102] Server  ERROR   Backend returned 400
[00:30:59.118] Server  ERROR   Backend returned 400
[00:30:59.223] Browser WARN    Inference API failed, using simulation 400
[00:30:59.223] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.0235140298475639
[00:30:59.223] Browser WARN    Inference API failed, using simulation 400
[00:30:59.223] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.37640849991691283
[00:30:59.223] Browser WARN    Inference API failed, using simulation 400
[00:30:59.223] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.07807867246865507
[00:30:59.582] Server  ERROR   Backend returned 400
[00:30:59.607] Server  ERROR   Backend returned 400
[00:30:59.611] Server  ERROR   Backend returned 400
[00:30:59.721] Browser WARN    Inference API failed, using simulation 400
[00:30:59.721] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.763578880203296
[00:30:59.721] Browser WARN    Inference API failed, using simulation 400
[00:30:59.721] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8025045725563988
[00:30:59.721] Browser WARN    Inference API failed, using simulation 400
[00:30:59.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.202520696679045
[00:31:00.082] Server  ERROR   Backend returned 400
[00:31:00.107] Server  ERROR   Backend returned 400
[00:31:00.119] Server  ERROR   Backend returned 400
[00:31:00.224] Browser WARN    Inference API failed, using simulation 400
[00:31:00.224] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9005321496965513
[00:31:00.224] Browser WARN    Inference API failed, using simulation 400
[00:31:00.224] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.0676162947198603
[00:31:00.224] Browser WARN    Inference API failed, using simulation 400
[00:31:00.224] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8956807029987099
[00:31:00.581] Server  ERROR   Backend returned 400
[00:31:00.606] Server  ERROR   Backend returned 400
[00:31:00.610] Server  ERROR   Backend returned 400
[00:31:00.719] Browser WARN    Inference API failed, using simulation 400
[00:31:00.719] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3129978380668157
[00:31:00.719] Browser WARN    Inference API failed, using simulation 400
[00:31:00.719] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9361525117337096
[00:31:00.719] Browser WARN    Inference API failed, using simulation 400
[00:31:00.719] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7609160232957745
[00:31:01.079] Server  ERROR   Backend returned 400
[00:31:01.099] Server  ERROR   Backend returned 400
[00:31:01.112] Server  ERROR   Backend returned 400
[00:31:01.216] Browser WARN    Inference API failed, using simulation 400
[00:31:01.216] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3722289246938938
[00:31:01.216] Browser WARN    Inference API failed, using simulation 400
[00:31:01.216] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4586601374973303
[00:31:01.216] Browser WARN    Inference API failed, using simulation 400
[00:31:01.216] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.018195638460213437
[00:31:01.586] Server  ERROR   Backend returned 400
[00:31:01.604] Server  ERROR   Backend returned 400
[00:31:01.616] Server  ERROR   Backend returned 400
[00:31:01.720] Browser WARN    Inference API failed, using simulation 400
[00:31:01.720] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8502348710067793
[00:31:01.720] Browser WARN    Inference API failed, using simulation 400
[00:31:01.720] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9312332083804818
[00:31:01.720] Browser WARN    Inference API failed, using simulation 400
[00:31:01.720] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.0284764177167533
[00:31:02.078] Server  ERROR   Backend returned 400
[00:31:02.097] Server  ERROR   Backend returned 400
[00:31:02.110] Server  ERROR   Backend returned 400
[00:31:02.216] Browser WARN    Inference API failed, using simulation 400
[00:31:02.216] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.35044642924751973
[00:31:02.216] Browser WARN    Inference API failed, using simulation 400
[00:31:02.216] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3320100881234784
[00:31:02.216] Browser WARN    Inference API failed, using simulation 400
[00:31:02.216] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.146885303960047
[00:31:02.594] Server  ERROR   Backend returned 400
[00:31:02.617] Server  ERROR   Backend returned 400
[00:31:02.633] Server  ERROR   Backend returned 400
[00:31:02.737] Browser WARN    Inference API failed, using simulation 400
[00:31:02.737] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9840930717166487
[00:31:02.737] Browser WARN    Inference API failed, using simulation 400
[00:31:02.737] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.47152730546540345
[00:31:02.737] Browser WARN    Inference API failed, using simulation 400
[00:31:02.738] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.32541489191078327
[00:31:03.083] Server  ERROR   Backend returned 400
[00:31:03.100] Server  ERROR   Backend returned 400
[00:31:03.113] Server  ERROR   Backend returned 400
[00:31:03.217] Browser WARN    Inference API failed, using simulation 400
[00:31:03.217] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7624114648154663
[00:31:03.217] Browser WARN    Inference API failed, using simulation 400
[00:31:03.217] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.403490666171769
[00:31:03.217] Browser WARN    Inference API failed, using simulation 400
[00:31:03.217] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9618529350782974
[00:31:03.578] Server  ERROR   Backend returned 400
[00:31:03.599] Server  ERROR   Backend returned 400
[00:31:03.611] Server  ERROR   Backend returned 400
[00:31:03.715] Browser WARN    Inference API failed, using simulation 400
[00:31:03.715] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.08414816076499326
[00:31:03.715] Browser WARN    Inference API failed, using simulation 400
[00:31:03.715] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.11194801650903036
[00:31:03.715] Browser WARN    Inference API failed, using simulation 400
[00:31:03.715] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7244208568846437
[00:31:04.077] Server  ERROR   Backend returned 400
[00:31:04.106] Server  ERROR   Backend returned 400
[00:31:04.118] Server  ERROR   Backend returned 400
[00:31:04.223] Browser WARN    Inference API failed, using simulation 400
[00:31:04.223] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8571905797258034
[00:31:04.223] Browser WARN    Inference API failed, using simulation 400
[00:31:04.223] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9934541712386372
[00:31:04.223] Browser WARN    Inference API failed, using simulation 400
[00:31:04.223] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7703081366044112
[00:31:04.583] Server  ERROR   Backend returned 400
[00:31:04.598] Server  ERROR   Backend returned 400
[00:31:04.608] Server  ERROR   Backend returned 400
[00:31:04.713] Browser WARN    Inference API failed, using simulation 400
[00:31:04.713] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.25183915767036386
[00:31:04.713] Browser WARN    Inference API failed, using simulation 400
[00:31:04.713] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8122316201025287
[00:31:04.713] Browser WARN    Inference API failed, using simulation 400
[00:31:04.713] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7172781293840772
[00:31:05.077] Server  ERROR   Backend returned 400
[00:31:05.100] Server  ERROR   Backend returned 400
[00:31:05.112] Server  ERROR   Backend returned 400
[00:31:05.215] Browser WARN    Inference API failed, using simulation 400
[00:31:05.216] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7667655458790898
[00:31:05.216] Browser WARN    Inference API failed, using simulation 400
[00:31:05.216] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2569215048232064
[00:31:05.216] Browser WARN    Inference API failed, using simulation 400
[00:31:05.216] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.964685939527304
[00:31:05.589] Server  ERROR   Backend returned 400
[00:31:05.612] Server  ERROR   Backend returned 400
[00:31:05.627] Server  ERROR   Backend returned 400
[00:31:05.730] Browser WARN    Inference API failed, using simulation 400
[00:31:05.731] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8488278180597026
[00:31:05.731] Browser WARN    Inference API failed, using simulation 400
[00:31:05.731] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1287104275179864
[00:31:05.731] Browser WARN    Inference API failed, using simulation 400
[00:31:05.731] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2663264649955426
[00:31:06.085] Server  ERROR   Backend returned 400
[00:31:06.105] Server  ERROR   Backend returned 400
[00:31:06.118] Server  ERROR   Backend returned 400
[00:31:06.221] Browser WARN    Inference API failed, using simulation 400
[00:31:06.221] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.28241535983103006
[00:31:06.221] Browser WARN    Inference API failed, using simulation 400
[00:31:06.221] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2641257951109684
[00:31:06.221] Browser WARN    Inference API failed, using simulation 400
[00:31:06.221] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.049797694978894524
[00:31:06.576] Server  ERROR   Backend returned 400
[00:31:06.593] Server  ERROR   Backend returned 400
[00:31:06.611] Server  ERROR   Backend returned 400
[00:31:06.719] Browser WARN    Inference API failed, using simulation 400
[00:31:06.719] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.342271909726823
[00:31:06.719] Browser WARN    Inference API failed, using simulation 400
[00:31:06.719] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1456653804533658
[00:31:06.719] Browser WARN    Inference API failed, using simulation 400
[00:31:06.719] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.24616023183661817
[00:31:07.078] Server  ERROR   Backend returned 400
[00:31:07.094] Server  ERROR   Backend returned 400
[00:31:07.111] Server  ERROR   Backend returned 400
[00:31:07.217] Browser WARN    Inference API failed, using simulation 400
[00:31:07.217] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1845773981868447
[00:31:07.217] Browser WARN    Inference API failed, using simulation 400
[00:31:07.217] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9443104918762826
[00:31:07.217] Browser WARN    Inference API failed, using simulation 400
[00:31:07.217] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.002326391968532182
[00:31:07.579] Server  ERROR   Backend returned 400
[00:31:07.605] Server  ERROR   Backend returned 400
[00:31:07.616] Server  ERROR   Backend returned 400
[00:31:07.720] Browser WARN    Inference API failed, using simulation 400
[00:31:07.720] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3656871718526571
[00:31:07.720] Browser WARN    Inference API failed, using simulation 400
[00:31:07.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.20141226679211904
[00:31:07.720] Browser WARN    Inference API failed, using simulation 400
[00:31:07.720] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7544151553644532
[00:31:08.078] Server  ERROR   Backend returned 400
[00:31:08.111] Server  ERROR   Backend returned 400
[00:31:08.121] Server  ERROR   Backend returned 400
[00:31:08.228] Browser WARN    Inference API failed, using simulation 400
[00:31:08.228] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3945906114919458
[00:31:08.228] Browser WARN    Inference API failed, using simulation 400
[00:31:08.228] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4550889631231198
[00:31:08.228] Browser WARN    Inference API failed, using simulation 400
[00:31:08.228] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9793943858675667
[00:31:08.577] Server  ERROR   Backend returned 400
[00:31:08.604] Server  ERROR   Backend returned 400
[00:31:08.615] Server  ERROR   Backend returned 400
[00:31:08.720] Browser WARN    Inference API failed, using simulation 400
[00:31:08.720] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9364905009412245
[00:31:08.720] Browser WARN    Inference API failed, using simulation 400
[00:31:08.720] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.04371696586152035
[00:31:08.720] Browser WARN    Inference API failed, using simulation 400
[00:31:08.720] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.18654979891955698
[00:31:09.083] Server  ERROR   Backend returned 400
[00:31:09.100] Server  ERROR   Backend returned 400
[00:31:09.114] Server  ERROR   Backend returned 400
[00:31:09.218] Browser WARN    Inference API failed, using simulation 400
[00:31:09.218] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.941224583047521
[00:31:09.218] Browser WARN    Inference API failed, using simulation 400
[00:31:09.218] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7389535763991255
[00:31:09.218] Browser WARN    Inference API failed, using simulation 400
[00:31:09.218] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7179914600778741
[00:31:09.578] Server  ERROR   Backend returned 400
[00:31:09.602] Server  ERROR   Backend returned 400
[00:31:09.614] Server  ERROR   Backend returned 400
[00:31:09.718] Browser WARN    Inference API failed, using simulation 400
[00:31:09.718] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.21369982180527486
[00:31:09.718] Browser WARN    Inference API failed, using simulation 400
[00:31:09.718] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.0510847596094392
[00:31:09.718] Browser WARN    Inference API failed, using simulation 400
[00:31:09.718] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.014026348650348852
[00:31:10.079] Server  ERROR   Backend returned 400
[00:31:10.103] Server  ERROR   Backend returned 400
[00:31:10.116] Server  ERROR   Backend returned 400
[00:31:10.232] Browser WARN    Inference API failed, using simulation 400
[00:31:10.232] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3038153047391801
[00:31:10.232] Browser WARN    Inference API failed, using simulation 400
[00:31:10.232] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4611315348151323
[00:31:10.232] Browser WARN    Inference API failed, using simulation 400
[00:31:10.232] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2791832463880079
[00:31:10.578] Server  ERROR   Backend returned 400
[00:31:10.599] Server  ERROR   Backend returned 400
[00:31:10.618] Server  ERROR   Backend returned 400
[00:31:10.723] Browser WARN    Inference API failed, using simulation 400
[00:31:10.723] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.10689121953328101
[00:31:10.723] Browser WARN    Inference API failed, using simulation 400
[00:31:10.723] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.012761163766318695
[00:31:10.723] Browser WARN    Inference API failed, using simulation 400
[00:31:10.723] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9593873263421553
[00:31:11.080] Server  ERROR   Backend returned 400
[00:31:11.100] Server  ERROR   Backend returned 400
[00:31:11.110] Server  ERROR   Backend returned 400
[00:31:11.214] Browser WARN    Inference API failed, using simulation 400
[00:31:11.214] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.21896906044539272
[00:31:11.214] Browser WARN    Inference API failed, using simulation 400
[00:31:11.214] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2813341606688694
[00:31:11.214] Browser WARN    Inference API failed, using simulation 400
[00:31:11.214] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.20173437895726382
[00:31:11.633] Server  ERROR   Backend returned 400
[00:31:11.641] Server  ERROR   Backend returned 400
[00:31:11.644] Server  ERROR   Backend returned 400
[00:31:11.752] Browser WARN    Inference API failed, using simulation 400
[00:31:11.752] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7807076166047658
[00:31:11.752] Browser WARN    Inference API failed, using simulation 400
[00:31:11.752] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2343421238654465
[00:31:11.752] Browser WARN    Inference API failed, using simulation 400
[00:31:11.752] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3915718871140378
[00:31:12.078] Server  ERROR   Backend returned 400
[00:31:12.105] Server  ERROR   Backend returned 400
[00:31:12.117] Server  ERROR   Backend returned 400
[00:31:12.221] Browser WARN    Inference API failed, using simulation 400
[00:31:12.221] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.420368307195582
[00:31:12.221] Browser WARN    Inference API failed, using simulation 400
[00:31:12.221] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.20637825667624665
[00:31:12.221] Browser WARN    Inference API failed, using simulation 400
[00:31:12.221] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.30027389723374287
[00:31:12.584] Server  ERROR   Backend returned 400
[00:31:12.602] Server  ERROR   Backend returned 400
[00:31:12.627] Server  ERROR   Backend returned 400
[00:31:12.731] Browser WARN    Inference API failed, using simulation 400
[00:31:12.731] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4602586864446294
[00:31:12.731] Browser WARN    Inference API failed, using simulation 400
[00:31:12.731] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.953566355093562
[00:31:12.731] Browser WARN    Inference API failed, using simulation 400
[00:31:12.731] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.28835303955221625
[00:31:13.076] Server  ERROR   Backend returned 400
[00:31:13.107] Server  ERROR   Backend returned 400
[00:31:13.110] Server  ERROR   Backend returned 400
[00:31:13.219] Browser WARN    Inference API failed, using simulation 400
[00:31:13.219] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.08054189838716341
[00:31:13.219] Browser WARN    Inference API failed, using simulation 400
[00:31:13.219] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.36516600683428946
[00:31:13.219] Browser WARN    Inference API failed, using simulation 400
[00:31:13.219] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.22572744824296698
[00:31:13.584] Server  ERROR   Backend returned 400
[00:31:13.604] Server  ERROR   Backend returned 400
[00:31:13.617] Server  ERROR   Backend returned 400
[00:31:13.720] Browser WARN    Inference API failed, using simulation 400
[00:31:13.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.11703154489484657
[00:31:13.720] Browser WARN    Inference API failed, using simulation 400
[00:31:13.720] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8791716064780134
[00:31:13.720] Browser WARN    Inference API failed, using simulation 400
[00:31:13.720] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.034172107219807846
[00:31:14.086] Server  ERROR   Backend returned 400
[00:31:14.103] Server  ERROR   Backend returned 400
[00:31:14.116] Server  ERROR   Backend returned 400
[00:31:14.219] Browser WARN    Inference API failed, using simulation 400
[00:31:14.219] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3627513805806966
[00:31:14.219] Browser WARN    Inference API failed, using simulation 400
[00:31:14.220] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.02822698281160263
[00:31:14.220] Browser WARN    Inference API failed, using simulation 400
[00:31:14.220] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9907076492981576
[00:31:14.578] Server  ERROR   Backend returned 400
[00:31:14.602] Server  ERROR   Backend returned 400
[00:31:14.619] Server  ERROR   Backend returned 400
[00:31:14.725] Browser WARN    Inference API failed, using simulation 400
[00:31:14.725] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8581319181121138
[00:31:14.725] Browser WARN    Inference API failed, using simulation 400
[00:31:14.725] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9170302540379539
[00:31:14.725] Browser WARN    Inference API failed, using simulation 400
[00:31:14.725] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.46076667394688836
[00:31:15.078] Server  ERROR   Backend returned 400
[00:31:15.102] Server  ERROR   Backend returned 400
[00:31:15.113] Server  ERROR   Backend returned 400
[00:31:15.217] Browser WARN    Inference API failed, using simulation 400
[00:31:15.217] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7384690246533182
[00:31:15.217] Browser WARN    Inference API failed, using simulation 400
[00:31:15.217] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4759540777078328
[00:31:15.217] Browser WARN    Inference API failed, using simulation 400
[00:31:15.217] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3872518887911744
[00:31:15.585] Server  ERROR   Backend returned 400
[00:31:15.602] Server  ERROR   Backend returned 400
[00:31:15.621] Server  ERROR   Backend returned 400
[00:31:15.726] Browser WARN    Inference API failed, using simulation 400
[00:31:15.726] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.43019515180809226
[00:31:15.726] Browser WARN    Inference API failed, using simulation 400
[00:31:15.726] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.060044279288095526
[00:31:15.726] Browser WARN    Inference API failed, using simulation 400
[00:31:15.726] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2609222754245787
[00:31:16.086] Server  ERROR   Backend returned 400
[00:31:16.106] Server  ERROR   Backend returned 400
[00:31:16.127] Server  ERROR   Backend returned 400
[00:31:16.231] Browser WARN    Inference API failed, using simulation 400
[00:31:16.231] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.18637756615660755
[00:31:16.231] Browser WARN    Inference API failed, using simulation 400
[00:31:16.231] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1771577663410719
[00:31:16.231] Browser WARN    Inference API failed, using simulation 400
[00:31:16.231] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4303171024719544
[00:31:16.584] Server  ERROR   Backend returned 400
[00:31:16.603] Server  ERROR   Backend returned 400
[00:31:16.615] Server  ERROR   Backend returned 400
[00:31:16.718] Browser WARN    Inference API failed, using simulation 400
[00:31:16.718] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.17950146655004534
[00:31:16.718] Browser WARN    Inference API failed, using simulation 400
[00:31:16.718] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.02395212662358459
[00:31:16.718] Browser WARN    Inference API failed, using simulation 400
[00:31:16.718] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.42461989320665583
[00:31:17.081] Server  ERROR   Backend returned 400
[00:31:17.098] Server  ERROR   Backend returned 400
[00:31:17.112] Server  ERROR   Backend returned 400
[00:31:17.216] Browser WARN    Inference API failed, using simulation 400
[00:31:17.216] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.19736048448665833
[00:31:17.216] Browser WARN    Inference API failed, using simulation 400
[00:31:17.216] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.27212509691094616
[00:31:17.216] Browser WARN    Inference API failed, using simulation 400
[00:31:17.216] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.34714833208053125
[00:31:17.584] Server  ERROR   Backend returned 400
[00:31:17.600] Server  ERROR   Backend returned 400
[00:31:17.612] Server  ERROR   Backend returned 400
[00:31:17.716] Browser WARN    Inference API failed, using simulation 400
[00:31:17.716] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4147045730815195
[00:31:17.716] Browser WARN    Inference API failed, using simulation 400
[00:31:17.716] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.21038693799327762
[00:31:17.716] Browser WARN    Inference API failed, using simulation 400
[00:31:17.716] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7285357483588933
[00:31:18.091] Server  ERROR   Backend returned 400
[00:31:18.115] Server  ERROR   Backend returned 400
[00:31:18.122] Server  ERROR   Backend returned 400
[00:31:18.227] Browser WARN    Inference API failed, using simulation 400
[00:31:18.227] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.18782716708020486
[00:31:18.227] Browser WARN    Inference API failed, using simulation 400
[00:31:18.227] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8439681073678073
[00:31:18.227] Browser WARN    Inference API failed, using simulation 400
[00:31:18.227] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.007108255539268082
[00:31:18.583] Server  ERROR   Backend returned 400
[00:31:18.618] Server  ERROR   Backend returned 400
[00:31:18.621] Server  ERROR   Backend returned 400
[00:31:18.730] Browser WARN    Inference API failed, using simulation 400
[00:31:18.730] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7181837844301303
[00:31:18.730] Browser WARN    Inference API failed, using simulation 400
[00:31:18.730] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7719920830446575
[00:31:18.730] Browser WARN    Inference API failed, using simulation 400
[00:31:18.730] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3927578477742668
[00:31:19.089] Server  ERROR   Backend returned 400
[00:31:19.113] Server  ERROR   Backend returned 400
[00:31:19.127] Server  ERROR   Backend returned 400
[00:31:19.231] Browser WARN    Inference API failed, using simulation 400
[00:31:19.231] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9662864386088341
[00:31:19.231] Browser WARN    Inference API failed, using simulation 400
[00:31:19.231] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.981812919262627
[00:31:19.231] Browser WARN    Inference API failed, using simulation 400
[00:31:19.231] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.08080727126956055
[00:31:19.586] Server  ERROR   Backend returned 400
[00:31:19.601] Server  ERROR   Backend returned 400
[00:31:19.615] Server  ERROR   Backend returned 400
[00:31:19.720] Browser WARN    Inference API failed, using simulation 400
[00:31:19.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2678117839805246
[00:31:19.720] Browser WARN    Inference API failed, using simulation 400
[00:31:19.720] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.21146646695978716
[00:31:19.720] Browser WARN    Inference API failed, using simulation 400
[00:31:19.720] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.10812376447586397
[00:31:20.079] Server  ERROR   Backend returned 400
[00:31:20.100] Server  ERROR   Backend returned 400
[00:31:20.115] Server  ERROR   Backend returned 400
[00:31:20.219] Browser WARN    Inference API failed, using simulation 400
[00:31:20.219] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4601607185852655
[00:31:20.219] Browser WARN    Inference API failed, using simulation 400
[00:31:20.219] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.23203479068938188
[00:31:20.219] Browser WARN    Inference API failed, using simulation 400
[00:31:20.219] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9786945330928254
[00:31:20.589] Server  ERROR   Backend returned 400
[00:31:20.616] Server  ERROR   Backend returned 400
[00:31:20.619] Server  ERROR   Backend returned 400
[00:31:20.729] Browser WARN    Inference API failed, using simulation 400
[00:31:20.729] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.36941025681440304
[00:31:20.729] Browser WARN    Inference API failed, using simulation 400
[00:31:20.729] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.08496161946132996
[00:31:20.729] Browser WARN    Inference API failed, using simulation 400
[00:31:20.729] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7888277440872223
[00:31:21.082] Server  ERROR   Backend returned 400
[00:31:21.098] Server  ERROR   Backend returned 400
[00:31:21.109] Server  ERROR   Backend returned 400
[00:31:21.214] Browser WARN    Inference API failed, using simulation 400
[00:31:21.214] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3142595961206727
[00:31:21.214] Browser WARN    Inference API failed, using simulation 400
[00:31:21.214] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9285946347242551
[00:31:21.214] Browser WARN    Inference API failed, using simulation 400
[00:31:21.214] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.15333658467854328
[00:31:21.578] Server  ERROR   Backend returned 400
[00:31:21.594] Server  ERROR   Backend returned 400
[00:31:21.607] Server  ERROR   Backend returned 400
[00:31:21.722] Browser WARN    Inference API failed, using simulation 400
[00:31:21.722] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1320753203617595
[00:31:21.722] Browser WARN    Inference API failed, using simulation 400
[00:31:21.722] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8883681313692041
[00:31:21.722] Browser WARN    Inference API failed, using simulation 400
[00:31:21.722] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7321444738321028
[00:31:22.078] Server  ERROR   Backend returned 400
[00:31:22.104] Server  ERROR   Backend returned 400
[00:31:22.116] Server  ERROR   Backend returned 400
[00:31:22.220] Browser WARN    Inference API failed, using simulation 400
[00:31:22.220] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8476353259723763
[00:31:22.220] Browser WARN    Inference API failed, using simulation 400
[00:31:22.220] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.16680341321742892
[00:31:22.220] Browser WARN    Inference API failed, using simulation 400
[00:31:22.220] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7604142062371751
[00:31:22.589] Server  ERROR   Backend returned 400
[00:31:22.612] Server  ERROR   Backend returned 400
[00:31:22.628] Server  ERROR   Backend returned 400
[00:31:22.734] Browser WARN    Inference API failed, using simulation 400
[00:31:22.734] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.10411766397668282
[00:31:22.734] Browser WARN    Inference API failed, using simulation 400
[00:31:22.734] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9420362631849273
[00:31:22.734] Browser WARN    Inference API failed, using simulation 400
[00:31:22.734] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.36036759128376544
[00:31:23.079] Server  ERROR   Backend returned 400
[00:31:23.094] Server  ERROR   Backend returned 400
[00:31:23.112] Server  ERROR   Backend returned 400
[00:31:23.220] Browser WARN    Inference API failed, using simulation 400
[00:31:23.220] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8835105586508446
[00:31:23.220] Browser WARN    Inference API failed, using simulation 400
[00:31:23.220] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.010698286576617977
[00:31:23.220] Browser WARN    Inference API failed, using simulation 400
[00:31:23.220] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.39863963495012966
[00:31:23.581] Server  ERROR   Backend returned 400
[00:31:23.604] Server  ERROR   Backend returned 400
[00:31:23.617] Server  ERROR   Backend returned 400
[00:31:23.720] Browser WARN    Inference API failed, using simulation 400
[00:31:23.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.40213825774701956
[00:31:23.720] Browser WARN    Inference API failed, using simulation 400
[00:31:23.720] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.0864039350606759
[00:31:23.720] Browser WARN    Inference API failed, using simulation 400
[00:31:23.720] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.06392836167550758
[00:31:24.084] Server  ERROR   Backend returned 400
[00:31:24.099] Server  ERROR   Backend returned 400
[00:31:24.123] Server  ERROR   Backend returned 400
[00:31:24.234] Browser WARN    Inference API failed, using simulation 400
[00:31:24.234] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3148237773710843
[00:31:24.234] Browser WARN    Inference API failed, using simulation 400
[00:31:24.234] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.10566741369179666
[00:31:24.234] Browser WARN    Inference API failed, using simulation 400
[00:31:24.234] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.012434604217602674
[00:31:24.585] Server  ERROR   Backend returned 400
[00:31:24.600] Server  ERROR   Backend returned 400
[00:31:24.613] Server  ERROR   Backend returned 400
[00:31:24.724] Browser WARN    Inference API failed, using simulation 400
[00:31:24.724] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.30006643170815994
[00:31:24.724] Browser WARN    Inference API failed, using simulation 400
[00:31:24.724] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8467782490052785
[00:31:24.724] Browser WARN    Inference API failed, using simulation 400
[00:31:24.724] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.02293840307517342
[00:31:25.079] Server  ERROR   Backend returned 400
[00:31:25.112] Server  ERROR   Backend returned 400
[00:31:25.117] Server  ERROR   Backend returned 400
[00:31:25.223] Browser WARN    Inference API failed, using simulation 400
[00:31:25.223] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8425403731866783
[00:31:25.223] Browser WARN    Inference API failed, using simulation 400
[00:31:25.223] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.13304642037273673
[00:31:25.223] Browser WARN    Inference API failed, using simulation 400
[00:31:25.223] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4578263823209579
[00:31:25.580] Server  ERROR   Backend returned 400
[00:31:25.599] Server  ERROR   Backend returned 400
[00:31:25.613] Server  ERROR   Backend returned 400
[00:31:25.716] Browser WARN    Inference API failed, using simulation 400
[00:31:25.716] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.47347785476567783
[00:31:25.716] Browser WARN    Inference API failed, using simulation 400
[00:31:25.716] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1376600627751256
[00:31:25.716] Browser WARN    Inference API failed, using simulation 400
[00:31:25.716] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9766348456818945
[00:31:26.087] Server  ERROR   Backend returned 400
[00:31:26.099] Server  ERROR   Backend returned 400
[00:31:26.126] Server  ERROR   Backend returned 400
[00:31:26.230] Browser WARN    Inference API failed, using simulation 400
[00:31:26.230] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.09224695448649223
[00:31:26.230] Browser WARN    Inference API failed, using simulation 400
[00:31:26.230] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3012063020488776
[00:31:26.230] Browser WARN    Inference API failed, using simulation 400
[00:31:26.230] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9063281599985898
[00:31:26.589] Server  ERROR   Backend returned 400
[00:31:26.604] Server  ERROR   Backend returned 400
[00:31:26.616] Server  ERROR   Backend returned 400
[00:31:26.720] Browser WARN    Inference API failed, using simulation 400
[00:31:26.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.34025104469577516
[00:31:26.720] Browser WARN    Inference API failed, using simulation 400
[00:31:26.720] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8182746171874998
[00:31:26.720] Browser WARN    Inference API failed, using simulation 400
[00:31:26.720] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8471428588335279
[00:31:27.146] Server  ERROR   Backend returned 400
[00:31:27.186] Server  ERROR   Backend returned 400
[00:31:27.217] Server  ERROR   Backend returned 400
[00:31:27.323] Browser WARN    Inference API failed, using simulation 400
[00:31:27.323] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.40414913949791476
[00:31:27.323] Browser WARN    Inference API failed, using simulation 400
[00:31:27.323] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9598504845908729
[00:31:27.323] Browser WARN    Inference API failed, using simulation 400
[00:31:27.323] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9617047633569865
[00:31:27.580] Server  ERROR   Backend returned 400
[00:31:27.598] Server  ERROR   Backend returned 400
[00:31:27.612] Server  ERROR   Backend returned 400
[00:31:27.717] Browser WARN    Inference API failed, using simulation 400
[00:31:27.717] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3578457337445716
[00:31:27.717] Browser WARN    Inference API failed, using simulation 400
[00:31:27.717] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.06262799437870481
[00:31:27.717] Browser WARN    Inference API failed, using simulation 400
[00:31:27.717] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9438907600159436
[00:31:28.079] Server  ERROR   Backend returned 400
[00:31:28.099] Server  ERROR   Backend returned 400
[00:31:28.112] Server  ERROR   Backend returned 400
[00:31:28.216] Browser WARN    Inference API failed, using simulation 400
[00:31:28.216] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.079735112363921
[00:31:28.216] Browser WARN    Inference API failed, using simulation 400
[00:31:28.216] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.07598314924128113
[00:31:28.216] Browser WARN    Inference API failed, using simulation 400
[00:31:28.216] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7387776037577936
[00:31:28.592] Server  ERROR   Backend returned 400
[00:31:28.608] Server  ERROR   Backend returned 400
[00:31:28.620] Server  ERROR   Backend returned 400
[00:31:28.725] Browser WARN    Inference API failed, using simulation 400
[00:31:28.725] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9563332446549802
[00:31:28.725] Browser WARN    Inference API failed, using simulation 400
[00:31:28.725] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.28318599344781886
[00:31:28.725] Browser WARN    Inference API failed, using simulation 400
[00:31:28.725] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4586125087539936
[00:31:29.078] Server  ERROR   Backend returned 400
[00:31:29.101] Server  ERROR   Backend returned 400
[00:31:29.113] Server  ERROR   Backend returned 400
[00:31:29.217] Browser WARN    Inference API failed, using simulation 400
[00:31:29.217] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.40528158450777746
[00:31:29.217] Browser WARN    Inference API failed, using simulation 400
[00:31:29.217] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4539037335027389
[00:31:29.217] Browser WARN    Inference API failed, using simulation 400
[00:31:29.217] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.40982066323133054
[00:31:29.576] Server  ERROR   Backend returned 400
[00:31:29.600] Server  ERROR   Backend returned 400
[00:31:29.613] Server  ERROR   Backend returned 400
[00:31:29.718] Browser WARN    Inference API failed, using simulation 400
[00:31:29.718] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.09612464455902692
[00:31:29.718] Browser WARN    Inference API failed, using simulation 400
[00:31:29.718] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3393742347309223
[00:31:29.718] Browser WARN    Inference API failed, using simulation 400
[00:31:29.718] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.07858441307240605
[00:31:30.081] Server  ERROR   Backend returned 400
[00:31:30.097] Server  ERROR   Backend returned 400
[00:31:30.107] Server  ERROR   Backend returned 400
[00:31:30.227] Browser WARN    Inference API failed, using simulation 400
[00:31:30.227] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.16511134974507352
[00:31:30.227] Browser WARN    Inference API failed, using simulation 400
[00:31:30.227] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.039183203635322905
[00:31:30.227] Browser WARN    Inference API failed, using simulation 400
[00:31:30.227] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2839863926996437
[00:31:30.583] Server  ERROR   Backend returned 400
[00:31:30.598] Server  ERROR   Backend returned 400
[00:31:30.610] Server  ERROR   Backend returned 400
[00:31:30.713] Browser WARN    Inference API failed, using simulation 400
[00:31:30.713] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.18167745075527708
[00:31:30.713] Browser WARN    Inference API failed, using simulation 400
[00:31:30.713] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.05878087558644113
[00:31:30.713] Browser WARN    Inference API failed, using simulation 400
[00:31:30.713] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9138892602140388
[00:31:31.083] Server  ERROR   Backend returned 400
[00:31:31.114] Server  ERROR   Backend returned 400
[00:31:31.120] Server  ERROR   Backend returned 400
[00:31:31.226] Browser WARN    Inference API failed, using simulation 400
[00:31:31.226] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7962280947225749
[00:31:31.226] Browser WARN    Inference API failed, using simulation 400
[00:31:31.226] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1155366644897004
[00:31:31.226] Browser WARN    Inference API failed, using simulation 400
[00:31:31.226] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3347286553470919
[00:31:31.581] Server  ERROR   Backend returned 400
[00:31:31.597] Server  ERROR   Backend returned 400
[00:31:31.611] Server  ERROR   Backend returned 400
[00:31:31.719] Browser WARN    Inference API failed, using simulation 400
[00:31:31.719] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.17388608976846243
[00:31:31.719] Browser WARN    Inference API failed, using simulation 400
[00:31:31.719] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.23110285033458816
[00:31:31.719] Browser WARN    Inference API failed, using simulation 400
[00:31:31.719] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8295417533892316
[00:31:32.079] Server  ERROR   Backend returned 400
[00:31:32.096] Server  ERROR   Backend returned 400
[00:31:32.114] Server  ERROR   Backend returned 400
[00:31:32.220] Browser WARN    Inference API failed, using simulation 400
[00:31:32.220] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.279031934284197
[00:31:32.220] Browser WARN    Inference API failed, using simulation 400
[00:31:32.220] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9590754258459373
[00:31:32.220] Browser WARN    Inference API failed, using simulation 400
[00:31:32.220] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7512520455661137
[00:31:32.578] Server  ERROR   Backend returned 400
[00:31:32.610] Server  ERROR   Backend returned 400
[00:31:32.612] Server  ERROR   Backend returned 400
[00:31:32.722] Browser WARN    Inference API failed, using simulation 400
[00:31:32.722] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.17704928452721536
[00:31:32.722] Browser WARN    Inference API failed, using simulation 400
[00:31:32.722] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9515928378720813
[00:31:32.722] Browser WARN    Inference API failed, using simulation 400
[00:31:32.722] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.09203583988267305
[00:31:33.079] Server  ERROR   Backend returned 400
[00:31:33.099] Server  ERROR   Backend returned 400
[00:31:33.112] Server  ERROR   Backend returned 400
[00:31:33.215] Browser WARN    Inference API failed, using simulation 400
[00:31:33.216] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.13135257971601283
[00:31:33.216] Browser WARN    Inference API failed, using simulation 400
[00:31:33.216] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3787870864319593
[00:31:33.216] Browser WARN    Inference API failed, using simulation 400
[00:31:33.216] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.21027799850260231
[00:31:33.577] Server  ERROR   Backend returned 400
[00:31:33.600] Server  ERROR   Backend returned 400
[00:31:33.614] Server  ERROR   Backend returned 400
[00:31:33.718] Browser WARN    Inference API failed, using simulation 400
[00:31:33.718] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.736773492039527
[00:31:33.718] Browser WARN    Inference API failed, using simulation 400
[00:31:33.718] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.049255052365826724
[00:31:33.718] Browser WARN    Inference API failed, using simulation 400
[00:31:33.718] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1286361636317912
[00:31:34.077] Server  ERROR   Backend returned 400
[00:31:34.095] Server  ERROR   Backend returned 400
[00:31:34.107] Server  ERROR   Backend returned 400
[00:31:34.219] Browser WARN    Inference API failed, using simulation 400
[00:31:34.219] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9648216727668668
[00:31:34.219] Browser WARN    Inference API failed, using simulation 400
[00:31:34.219] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4660314422315261
[00:31:34.219] Browser WARN    Inference API failed, using simulation 400
[00:31:34.219] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.23613634398472555
[00:31:34.583] Server  ERROR   Backend returned 400
[00:31:34.599] Server  ERROR   Backend returned 400
[00:31:34.612] Server  ERROR   Backend returned 400
[00:31:34.715] Browser WARN    Inference API failed, using simulation 400
[00:31:34.716] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.49919936204282767
[00:31:34.716] Browser WARN    Inference API failed, using simulation 400
[00:31:34.716] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7515536522357013
[00:31:34.716] Browser WARN    Inference API failed, using simulation 400
[00:31:34.716] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.26754924219602444
[00:31:35.081] Server  ERROR   Backend returned 400
[00:31:35.096] Server  ERROR   Backend returned 400
[00:31:35.107] Server  ERROR   Backend returned 400
[00:31:35.214] Browser WARN    Inference API failed, using simulation 400
[00:31:35.214] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.08045493041833546
[00:31:35.214] Browser WARN    Inference API failed, using simulation 400
[00:31:35.214] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3135845013890859
[00:31:35.214] Browser WARN    Inference API failed, using simulation 400
[00:31:35.214] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.0020834389180533486
[00:31:35.580] Server  ERROR   Backend returned 400
[00:31:35.591] Server  ERROR   Backend returned 400
[00:31:35.610] Server  ERROR   Backend returned 400
[00:31:35.729] Browser WARN    Inference API failed, using simulation 400
[00:31:35.729] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.07095476254984301
[00:31:35.729] Browser WARN    Inference API failed, using simulation 400
[00:31:35.729] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8649280156877738
[00:31:35.729] Browser WARN    Inference API failed, using simulation 400
[00:31:35.729] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.943989442353282
[00:31:36.079] Server  ERROR   Backend returned 400
[00:31:36.093] Server  ERROR   Backend returned 400
[00:31:36.105] Server  ERROR   Backend returned 400
[00:31:36.210] Browser WARN    Inference API failed, using simulation 400
[00:31:36.210] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4195871997986827
[00:31:36.210] Browser WARN    Inference API failed, using simulation 400
[00:31:36.210] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.45510032556765556
[00:31:36.210] Browser WARN    Inference API failed, using simulation 400
[00:31:36.210] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.0038871866775136987
[00:31:36.577] Server  ERROR   Backend returned 400
[00:31:36.596] Server  ERROR   Backend returned 400
[00:31:36.610] Server  ERROR   Backend returned 400
[00:31:36.728] Browser WARN    Inference API failed, using simulation 400
[00:31:36.728] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.19354551604425402
[00:31:36.728] Browser WARN    Inference API failed, using simulation 400
[00:31:36.728] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.04887622778817041
[00:31:36.728] Browser WARN    Inference API failed, using simulation 400
[00:31:36.728] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4844203106488562
[00:31:37.084] Server  ERROR   Backend returned 400
[00:31:37.099] Server  ERROR   Backend returned 400
[00:31:37.135] Server  ERROR   Backend returned 400
[00:31:37.239] Browser WARN    Inference API failed, using simulation 400
[00:31:37.239] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8471165003875153
[00:31:37.239] Browser WARN    Inference API failed, using simulation 400
[00:31:37.239] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.12814582127898588
[00:31:37.239] Browser WARN    Inference API failed, using simulation 400
[00:31:37.239] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.14439376806899162
[00:31:37.581] Server  ERROR   Backend returned 400
[00:31:37.611] Server  ERROR   Backend returned 400
[00:31:37.631] Server  ERROR   Backend returned 400
[00:31:37.734] Browser WARN    Inference API failed, using simulation 400
[00:31:37.734] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7661743821958518
[00:31:37.735] Browser WARN    Inference API failed, using simulation 400
[00:31:37.735] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.06707498406494755
[00:31:37.735] Browser WARN    Inference API failed, using simulation 400
[00:31:37.735] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.02263131090132864
[00:31:38.083] Server  ERROR   Backend returned 400
[00:31:38.110] Server  ERROR   Backend returned 400
[00:31:38.122] Server  ERROR   Backend returned 400
[00:31:38.226] Browser WARN    Inference API failed, using simulation 400
[00:31:38.227] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9825718521199257
[00:31:38.227] Browser WARN    Inference API failed, using simulation 400
[00:31:38.227] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7561459308732017
[00:31:38.227] Browser WARN    Inference API failed, using simulation 400
[00:31:38.227] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.0665756947890993
[00:31:38.588] Server  ERROR   Backend returned 400
[00:31:38.603] Server  ERROR   Backend returned 400
[00:31:38.620] Server  ERROR   Backend returned 400
[00:31:38.724] Browser WARN    Inference API failed, using simulation 400
[00:31:38.724] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.77486507159045
[00:31:38.724] Browser WARN    Inference API failed, using simulation 400
[00:31:38.724] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2770258179526233
[00:31:38.724] Browser WARN    Inference API failed, using simulation 400
[00:31:38.724] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.056958831269409926
[00:31:39.079] Server  ERROR   Backend returned 400
[00:31:39.103] Server  ERROR   Backend returned 400
[00:31:39.109] Server  ERROR   Backend returned 400
[00:31:39.215] Browser WARN    Inference API failed, using simulation 400
[00:31:39.215] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.10985267235071317
[00:31:39.215] Browser WARN    Inference API failed, using simulation 400
[00:31:39.215] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2228304795848351
[00:31:39.215] Browser WARN    Inference API failed, using simulation 400
[00:31:39.215] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3114715043203373
[00:31:39.579] Server  ERROR   Backend returned 400
[00:31:39.595] Server  ERROR   Backend returned 400
[00:31:39.608] Server  ERROR   Backend returned 400
[00:31:39.712] Browser WARN    Inference API failed, using simulation 400
[00:31:39.712] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.01581109333912384
[00:31:39.712] Browser WARN    Inference API failed, using simulation 400
[00:31:39.712] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.13543386215900632
[00:31:39.712] Browser WARN    Inference API failed, using simulation 400
[00:31:39.712] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8644006689959408
[00:31:40.088] Server  ERROR   Backend returned 400
[00:31:40.107] Server  ERROR   Backend returned 400
[00:31:40.124] Server  ERROR   Backend returned 400
[00:31:40.227] Browser WARN    Inference API failed, using simulation 400
[00:31:40.227] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.051231406093838194
[00:31:40.227] Browser WARN    Inference API failed, using simulation 400
[00:31:40.227] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.32278918530916906
[00:31:40.227] Browser WARN    Inference API failed, using simulation 400
[00:31:40.227] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9261464537022465
[00:31:40.579] Server  ERROR   Backend returned 400
[00:31:40.606] Server  ERROR   Backend returned 400
[00:31:40.618] Server  ERROR   Backend returned 400
[00:31:40.722] Browser WARN    Inference API failed, using simulation 400
[00:31:40.722] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4031901835820585
[00:31:40.722] Browser WARN    Inference API failed, using simulation 400
[00:31:40.722] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.325615974644433
[00:31:40.722] Browser WARN    Inference API failed, using simulation 400
[00:31:40.722] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7202069391190339
[00:31:41.075] Server  ERROR   Backend returned 400
[00:31:41.093] Server  ERROR   Backend returned 400
[00:31:41.111] Server  ERROR   Backend returned 400
[00:31:41.214] Browser WARN    Inference API failed, using simulation 400
[00:31:41.215] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8671205915600292
[00:31:41.215] Browser WARN    Inference API failed, using simulation 400
[00:31:41.215] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.926997301566938
[00:31:41.215] Browser WARN    Inference API failed, using simulation 400
[00:31:41.215] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.15025199030392472
[00:31:41.581] Server  ERROR   Backend returned 400
[00:31:41.603] Server  ERROR   Backend returned 400
[00:31:41.615] Server  ERROR   Backend returned 400
[00:31:41.719] Browser WARN    Inference API failed, using simulation 400
[00:31:41.719] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7358442115741841
[00:31:41.719] Browser WARN    Inference API failed, using simulation 400
[00:31:41.719] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1614561221201431
[00:31:41.719] Browser WARN    Inference API failed, using simulation 400
[00:31:41.719] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.26155195430001105
[00:31:42.076] Server  ERROR   Backend returned 400
[00:31:42.099] Server  ERROR   Backend returned 400
[00:31:42.112] Server  ERROR   Backend returned 400
[00:31:42.216] Browser WARN    Inference API failed, using simulation 400
[00:31:42.216] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.37111742803303494
[00:31:42.216] Browser WARN    Inference API failed, using simulation 400
[00:31:42.216] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2798112166853156
[00:31:42.216] Browser WARN    Inference API failed, using simulation 400
[00:31:42.216] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9689132522134436
[00:31:42.578] Server  ERROR   Backend returned 400
[00:31:42.592] Server  ERROR   Backend returned 400
[00:31:42.604] Server  ERROR   Backend returned 400
[00:31:42.707] Browser WARN    Inference API failed, using simulation 400
[00:31:42.707] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8139853025564369
[00:31:42.707] Browser WARN    Inference API failed, using simulation 400
[00:31:42.707] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7114850887345274
[00:31:42.707] Browser WARN    Inference API failed, using simulation 400
[00:31:42.707] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.479826071755537
[00:31:43.079] Server  ERROR   Backend returned 400
[00:31:43.101] Server  ERROR   Backend returned 400
[00:31:43.121] Server  ERROR   Backend returned 400
[00:31:43.226] Browser WARN    Inference API failed, using simulation 400
[00:31:43.226] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.29751689716858976
[00:31:43.226] Browser WARN    Inference API failed, using simulation 400
[00:31:43.226] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.49363749870816104
[00:31:43.226] Browser WARN    Inference API failed, using simulation 400
[00:31:43.226] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.03945811461633397
[00:31:43.577] Server  ERROR   Backend returned 400
[00:31:43.598] Server  ERROR   Backend returned 400
[00:31:43.611] Server  ERROR   Backend returned 400
[00:31:43.715] Browser WARN    Inference API failed, using simulation 400
[00:31:43.715] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8050060228140146
[00:31:43.715] Browser WARN    Inference API failed, using simulation 400
[00:31:43.715] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.27952467319313157
[00:31:43.715] Browser WARN    Inference API failed, using simulation 400
[00:31:43.715] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4622279506239193
[00:31:44.079] Server  ERROR   Backend returned 400
[00:31:44.096] Server  ERROR   Backend returned 400
[00:31:44.115] Server  ERROR   Backend returned 400
[00:31:44.222] Browser WARN    Inference API failed, using simulation 400
[00:31:44.222] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3149416833466334
[00:31:44.222] Browser WARN    Inference API failed, using simulation 400
[00:31:44.222] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3390238279268906
[00:31:44.222] Browser WARN    Inference API failed, using simulation 400
[00:31:44.222] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8162504823951815
[00:31:44.580] Server  ERROR   Backend returned 400
[00:31:44.595] Server  ERROR   Backend returned 400
[00:31:44.609] Server  ERROR   Backend returned 400
[00:31:44.714] Browser WARN    Inference API failed, using simulation 400
[00:31:44.714] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.498746916942283
[00:31:44.714] Browser WARN    Inference API failed, using simulation 400
[00:31:44.714] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.073274943105613
[00:31:44.714] Browser WARN    Inference API failed, using simulation 400
[00:31:44.714] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2504297843471459
[00:31:45.088] Server  ERROR   Backend returned 400
[00:31:45.104] Server  ERROR   Backend returned 400
[00:31:45.113] Server  ERROR   Backend returned 400
[00:31:45.222] Browser WARN    Inference API failed, using simulation 400
[00:31:45.222] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1773759309736614
[00:31:45.222] Browser WARN    Inference API failed, using simulation 400
[00:31:45.222] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.37502329610233814
[00:31:45.222] Browser WARN    Inference API failed, using simulation 400
[00:31:45.222] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.25881497173928225
[00:31:45.584] Server  ERROR   Backend returned 400
[00:31:45.599] Server  ERROR   Backend returned 400
[00:31:45.610] Server  ERROR   Backend returned 400
[00:31:45.714] Browser WARN    Inference API failed, using simulation 400
[00:31:45.714] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7379652894056606
[00:31:45.714] Browser WARN    Inference API failed, using simulation 400
[00:31:45.714] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4868182988473462
[00:31:45.714] Browser WARN    Inference API failed, using simulation 400
[00:31:45.714] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2802511389512578
[00:31:46.081] Server  ERROR   Backend returned 400
[00:31:46.096] Server  ERROR   Backend returned 400
[00:31:46.107] Server  ERROR   Backend returned 400
[00:31:46.211] Browser WARN    Inference API failed, using simulation 400
[00:31:46.211] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.22608177575286115
[00:31:46.211] Browser WARN    Inference API failed, using simulation 400
[00:31:46.211] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7697589310774473
[00:31:46.211] Browser WARN    Inference API failed, using simulation 400
[00:31:46.211] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8616847337165932
[00:31:46.584] Server  ERROR   Backend returned 400
[00:31:46.598] Server  ERROR   Backend returned 400
[00:31:46.609] Server  ERROR   Backend returned 400
[00:31:46.714] Browser WARN    Inference API failed, using simulation 400
[00:31:46.714] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.10835428137818698
[00:31:46.714] Browser WARN    Inference API failed, using simulation 400
[00:31:46.714] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.13911075630744413
[00:31:46.714] Browser WARN    Inference API failed, using simulation 400
[00:31:46.714] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.32738342079227933
[00:31:47.084] Server  ERROR   Backend returned 400
[00:31:47.099] Server  ERROR   Backend returned 400
[00:31:47.110] Server  ERROR   Backend returned 400
[00:31:47.215] Browser WARN    Inference API failed, using simulation 400
[00:31:47.215] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.06467570912106696
[00:31:47.215] Browser WARN    Inference API failed, using simulation 400
[00:31:47.215] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3959456037685587
[00:31:47.215] Browser WARN    Inference API failed, using simulation 400
[00:31:47.215] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.25993800235540965
[00:31:47.591] Server  ERROR   Backend returned 400
[00:31:47.612] Server  ERROR   Backend returned 400
[00:31:47.622] Server  ERROR   Backend returned 400
[00:31:47.727] Browser WARN    Inference API failed, using simulation 400
[00:31:47.727] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.23405187015029894
[00:31:47.727] Browser WARN    Inference API failed, using simulation 400
[00:31:47.727] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.10381534884478927
[00:31:47.727] Browser WARN    Inference API failed, using simulation 400
[00:31:47.727] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9515518350777347
[00:31:48.087] Server  ERROR   Backend returned 400
[00:31:48.100] Server  ERROR   Backend returned 400
[00:31:48.124] Server  ERROR   Backend returned 400
[00:31:48.229] Browser WARN    Inference API failed, using simulation 400
[00:31:48.229] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.12761152183108704
[00:31:48.229] Browser WARN    Inference API failed, using simulation 400
[00:31:48.229] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8993208963813897
[00:31:48.229] Browser WARN    Inference API failed, using simulation 400
[00:31:48.229] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7111895888829249
[00:31:48.584] Server  ERROR   Backend returned 400
[00:31:48.599] Server  ERROR   Backend returned 400
[00:31:48.616] Server  ERROR   Backend returned 400
[00:31:48.734] Browser WARN    Inference API failed, using simulation 400
[00:31:48.734] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8097173178932029
[00:31:48.734] Browser WARN    Inference API failed, using simulation 400
[00:31:48.734] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.14900373250517251
[00:31:48.734] Browser WARN    Inference API failed, using simulation 400
[00:31:48.734] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8097726225109707
[00:31:49.078] Server  ERROR   Backend returned 400
[00:31:49.109] Server  ERROR   Backend returned 400
[00:31:49.114] Server  ERROR   Backend returned 400
[00:31:49.220] Browser WARN    Inference API failed, using simulation 400
[00:31:49.220] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.39507725315628084
[00:31:49.220] Browser WARN    Inference API failed, using simulation 400
[00:31:49.220] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1309677147146155
[00:31:49.220] Browser WARN    Inference API failed, using simulation 400
[00:31:49.220] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.07952058139866419
[00:31:49.582] Server  ERROR   Backend returned 400
[00:31:49.610] Server  ERROR   Backend returned 400
[00:31:49.612] Server  ERROR   Backend returned 400
[00:31:49.721] Browser WARN    Inference API failed, using simulation 400
[00:31:49.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.38400424567597785
[00:31:49.721] Browser WARN    Inference API failed, using simulation 400
[00:31:49.721] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7583834787099646
[00:31:49.721] Browser WARN    Inference API failed, using simulation 400
[00:31:49.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.020783123327535402
[00:31:50.078] Server  ERROR   Backend returned 400
[00:31:50.104] Server  ERROR   Backend returned 400
[00:31:50.113] Server  ERROR   Backend returned 400
[00:31:50.218] Browser WARN    Inference API failed, using simulation 400
[00:31:50.218] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.23913092694815852
[00:31:50.218] Browser WARN    Inference API failed, using simulation 400
[00:31:50.218] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4706793929930475
[00:31:50.218] Browser WARN    Inference API failed, using simulation 400
[00:31:50.218] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.03991928561862301
[00:31:50.584] Server  ERROR   Backend returned 400
[00:31:50.599] Server  ERROR   Backend returned 400
[00:31:50.612] Server  ERROR   Backend returned 400
[00:31:50.728] Browser WARN    Inference API failed, using simulation 400
[00:31:50.728] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.31569514037375906
[00:31:50.728] Browser WARN    Inference API failed, using simulation 400
[00:31:50.728] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.09753176017488341
[00:31:50.728] Browser WARN    Inference API failed, using simulation 400
[00:31:50.728] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.21122494977672746
[00:31:51.087] Server  ERROR   Backend returned 400
[00:31:51.110] Server  ERROR   Backend returned 400
[00:31:51.130] Server  ERROR   Backend returned 400
[00:31:51.234] Browser WARN    Inference API failed, using simulation 400
[00:31:51.234] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7493794697445304
[00:31:51.234] Browser WARN    Inference API failed, using simulation 400
[00:31:51.234] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7182088545544615
[00:31:51.234] Browser WARN    Inference API failed, using simulation 400
[00:31:51.234] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.748856469824466
[00:31:51.584] Server  ERROR   Backend returned 400
[00:31:51.599] Server  ERROR   Backend returned 400
[00:31:51.613] Server  ERROR   Backend returned 400
[00:31:51.716] Browser WARN    Inference API failed, using simulation 400
[00:31:51.716] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.041959261124832115
[00:31:51.716] Browser WARN    Inference API failed, using simulation 400
[00:31:51.716] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.39506758803661096
[00:31:51.716] Browser WARN    Inference API failed, using simulation 400
[00:31:51.716] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8113458183420917
[00:31:52.074] Server  ERROR   Backend returned 400
[00:31:52.096] Server  ERROR   Backend returned 400
[00:31:52.107] Server  ERROR   Backend returned 400
[00:31:52.212] Browser WARN    Inference API failed, using simulation 400
[00:31:52.212] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.24062491483816817
[00:31:52.212] Browser WARN    Inference API failed, using simulation 400
[00:31:52.212] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7132919370147246
[00:31:52.212] Browser WARN    Inference API failed, using simulation 400
[00:31:52.212] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.39260009542843166
[00:31:52.578] Server  ERROR   Backend returned 400
[00:31:52.593] Server  ERROR   Backend returned 400
[00:31:52.606] Server  ERROR   Backend returned 400
[00:31:52.713] Browser WARN    Inference API failed, using simulation 400
[00:31:52.713] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.38693903115437467
[00:31:52.713] Browser WARN    Inference API failed, using simulation 400
[00:31:52.713] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.37656579042064775
[00:31:52.713] Browser WARN    Inference API failed, using simulation 400
[00:31:52.713] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2019522576681297
[00:31:53.080] Server  ERROR   Backend returned 400
[00:31:53.107] Server  ERROR   Backend returned 400
[00:31:53.117] Server  ERROR   Backend returned 400
[00:31:53.223] Browser WARN    Inference API failed, using simulation 400
[00:31:53.223] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.19406185454491526
[00:31:53.223] Browser WARN    Inference API failed, using simulation 400
[00:31:53.223] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.40464307710914843
[00:31:53.223] Browser WARN    Inference API failed, using simulation 400
[00:31:53.223] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9337552060255586
[00:31:53.578] Server  ERROR   Backend returned 400
[00:31:53.600] Server  ERROR   Backend returned 400
[00:31:53.623] Server  ERROR   Backend returned 400
[00:31:53.726] Browser WARN    Inference API failed, using simulation 400
[00:31:53.726] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.34580015138968806
[00:31:53.726] Browser WARN    Inference API failed, using simulation 400
[00:31:53.726] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7844067635742279
[00:31:53.726] Browser WARN    Inference API failed, using simulation 400
[00:31:53.726] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.44404801939779603
[00:31:54.077] Server  ERROR   Backend returned 400
[00:31:54.102] Server  ERROR   Backend returned 400
[00:31:54.105] Server  ERROR   Backend returned 400
[00:31:54.214] Browser WARN    Inference API failed, using simulation 400
[00:31:54.214] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.22185554679717645
[00:31:54.214] Browser WARN    Inference API failed, using simulation 400
[00:31:54.214] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.19913472680177102
[00:31:54.214] Browser WARN    Inference API failed, using simulation 400
[00:31:54.214] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.20791881364193188
[00:31:54.582] Server  ERROR   Backend returned 400
[00:31:54.605] Server  ERROR   Backend returned 400
[00:31:54.616] Server  ERROR   Backend returned 400
[00:31:54.720] Browser WARN    Inference API failed, using simulation 400
[00:31:54.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.22153401900351943
[00:31:54.720] Browser WARN    Inference API failed, using simulation 400
[00:31:54.720] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.03904846132539724
[00:31:54.720] Browser WARN    Inference API failed, using simulation 400
[00:31:54.720] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.28191809577193183
[00:31:55.080] Server  ERROR   Backend returned 400
[00:31:55.103] Server  ERROR   Backend returned 400
[00:31:55.115] Server  ERROR   Backend returned 400
[00:31:55.219] Browser WARN    Inference API failed, using simulation 400
[00:31:55.219] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.40808337502240755
[00:31:55.219] Browser WARN    Inference API failed, using simulation 400
[00:31:55.219] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7932098904094226
[00:31:55.219] Browser WARN    Inference API failed, using simulation 400
[00:31:55.219] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.14051258907149006
[00:31:55.585] Server  ERROR   Backend returned 400
[00:31:55.602] Server  ERROR   Backend returned 400
[00:31:55.613] Server  ERROR   Backend returned 400
[00:31:55.724] Browser WARN    Inference API failed, using simulation 400
[00:31:55.724] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.48621978634046203
[00:31:55.724] Browser WARN    Inference API failed, using simulation 400
[00:31:55.724] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.0723823519076312
[00:31:55.724] Browser WARN    Inference API failed, using simulation 400
[00:31:55.724] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.03829914321751943
[00:31:56.079] Server  ERROR   Backend returned 400
[00:31:56.115] Server  ERROR   Backend returned 400
[00:31:56.118] Server  ERROR   Backend returned 400
[00:31:56.230] Browser WARN    Inference API failed, using simulation 400
[00:31:56.230] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8753575929992002
[00:31:56.230] Browser WARN    Inference API failed, using simulation 400
[00:31:56.230] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7023142593539399
[00:31:56.230] Browser WARN    Inference API failed, using simulation 400
[00:31:56.230] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8873167352208164
[00:31:56.579] Server  ERROR   Backend returned 400
[00:31:56.596] Server  ERROR   Backend returned 400
[00:31:56.607] Server  ERROR   Backend returned 400
[00:31:56.719] Browser WARN    Inference API failed, using simulation 400
[00:31:56.719] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9290148159187619
[00:31:56.720] Browser WARN    Inference API failed, using simulation 400
[00:31:56.720] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7978795453012425
[00:31:56.720] Browser WARN    Inference API failed, using simulation 400
[00:31:56.720] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1407683421486261
[00:31:57.084] Server  ERROR   Backend returned 400
[00:31:57.110] Server  ERROR   Backend returned 400
[00:31:57.112] Server  ERROR   Backend returned 400
[00:31:57.221] Browser WARN    Inference API failed, using simulation 400
[00:31:57.221] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.14591092637015457
[00:31:57.221] Browser WARN    Inference API failed, using simulation 400
[00:31:57.221] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.10103984836682217
[00:31:57.221] Browser WARN    Inference API failed, using simulation 400
[00:31:57.221] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.40434878954477066
[00:31:57.586] Server  ERROR   Backend returned 400
[00:31:57.602] Server  ERROR   Backend returned 400
[00:31:57.616] Server  ERROR   Backend returned 400
[00:31:57.720] Browser WARN    Inference API failed, using simulation 400
[00:31:57.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4139021216697484
[00:31:57.721] Browser WARN    Inference API failed, using simulation 400
[00:31:57.721] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7917632676004553
[00:31:57.721] Browser WARN    Inference API failed, using simulation 400
[00:31:57.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.09785112253123229
[00:31:58.078] Server  ERROR   Backend returned 400
[00:31:58.103] Server  ERROR   Backend returned 400
[00:31:58.105] Server  ERROR   Backend returned 400
[00:31:58.213] Browser WARN    Inference API failed, using simulation 400
[00:31:58.213] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.33972729225430187
[00:31:58.213] Browser WARN    Inference API failed, using simulation 400
[00:31:58.213] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3748308953781216
[00:31:58.213] Browser WARN    Inference API failed, using simulation 400
[00:31:58.213] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.03972959729403669
[00:31:58.583] Server  ERROR   Backend returned 400
[00:31:58.605] Server  ERROR   Backend returned 400
[00:31:58.618] Server  ERROR   Backend returned 400
[00:31:58.722] Browser WARN    Inference API failed, using simulation 400
[00:31:58.722] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.45284762117807414
[00:31:58.722] Browser WARN    Inference API failed, using simulation 400
[00:31:58.722] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.06379202199495615
[00:31:58.722] Browser WARN    Inference API failed, using simulation 400
[00:31:58.722] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7538286443125001
[00:31:59.079] Server  ERROR   Backend returned 400
[00:31:59.098] Server  ERROR   Backend returned 400
[00:31:59.111] Server  ERROR   Backend returned 400
[00:31:59.227] Browser WARN    Inference API failed, using simulation 400
[00:31:59.227] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.38156954951600114
[00:31:59.227] Browser WARN    Inference API failed, using simulation 400
[00:31:59.227] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.49370095297506006
[00:31:59.227] Browser WARN    Inference API failed, using simulation 400
[00:31:59.227] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.01385873625755163
[00:31:59.585] Server  ERROR   Backend returned 400
[00:31:59.605] Server  ERROR   Backend returned 400
[00:31:59.620] Server  ERROR   Backend returned 400
[00:31:59.723] Browser WARN    Inference API failed, using simulation 400
[00:31:59.723] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.34902732192683805
[00:31:59.723] Browser WARN    Inference API failed, using simulation 400
[00:31:59.723] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.38371923520197065
[00:31:59.723] Browser WARN    Inference API failed, using simulation 400
[00:31:59.723] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3987843891736706
[00:32:00.078] Server  ERROR   Backend returned 400
[00:32:00.103] Server  ERROR   Backend returned 400
[00:32:00.114] Server  ERROR   Backend returned 400
[00:32:00.217] Browser WARN    Inference API failed, using simulation 400
[00:32:00.217] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7053021500102183
[00:32:00.217] Browser WARN    Inference API failed, using simulation 400
[00:32:00.217] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.005777471172793036
[00:32:00.217] Browser WARN    Inference API failed, using simulation 400
[00:32:00.217] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8412297755124895
[00:32:00.575] Server  ERROR   Backend returned 400
[00:32:00.591] Server  ERROR   Backend returned 400
[00:32:00.618] Server  ERROR   Backend returned 400
[00:32:00.736] Browser WARN    Inference API failed, using simulation 400
[00:32:00.736] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8884318491868104
[00:32:00.736] Browser WARN    Inference API failed, using simulation 400
[00:32:00.736] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.36079801431832365
[00:32:00.736] Browser WARN    Inference API failed, using simulation 400
[00:32:00.736] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1670894777315945
[00:32:01.078] Server  ERROR   Backend returned 400
[00:32:01.099] Server  ERROR   Backend returned 400
[00:32:01.112] Server  ERROR   Backend returned 400
[00:32:01.216] Browser WARN    Inference API failed, using simulation 400
[00:32:01.216] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1667536140833379
[00:32:01.216] Browser WARN    Inference API failed, using simulation 400
[00:32:01.216] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7455050719090485
[00:32:01.216] Browser WARN    Inference API failed, using simulation 400
[00:32:01.216] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3736850745387675
[00:32:01.582] Server  ERROR   Backend returned 400
[00:32:01.613] Server  ERROR   Backend returned 400
[00:32:01.627] Server  ERROR   Backend returned 400
[00:32:01.729] Browser WARN    Inference API failed, using simulation 400
[00:32:01.729] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8723054575920041
[00:32:01.729] Browser WARN    Inference API failed, using simulation 400
[00:32:01.729] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9586187806202984
[00:32:01.729] Browser WARN    Inference API failed, using simulation 400
[00:32:01.729] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3824654714256894
[00:32:02.076] Server  ERROR   Backend returned 400
[00:32:02.100] Server  ERROR   Backend returned 400
[00:32:02.111] Server  ERROR   Backend returned 400
[00:32:02.216] Browser WARN    Inference API failed, using simulation 400
[00:32:02.216] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.19516800428964448
[00:32:02.216] Browser WARN    Inference API failed, using simulation 400
[00:32:02.216] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7017416862357778
[00:32:02.216] Browser WARN    Inference API failed, using simulation 400
[00:32:02.216] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3828296133428881
[00:32:02.602] Server  ERROR   Backend returned 400
[00:32:02.625] Server  ERROR   Backend returned 400
[00:32:02.640] Server  ERROR   Backend returned 400
[00:32:02.746] Browser WARN    Inference API failed, using simulation 400
[00:32:02.746] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7428019792727681
[00:32:02.746] Browser WARN    Inference API failed, using simulation 400
[00:32:02.746] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3941119120125225
[00:32:02.746] Browser WARN    Inference API failed, using simulation 400
[00:32:02.746] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2552385861595629
[00:32:03.080] Server  ERROR   Backend returned 400
[00:32:03.099] Server  ERROR   Backend returned 400
[00:32:03.113] Server  ERROR   Backend returned 400
[00:32:03.217] Browser WARN    Inference API failed, using simulation 400
[00:32:03.217] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8007979379293548
[00:32:03.217] Browser WARN    Inference API failed, using simulation 400
[00:32:03.217] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9860333243137963
[00:32:03.217] Browser WARN    Inference API failed, using simulation 400
[00:32:03.217] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7176367945120904
[00:32:03.586] Server  ERROR   Backend returned 400
[00:32:03.600] Server  ERROR   Backend returned 400
[00:32:03.614] Server  ERROR   Backend returned 400
[00:32:03.718] Browser WARN    Inference API failed, using simulation 400
[00:32:03.718] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9220730288140673
[00:32:03.718] Browser WARN    Inference API failed, using simulation 400
[00:32:03.718] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3425084103793686
[00:32:03.718] Browser WARN    Inference API failed, using simulation 400
[00:32:03.718] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9248631891640962
[00:32:04.079] Server  ERROR   Backend returned 400
[00:32:04.111] Server  ERROR   Backend returned 400
[00:32:04.120] Server  ERROR   Backend returned 400
[00:32:04.226] Browser WARN    Inference API failed, using simulation 400
[00:32:04.226] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3370102915124899
[00:32:04.226] Browser WARN    Inference API failed, using simulation 400
[00:32:04.226] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8625586841536582
[00:32:04.226] Browser WARN    Inference API failed, using simulation 400
[00:32:04.226] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.22757144278946417
[00:32:04.624] Server  ERROR   Backend returned 400
[00:32:04.686] Server  ERROR   Backend returned 400
[00:32:04.690] Server  ERROR   Backend returned 400
[00:32:04.806] Browser WARN    Inference API failed, using simulation 400
[00:32:04.806] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7701241247676229
[00:32:04.806] Browser WARN    Inference API failed, using simulation 400
[00:32:04.806] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.49691503876187065
[00:32:04.806] Browser WARN    Inference API failed, using simulation 400
[00:32:04.806] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.14856306094933047
[00:32:05.107] Server  ERROR   Backend returned 400
[00:32:05.154] Server  ERROR   Backend returned 400
[00:32:05.174] Server  ERROR   Backend returned 400
[00:32:05.277] Browser WARN    Inference API failed, using simulation 400
[00:32:05.277] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.994387985105491
[00:32:05.277] Browser WARN    Inference API failed, using simulation 400
[00:32:05.277] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.048175944800800063
[00:32:05.277] Browser WARN    Inference API failed, using simulation 400
[00:32:05.277] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.43601747045828887
[00:32:05.579] Server  ERROR   Backend returned 400
[00:32:05.611] Server  ERROR   Backend returned 400
[00:32:05.613] Server  ERROR   Backend returned 400
[00:32:05.721] Browser WARN    Inference API failed, using simulation 400
[00:32:05.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.24903718797516117
[00:32:05.721] Browser WARN    Inference API failed, using simulation 400
[00:32:05.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4685561990755489
[00:32:05.721] Browser WARN    Inference API failed, using simulation 400
[00:32:05.721] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8031918180640103
[00:32:06.084] Server  ERROR   Backend returned 400
[00:32:06.102] Server  ERROR   Backend returned 400
[00:32:06.117] Server  ERROR   Backend returned 400
[00:32:06.220] Browser WARN    Inference API failed, using simulation 400
[00:32:06.220] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.37109803895289645
[00:32:06.220] Browser WARN    Inference API failed, using simulation 400
[00:32:06.220] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.04753055993838867
[00:32:06.220] Browser WARN    Inference API failed, using simulation 400
[00:32:06.220] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.034628532488389985
[00:32:06.577] Server  ERROR   Backend returned 400
[00:32:06.599] Server  ERROR   Backend returned 400
[00:32:06.602] Server  ERROR   Backend returned 400
[00:32:06.710] Browser WARN    Inference API failed, using simulation 400
[00:32:06.710] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.16858896789294475
[00:32:06.710] Browser WARN    Inference API failed, using simulation 400
[00:32:06.710] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8880022832396143
[00:32:06.710] Browser WARN    Inference API failed, using simulation 400
[00:32:06.710] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.46884535457478654
[00:32:07.084] Server  ERROR   Backend returned 400
[00:32:07.103] Server  ERROR   Backend returned 400
[00:32:07.114] Server  ERROR   Backend returned 400
[00:32:07.229] Browser WARN    Inference API failed, using simulation 400
[00:32:07.229] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.14894485135710378
[00:32:07.229] Browser WARN    Inference API failed, using simulation 400
[00:32:07.229] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.003810638689683843
[00:32:07.229] Browser WARN    Inference API failed, using simulation 400
[00:32:07.229] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8142386757855178
[00:32:07.584] Server  ERROR   Backend returned 400
[00:32:07.604] Server  ERROR   Backend returned 400
[00:32:07.616] Server  ERROR   Backend returned 400
[00:32:07.720] Browser WARN    Inference API failed, using simulation 400
[00:32:07.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.20174472490560136
[00:32:07.720] Browser WARN    Inference API failed, using simulation 400
[00:32:07.720] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.23758508301222458
[00:32:07.720] Browser WARN    Inference API failed, using simulation 400
[00:32:07.720] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3561813871930526
[00:32:08.082] Server  ERROR   Backend returned 400
[00:32:08.104] Server  ERROR   Backend returned 400
[00:32:08.114] Server  ERROR   Backend returned 400
[00:32:08.218] Browser WARN    Inference API failed, using simulation 400
[00:32:08.219] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2751456092622548
[00:32:08.219] Browser WARN    Inference API failed, using simulation 400
[00:32:08.219] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.260508671382608
[00:32:08.219] Browser WARN    Inference API failed, using simulation 400
[00:32:08.219] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.25787014743563397
[00:32:08.587] Server  ERROR   Backend returned 400
[00:32:08.603] Server  ERROR   Backend returned 400
[00:32:08.618] Server  ERROR   Backend returned 400
[00:32:08.721] Browser WARN    Inference API failed, using simulation 400
[00:32:08.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.014411572622277102
[00:32:08.721] Browser WARN    Inference API failed, using simulation 400
[00:32:08.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.09334051848002783
[00:32:08.721] Browser WARN    Inference API failed, using simulation 400
[00:32:08.721] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9125369058527709
[00:32:09.079] Server  ERROR   Backend returned 400
[00:32:09.098] Server  ERROR   Backend returned 400
[00:32:09.110] Server  ERROR   Backend returned 400
[00:32:09.213] Browser WARN    Inference API failed, using simulation 400
[00:32:09.213] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3061154463412817
[00:32:09.213] Browser WARN    Inference API failed, using simulation 400
[00:32:09.213] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.14275822017274375
[00:32:09.213] Browser WARN    Inference API failed, using simulation 400
[00:32:09.213] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.23710124402528054
[00:32:09.580] Server  ERROR   Backend returned 400
[00:32:09.604] Server  ERROR   Backend returned 400
[00:32:09.619] Server  ERROR   Backend returned 400
[00:32:09.723] Browser WARN    Inference API failed, using simulation 400
[00:32:09.723] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.47822239027836677
[00:32:09.723] Browser WARN    Inference API failed, using simulation 400
[00:32:09.723] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3264173714749018
[00:32:09.723] Browser WARN    Inference API failed, using simulation 400
[00:32:09.723] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2405700110256938
[00:32:10.079] Server  ERROR   Backend returned 400
[00:32:10.102] Server  ERROR   Backend returned 400
[00:32:10.118] Server  ERROR   Backend returned 400
[00:32:10.222] Browser WARN    Inference API failed, using simulation 400
[00:32:10.222] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.870023380338202
[00:32:10.222] Browser WARN    Inference API failed, using simulation 400
[00:32:10.222] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4701580953110784
[00:32:10.222] Browser WARN    Inference API failed, using simulation 400
[00:32:10.222] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7678727740359608
[00:32:10.583] Server  ERROR   Backend returned 400
[00:32:10.606] Server  ERROR   Backend returned 400
[00:32:10.618] Server  ERROR   Backend returned 400
[00:32:10.721] Browser WARN    Inference API failed, using simulation 400
[00:32:10.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2987326825183503
[00:32:10.721] Browser WARN    Inference API failed, using simulation 400
[00:32:10.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3835535753501103
[00:32:10.721] Browser WARN    Inference API failed, using simulation 400
[00:32:10.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.23818845898845753
[00:32:11.078] Server  ERROR   Backend returned 400
[00:32:11.093] Server  ERROR   Backend returned 400
[00:32:11.105] Server  ERROR   Backend returned 400
[00:32:11.209] Browser WARN    Inference API failed, using simulation 400
[00:32:11.209] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.07504381857298259
[00:32:11.209] Browser WARN    Inference API failed, using simulation 400
[00:32:11.209] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8271358282695294
[00:32:11.209] Browser WARN    Inference API failed, using simulation 400
[00:32:11.209] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8924940435728138
[00:32:11.582] Server  ERROR   Backend returned 400
[00:32:11.604] Server  ERROR   Backend returned 400
[00:32:11.618] Server  ERROR   Backend returned 400
[00:32:11.722] Browser WARN    Inference API failed, using simulation 400
[00:32:11.722] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9041027232725922
[00:32:11.722] Browser WARN    Inference API failed, using simulation 400
[00:32:11.722] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.14208850529953126
[00:32:11.722] Browser WARN    Inference API failed, using simulation 400
[00:32:11.722] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.30640768819631325
[00:32:12.081] Server  ERROR   Backend returned 400
[00:32:12.105] Server  ERROR   Backend returned 400
[00:32:12.111] Server  ERROR   Backend returned 400
[00:32:12.216] Browser WARN    Inference API failed, using simulation 400
[00:32:12.216] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.0014008256150215836
[00:32:12.216] Browser WARN    Inference API failed, using simulation 400
[00:32:12.216] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.10670445167177983
[00:32:12.216] Browser WARN    Inference API failed, using simulation 400
[00:32:12.216] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.44558473160680795
[00:32:12.581] Server  ERROR   Backend returned 400
[00:32:12.604] Server  ERROR   Backend returned 400
[00:32:12.618] Server  ERROR   Backend returned 400
[00:32:12.722] Browser WARN    Inference API failed, using simulation 400
[00:32:12.722] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.10246877841219981
[00:32:12.722] Browser WARN    Inference API failed, using simulation 400
[00:32:12.722] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7890571585911994
[00:32:12.722] Browser WARN    Inference API failed, using simulation 400
[00:32:12.722] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.09632817399751026
[00:32:13.085] Server  ERROR   Backend returned 400
[00:32:13.102] Server  ERROR   Backend returned 400
[00:32:13.113] Server  ERROR   Backend returned 400
[00:32:13.218] Browser WARN    Inference API failed, using simulation 400
[00:32:13.218] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9324595941058151
[00:32:13.218] Browser WARN    Inference API failed, using simulation 400
[00:32:13.218] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.11431274402426167
[00:32:13.218] Browser WARN    Inference API failed, using simulation 400
[00:32:13.218] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.13014250961426244
[00:32:13.581] Server  ERROR   Backend returned 400
[00:32:13.604] Server  ERROR   Backend returned 400
[00:32:13.616] Server  ERROR   Backend returned 400
[00:32:13.719] Browser WARN    Inference API failed, using simulation 400
[00:32:13.719] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.15142844037407394
[00:32:13.719] Browser WARN    Inference API failed, using simulation 400
[00:32:13.719] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.10933687667618719
[00:32:13.719] Browser WARN    Inference API failed, using simulation 400
[00:32:13.719] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.39942336845304266
[00:32:14.077] Server  ERROR   Backend returned 400
[00:32:14.092] Server  ERROR   Backend returned 400
[00:32:14.112] Server  ERROR   Backend returned 400
[00:32:14.219] Browser WARN    Inference API failed, using simulation 400
[00:32:14.219] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9843634930292278
[00:32:14.219] Browser WARN    Inference API failed, using simulation 400
[00:32:14.219] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.030205660129782264
[00:32:14.219] Browser WARN    Inference API failed, using simulation 400
[00:32:14.219] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.46416666104904253
[00:32:14.582] Server  ERROR   Backend returned 400
[00:32:14.605] Server  ERROR   Backend returned 400
[00:32:14.618] Server  ERROR   Backend returned 400
[00:32:14.722] Browser WARN    Inference API failed, using simulation 400
[00:32:14.722] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4053879645503139
[00:32:14.722] Browser WARN    Inference API failed, using simulation 400
[00:32:14.722] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.13746880664142713
[00:32:14.722] Browser WARN    Inference API failed, using simulation 400
[00:32:14.722] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.24143560296194555
[00:32:15.078] Server  ERROR   Backend returned 400
[00:32:15.098] Server  ERROR   Backend returned 400
[00:32:15.111] Server  ERROR   Backend returned 400
[00:32:15.215] Browser WARN    Inference API failed, using simulation 400
[00:32:15.215] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.22492502463423975
[00:32:15.215] Browser WARN    Inference API failed, using simulation 400
[00:32:15.215] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4030946066348848
[00:32:15.215] Browser WARN    Inference API failed, using simulation 400
[00:32:15.215] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.28932856228946474
[00:32:15.580] Server  ERROR   Backend returned 400
[00:32:15.597] Server  ERROR   Backend returned 400
[00:32:15.609] Server  ERROR   Backend returned 400
[00:32:15.727] Browser WARN    Inference API failed, using simulation 400
[00:32:15.727] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.43310676604254345
[00:32:15.727] Browser WARN    Inference API failed, using simulation 400
[00:32:15.727] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4092099958727481
[00:32:15.727] Browser WARN    Inference API failed, using simulation 400
[00:32:15.727] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.016086473150591585
[00:32:16.084] Server  ERROR   Backend returned 400
[00:32:16.106] Server  ERROR   Backend returned 400
[00:32:16.118] Server  ERROR   Backend returned 400
[00:32:16.222] Browser WARN    Inference API failed, using simulation 400
[00:32:16.222] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8063768164309532
[00:32:16.222] Browser WARN    Inference API failed, using simulation 400
[00:32:16.222] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.08202260005821865
[00:32:16.222] Browser WARN    Inference API failed, using simulation 400
[00:32:16.222] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.914057360654082
[00:32:16.586] Server  ERROR   Backend returned 400
[00:32:16.601] Server  ERROR   Backend returned 400
[00:32:16.616] Server  ERROR   Backend returned 400
[00:32:16.720] Browser WARN    Inference API failed, using simulation 400
[00:32:16.720] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.950913690845884
[00:32:16.720] Browser WARN    Inference API failed, using simulation 400
[00:32:16.720] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.17680847244281245
[00:32:16.720] Browser WARN    Inference API failed, using simulation 400
[00:32:16.720] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.052085021025140954
[00:32:17.085] Server  ERROR   Backend returned 400
[00:32:17.118] Server  ERROR   Backend returned 400
[00:32:17.125] Server  ERROR   Backend returned 400
[00:32:17.229] Browser WARN    Inference API failed, using simulation 400
[00:32:17.229] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.10248441056189145
[00:32:17.229] Browser WARN    Inference API failed, using simulation 400
[00:32:17.229] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.47814010828422
[00:32:17.229] Browser WARN    Inference API failed, using simulation 400
[00:32:17.229] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4204245902522689
[00:32:17.583] Server  ERROR   Backend returned 400
[00:32:17.603] Server  ERROR   Backend returned 400
[00:32:17.615] Server  ERROR   Backend returned 400
[00:32:17.720] Browser WARN    Inference API failed, using simulation 400
[00:32:17.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.22992451578170692
[00:32:17.720] Browser WARN    Inference API failed, using simulation 400
[00:32:17.720] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.08378160299976012
[00:32:17.720] Browser WARN    Inference API failed, using simulation 400
[00:32:17.720] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9215818773638307
[00:32:18.084] Server  ERROR   Backend returned 400
[00:32:18.111] Server  ERROR   Backend returned 400
[00:32:18.113] Server  ERROR   Backend returned 400
[00:32:18.221] Browser WARN    Inference API failed, using simulation 400
[00:32:18.221] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9844407302583735
[00:32:18.221] Browser WARN    Inference API failed, using simulation 400
[00:32:18.221] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4117534214732701
[00:32:18.221] Browser WARN    Inference API failed, using simulation 400
[00:32:18.221] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.07484248812746419
[00:32:18.588] Server  ERROR   Backend returned 400
[00:32:18.610] Server  ERROR   Backend returned 400
[00:32:18.623] Server  ERROR   Backend returned 400
[00:32:18.731] Browser WARN    Inference API failed, using simulation 400
[00:32:18.731] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3433696794520891
[00:32:18.731] Browser WARN    Inference API failed, using simulation 400
[00:32:18.731] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3945088035077492
[00:32:18.731] Browser WARN    Inference API failed, using simulation 400
[00:32:18.731] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.22624876732176996
[00:32:19.090] Server  ERROR   Backend returned 400
[00:32:19.113] Server  ERROR   Backend returned 400
[00:32:19.128] Server  ERROR   Backend returned 400
[00:32:19.231] Browser WARN    Inference API failed, using simulation 400
[00:32:19.231] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2915598423893068
[00:32:19.231] Browser WARN    Inference API failed, using simulation 400
[00:32:19.231] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7987621420011735
[00:32:19.231] Browser WARN    Inference API failed, using simulation 400
[00:32:19.231] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2521844417670759
[00:32:19.592] Server  ERROR   Backend returned 400
[00:32:19.624] Server  ERROR   Backend returned 400
[00:32:19.646] Server  ERROR   Backend returned 400
[00:32:19.751] Browser WARN    Inference API failed, using simulation 400
[00:32:19.751] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8813782196076614
[00:32:19.751] Browser WARN    Inference API failed, using simulation 400
[00:32:19.751] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.754440380200258
[00:32:19.751] Browser WARN    Inference API failed, using simulation 400
[00:32:19.751] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.05407049605941072
[00:32:20.087] Server  ERROR   Backend returned 400
[00:32:20.111] Server  ERROR   Backend returned 400
[00:32:20.114] Server  ERROR   Backend returned 400
[00:32:20.239] Browser WARN    Inference API failed, using simulation 400
[00:32:20.239] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9841963908428427
[00:32:20.239] Browser WARN    Inference API failed, using simulation 400
[00:32:20.239] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.450717992960403
[00:32:20.239] Browser WARN    Inference API failed, using simulation 400
[00:32:20.239] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.33393830820647197
[00:32:20.586] Server  ERROR   Backend returned 400
[00:32:20.619] Server  ERROR   Backend returned 400
[00:32:20.622] Server  ERROR   Backend returned 400
[00:32:20.730] Browser WARN    Inference API failed, using simulation 400
[00:32:20.730] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3063663840142197
[00:32:20.730] Browser WARN    Inference API failed, using simulation 400
[00:32:20.730] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.20212134558019956
[00:32:20.730] Browser WARN    Inference API failed, using simulation 400
[00:32:20.730] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9237194909133325
[00:32:21.088] Server  ERROR   Backend returned 400
[00:32:21.106] Server  ERROR   Backend returned 400
[00:32:21.119] Server  ERROR   Backend returned 400
[00:32:21.235] Browser WARN    Inference API failed, using simulation 400
[00:32:21.235] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.22562934650011296
[00:32:21.235] Browser WARN    Inference API failed, using simulation 400
[00:32:21.235] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3313241083350835
[00:32:21.235] Browser WARN    Inference API failed, using simulation 400
[00:32:21.235] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2680232090789325
[00:32:21.583] Server  ERROR   Backend returned 400
[00:32:21.599] Server  ERROR   Backend returned 400
[00:32:21.614] Server  ERROR   Backend returned 400
[00:32:21.732] Browser WARN    Inference API failed, using simulation 400
[00:32:21.732] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.22868829840486227
[00:32:21.732] Browser WARN    Inference API failed, using simulation 400
[00:32:21.732] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8946384462831495
[00:32:21.732] Browser WARN    Inference API failed, using simulation 400
[00:32:21.732] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8741265854145898
[00:32:22.087] Server  ERROR   Backend returned 400
[00:32:22.104] Server  ERROR   Backend returned 400
[00:32:22.116] Server  ERROR   Backend returned 400
[00:32:22.231] Browser WARN    Inference API failed, using simulation 400
[00:32:22.231] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.086128592538468
[00:32:22.231] Browser WARN    Inference API failed, using simulation 400
[00:32:22.231] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9115095888242628
[00:32:22.231] Browser WARN    Inference API failed, using simulation 400
[00:32:22.231] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1182444713954724
[00:32:22.578] Server  ERROR   Backend returned 400
[00:32:22.595] Server  ERROR   Backend returned 400
[00:32:22.612] Server  ERROR   Backend returned 400
[00:32:22.716] Browser WARN    Inference API failed, using simulation 400
[00:32:22.716] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.48313912492589434
[00:32:22.716] Browser WARN    Inference API failed, using simulation 400
[00:32:22.716] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4356910193828653
[00:32:22.716] Browser WARN    Inference API failed, using simulation 400
[00:32:22.716] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.0739462881458508
[00:32:23.088] Server  ERROR   Backend returned 400
[00:32:23.102] Server  ERROR   Backend returned 400
[00:32:23.127] Server  ERROR   Backend returned 400
[00:32:23.231] Browser WARN    Inference API failed, using simulation 400
[00:32:23.231] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.857815281556104
[00:32:23.231] Browser WARN    Inference API failed, using simulation 400
[00:32:23.231] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.38919382342381975
[00:32:23.231] Browser WARN    Inference API failed, using simulation 400
[00:32:23.231] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8499272071229274
[00:32:23.577] Server  ERROR   Backend returned 400
[00:32:23.601] Server  ERROR   Backend returned 400
[00:32:23.603] Server  ERROR   Backend returned 400
[00:32:23.715] Browser WARN    Inference API failed, using simulation 400
[00:32:23.715] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8273818709687583
[00:32:23.715] Browser WARN    Inference API failed, using simulation 400
[00:32:23.715] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.036336756015728944
[00:32:23.715] Browser WARN    Inference API failed, using simulation 400
[00:32:23.715] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3087947495211279
[00:32:24.079] Server  ERROR   Backend returned 400
[00:32:24.097] Server  ERROR   Backend returned 400
[00:32:24.110] Server  ERROR   Backend returned 400
[00:32:24.214] Browser WARN    Inference API failed, using simulation 400
[00:32:24.214] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8324551914910325
[00:32:24.214] Browser WARN    Inference API failed, using simulation 400
[00:32:24.214] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.29989476388505276
[00:32:24.214] Browser WARN    Inference API failed, using simulation 400
[00:32:24.214] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.75257469158673
[00:32:24.583] Server  ERROR   Backend returned 400
[00:32:24.600] Server  ERROR   Backend returned 400
[00:32:24.612] Server  ERROR   Backend returned 400
[00:32:24.716] Browser WARN    Inference API failed, using simulation 400
[00:32:24.716] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.09507126966996288
[00:32:24.716] Browser WARN    Inference API failed, using simulation 400
[00:32:24.716] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9061365691860856
[00:32:24.717] Browser WARN    Inference API failed, using simulation 400
[00:32:24.717] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7238200823102033
[00:32:25.078] Server  ERROR   Backend returned 400
[00:32:25.099] Server  ERROR   Backend returned 400
[00:32:25.119] Server  ERROR   Backend returned 400
[00:32:25.223] Browser WARN    Inference API failed, using simulation 400
[00:32:25.223] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8466449743582559
[00:32:25.223] Browser WARN    Inference API failed, using simulation 400
[00:32:25.223] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.17821881173525633
[00:32:25.223] Browser WARN    Inference API failed, using simulation 400
[00:32:25.223] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4572991621652084
[00:32:25.579] Server  ERROR   Backend returned 400
[00:32:25.602] Server  ERROR   Backend returned 400
[00:32:25.613] Server  ERROR   Backend returned 400
[00:32:25.719] Browser WARN    Inference API failed, using simulation 400
[00:32:25.719] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9224655008888286
[00:32:25.719] Browser WARN    Inference API failed, using simulation 400
[00:32:25.719] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9986311178668879
[00:32:25.719] Browser WARN    Inference API failed, using simulation 400
[00:32:25.719] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.20927158406943697
[00:32:26.078] Server  ERROR   Backend returned 400
[00:32:26.100] Server  ERROR   Backend returned 400
[00:32:26.113] Server  ERROR   Backend returned 400
[00:32:26.219] Browser WARN    Inference API failed, using simulation 400
[00:32:26.219] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.22584202958965655
[00:32:26.219] Browser WARN    Inference API failed, using simulation 400
[00:32:26.219] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7135547758427303
[00:32:26.219] Browser WARN    Inference API failed, using simulation 400
[00:32:26.219] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.10807773247283958
[00:32:26.578] Server  ERROR   Backend returned 400
[00:32:26.603] Server  ERROR   Backend returned 400
[00:32:26.615] Server  ERROR   Backend returned 400
[00:32:26.720] Browser WARN    Inference API failed, using simulation 400
[00:32:26.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.15208226236666855
[00:32:26.720] Browser WARN    Inference API failed, using simulation 400
[00:32:26.720] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2547870471379702
[00:32:26.720] Browser WARN    Inference API failed, using simulation 400
[00:32:26.720] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.09701671906250176
[00:32:27.102] Server  ERROR   Backend returned 400
[00:32:27.151] Server  ERROR   Backend returned 400
[00:32:27.199] Server  ERROR   Backend returned 400
[00:32:27.302] Browser WARN    Inference API failed, using simulation 400
[00:32:27.303] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7249072645972439
[00:32:27.303] Browser WARN    Inference API failed, using simulation 400
[00:32:27.303] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.07321643146730789
[00:32:27.303] Browser WARN    Inference API failed, using simulation 400
[00:32:27.303] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.47522060389532017
[00:32:27.577] Server  ERROR   Backend returned 400
[00:32:27.601] Server  ERROR   Backend returned 400
[00:32:27.612] Server  ERROR   Backend returned 400
[00:32:27.717] Browser WARN    Inference API failed, using simulation 400
[00:32:27.717] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7382075503539718
[00:32:27.717] Browser WARN    Inference API failed, using simulation 400
[00:32:27.717] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3286841954258549
[00:32:27.717] Browser WARN    Inference API failed, using simulation 400
[00:32:27.717] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.35431025068387895
[00:32:28.082] Server  ERROR   Backend returned 400
[00:32:28.110] Server  ERROR   Backend returned 400
[00:32:28.116] Server  ERROR   Backend returned 400
[00:32:28.223] Browser WARN    Inference API failed, using simulation 400
[00:32:28.223] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9875807443059554
[00:32:28.223] Browser WARN    Inference API failed, using simulation 400
[00:32:28.223] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.029023035758972582
[00:32:28.223] Browser WARN    Inference API failed, using simulation 400
[00:32:28.223] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.37847111855828125
[00:32:28.577] Server  ERROR   Backend returned 400
[00:32:28.603] Server  ERROR   Backend returned 400
[00:32:28.605] Server  ERROR   Backend returned 400
[00:32:28.714] Browser WARN    Inference API failed, using simulation 400
[00:32:28.714] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3889795107616088
[00:32:28.714] Browser WARN    Inference API failed, using simulation 400
[00:32:28.714] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3963592952849498
[00:32:28.714] Browser WARN    Inference API failed, using simulation 400
[00:32:28.714] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.09769514643561711
[00:32:29.077] Server  ERROR   Backend returned 400
[00:32:29.100] Server  ERROR   Backend returned 400
[00:32:29.111] Server  ERROR   Backend returned 400
[00:32:29.215] Browser WARN    Inference API failed, using simulation 400
[00:32:29.215] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3779230370465847
[00:32:29.215] Browser WARN    Inference API failed, using simulation 400
[00:32:29.215] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.15963858814728255
[00:32:29.215] Browser WARN    Inference API failed, using simulation 400
[00:32:29.215] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8221292820993281
[00:32:29.579] Server  ERROR   Backend returned 400
[00:32:29.595] Server  ERROR   Backend returned 400
[00:32:29.606] Server  ERROR   Backend returned 400
[00:32:29.713] Browser WARN    Inference API failed, using simulation 400
[00:32:29.713] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.049482160991551005
[00:32:29.713] Browser WARN    Inference API failed, using simulation 400
[00:32:29.713] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7141762226963004
[00:32:29.713] Browser WARN    Inference API failed, using simulation 400
[00:32:29.713] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9232599736114989
[00:32:30.080] Server  ERROR   Backend returned 400
[00:32:30.099] Server  ERROR   Backend returned 400
[00:32:30.117] Server  ERROR   Backend returned 400
[00:32:30.220] Browser WARN    Inference API failed, using simulation 400
[00:32:30.220] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4803899326824305
[00:32:30.220] Browser WARN    Inference API failed, using simulation 400
[00:32:30.220] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.04620419629230421
[00:32:30.220] Browser WARN    Inference API failed, using simulation 400
[00:32:30.220] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.04405714544338979
[00:32:30.583] Server  ERROR   Backend returned 400
[00:32:30.599] Server  ERROR   Backend returned 400
[00:32:30.611] Server  ERROR   Backend returned 400
[00:32:30.715] Browser WARN    Inference API failed, using simulation 400
[00:32:30.715] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8038047112194066
[00:32:30.715] Browser WARN    Inference API failed, using simulation 400
[00:32:30.715] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7497463729236906
[00:32:30.715] Browser WARN    Inference API failed, using simulation 400
[00:32:30.715] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7167448778303118
[00:32:31.083] Server  ERROR   Backend returned 400
[00:32:31.113] Server  ERROR   Backend returned 400
[00:32:31.130] Server  ERROR   Backend returned 400
[00:32:31.235] Browser WARN    Inference API failed, using simulation 400
[00:32:31.235] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8974238954589501
[00:32:31.235] Browser WARN    Inference API failed, using simulation 400
[00:32:31.235] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.03448684208365693
[00:32:31.235] Browser WARN    Inference API failed, using simulation 400
[00:32:31.235] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4798344356018565
[00:32:31.579] Server  ERROR   Backend returned 400
[00:32:31.597] Server  ERROR   Backend returned 400
[00:32:31.609] Server  ERROR   Backend returned 400
[00:32:31.713] Browser WARN    Inference API failed, using simulation 400
[00:32:31.713] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7947684622160541
[00:32:31.713] Browser WARN    Inference API failed, using simulation 400
[00:32:31.713] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3323997558881139
[00:32:31.713] Browser WARN    Inference API failed, using simulation 400
[00:32:31.713] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2942438293069143
[00:32:32.081] Server  ERROR   Backend returned 400
[00:32:32.098] Server  ERROR   Backend returned 400
[00:32:32.113] Server  ERROR   Backend returned 400
[00:32:32.217] Browser WARN    Inference API failed, using simulation 400
[00:32:32.217] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.44151027008233523
[00:32:32.217] Browser WARN    Inference API failed, using simulation 400
[00:32:32.217] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8672118115402285
[00:32:32.217] Browser WARN    Inference API failed, using simulation 400
[00:32:32.217] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8611952662977852
[00:32:32.579] Server  ERROR   Backend returned 400
[00:32:32.598] Server  ERROR   Backend returned 400
[00:32:32.612] Server  ERROR   Backend returned 400
[00:32:32.716] Browser WARN    Inference API failed, using simulation 400
[00:32:32.716] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8328860459122999
[00:32:32.716] Browser WARN    Inference API failed, using simulation 400
[00:32:32.716] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4738798318493751
[00:32:32.716] Browser WARN    Inference API failed, using simulation 400
[00:32:32.716] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.18272272860069882
[00:32:33.078] Server  ERROR   Backend returned 400
[00:32:33.097] Server  ERROR   Backend returned 400
[00:32:33.110] Server  ERROR   Backend returned 400
[00:32:33.215] Browser WARN    Inference API failed, using simulation 400
[00:32:33.216] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1499103994728928
[00:32:33.216] Browser WARN    Inference API failed, using simulation 400
[00:32:33.216] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.29361536297415725
[00:32:33.216] Browser WARN    Inference API failed, using simulation 400
[00:32:33.216] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.959282082087615
[00:32:33.586] Server  ERROR   Backend returned 400
[00:32:33.611] Server  ERROR   Backend returned 400
[00:32:33.617] Server  ERROR   Backend returned 400
[00:32:33.722] Browser WARN    Inference API failed, using simulation 400
[00:32:33.722] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.37047732308331766
[00:32:33.722] Browser WARN    Inference API failed, using simulation 400
[00:32:33.722] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.305154628588986
[00:32:33.722] Browser WARN    Inference API failed, using simulation 400
[00:32:33.722] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.44096178116784934
[00:32:34.078] Server  ERROR   Backend returned 400
[00:32:34.093] Server  ERROR   Backend returned 400
[00:32:34.110] Server  ERROR   Backend returned 400
[00:32:34.215] Browser WARN    Inference API failed, using simulation 400
[00:32:34.215] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.006414820487507966
[00:32:34.215] Browser WARN    Inference API failed, using simulation 400
[00:32:34.215] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.08519569416645406
[00:32:34.215] Browser WARN    Inference API failed, using simulation 400
[00:32:34.215] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3687274988225678
[00:32:34.579] Server  ERROR   Backend returned 400
[00:32:34.605] Server  ERROR   Backend returned 400
[00:32:34.619] Server  ERROR   Backend returned 400
[00:32:34.724] Browser WARN    Inference API failed, using simulation 400
[00:32:34.724] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3446152744994467
[00:32:34.724] Browser WARN    Inference API failed, using simulation 400
[00:32:34.724] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.08214866163639478
[00:32:34.724] Browser WARN    Inference API failed, using simulation 400
[00:32:34.724] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2886704183935918
[00:32:35.078] Server  ERROR   Backend returned 400
[00:32:35.101] Server  ERROR   Backend returned 400
[00:32:35.111] Server  ERROR   Backend returned 400
[00:32:35.216] Browser WARN    Inference API failed, using simulation 400
[00:32:35.216] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.43465052741644444
[00:32:35.216] Browser WARN    Inference API failed, using simulation 400
[00:32:35.216] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2868194589907017
[00:32:35.216] Browser WARN    Inference API failed, using simulation 400
[00:32:35.216] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.43651223210938783
[00:32:35.579] Server  ERROR   Backend returned 400
[00:32:35.610] Server  ERROR   Backend returned 400
[00:32:35.612] Server  ERROR   Backend returned 400
[00:32:35.721] Browser WARN    Inference API failed, using simulation 400
[00:32:35.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3089053982693606
[00:32:35.721] Browser WARN    Inference API failed, using simulation 400
[00:32:35.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.11532000560006295
[00:32:35.721] Browser WARN    Inference API failed, using simulation 400
[00:32:35.721] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7033109984562718
[00:32:36.085] Server  ERROR   Backend returned 400
[00:32:36.107] Server  ERROR   Backend returned 400
[00:32:36.118] Server  ERROR   Backend returned 400
[00:32:36.221] Browser WARN    Inference API failed, using simulation 400
[00:32:36.221] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.06339723762594629
[00:32:36.221] Browser WARN    Inference API failed, using simulation 400
[00:32:36.221] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.19174924464174803
[00:32:36.221] Browser WARN    Inference API failed, using simulation 400
[00:32:36.221] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.897010242349981
[00:32:36.580] Server  ERROR   Backend returned 400
[00:32:36.612] Server  ERROR   Backend returned 400
[00:32:36.624] Server  ERROR   Backend returned 400
[00:32:36.729] Browser WARN    Inference API failed, using simulation 400
[00:32:36.729] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.98667214755861
[00:32:36.729] Browser WARN    Inference API failed, using simulation 400
[00:32:36.729] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.20376151688952132
[00:32:36.729] Browser WARN    Inference API failed, using simulation 400
[00:32:36.729] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.035030441594583184
[00:32:37.077] Server  ERROR   Backend returned 400
[00:32:37.094] Server  ERROR   Backend returned 400
[00:32:37.112] Server  ERROR   Backend returned 400
[00:32:37.217] Browser WARN    Inference API failed, using simulation 400
[00:32:37.217] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9612199618528562
[00:32:37.217] Browser WARN    Inference API failed, using simulation 400
[00:32:37.217] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4051263463542077
[00:32:37.217] Browser WARN    Inference API failed, using simulation 400
[00:32:37.217] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.18673604124242377
[00:32:37.578] Server  ERROR   Backend returned 400
[00:32:37.604] Server  ERROR   Backend returned 400
[00:32:37.617] Server  ERROR   Backend returned 400
[00:32:37.721] Browser WARN    Inference API failed, using simulation 400
[00:32:37.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.17091743407643012
[00:32:37.721] Browser WARN    Inference API failed, using simulation 400
[00:32:37.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.011262733266698133
[00:32:37.721] Browser WARN    Inference API failed, using simulation 400
[00:32:37.721] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8998511530671066
[00:32:38.078] Server  ERROR   Backend returned 400
[00:32:38.093] Server  ERROR   Backend returned 400
[00:32:38.111] Server  ERROR   Backend returned 400
[00:32:38.214] Browser WARN    Inference API failed, using simulation 400
[00:32:38.214] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7281705979895863
[00:32:38.214] Browser WARN    Inference API failed, using simulation 400
[00:32:38.214] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.05963952256385263
[00:32:38.214] Browser WARN    Inference API failed, using simulation 400
[00:32:38.214] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2991599524030167
[00:32:38.582] Server  ERROR   Backend returned 400
[00:32:38.600] Server  ERROR   Backend returned 400
[00:32:38.613] Server  ERROR   Backend returned 400
[00:32:38.716] Browser WARN    Inference API failed, using simulation 400
[00:32:38.716] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.23862944718460116
[00:32:38.716] Browser WARN    Inference API failed, using simulation 400
[00:32:38.716] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.29552704333106605
[00:32:38.716] Browser WARN    Inference API failed, using simulation 400
[00:32:38.716] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.49089061927869265
[00:32:39.084] Server  ERROR   Backend returned 400
[00:32:39.113] Server  ERROR   Backend returned 400
[00:32:39.116] Server  ERROR   Backend returned 400
[00:32:39.226] Browser WARN    Inference API failed, using simulation 400
[00:32:39.226] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.12460217039817151
[00:32:39.226] Browser WARN    Inference API failed, using simulation 400
[00:32:39.226] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.21903915792088663
[00:32:39.226] Browser WARN    Inference API failed, using simulation 400
[00:32:39.226] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.20617912210082234
[00:32:39.588] Server  ERROR   Backend returned 400
[00:32:39.602] Server  ERROR   Backend returned 400
[00:32:39.634] Server  ERROR   Backend returned 400
[00:32:39.738] Browser WARN    Inference API failed, using simulation 400
[00:32:39.738] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9874645647082461
[00:32:39.738] Browser WARN    Inference API failed, using simulation 400
[00:32:39.738] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.49727883928520145
[00:32:39.738] Browser WARN    Inference API failed, using simulation 400
[00:32:39.738] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4001932240168766
[00:32:40.080] Server  ERROR   Backend returned 400
[00:32:40.106] Server  ERROR   Backend returned 400
[00:32:40.119] Server  ERROR   Backend returned 400
[00:32:40.223] Browser WARN    Inference API failed, using simulation 400
[00:32:40.223] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8105732512196191
[00:32:40.223] Browser WARN    Inference API failed, using simulation 400
[00:32:40.223] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.30176537468780446
[00:32:40.223] Browser WARN    Inference API failed, using simulation 400
[00:32:40.223] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3477459688127751
[00:32:40.579] Server  ERROR   Backend returned 400
[00:32:40.601] Server  ERROR   Backend returned 400
[00:32:40.614] Server  ERROR   Backend returned 400
[00:32:40.719] Browser WARN    Inference API failed, using simulation 400
[00:32:40.719] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.41143007584532776
[00:32:40.719] Browser WARN    Inference API failed, using simulation 400
[00:32:40.719] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8493302897493898
[00:32:40.719] Browser WARN    Inference API failed, using simulation 400
[00:32:40.719] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9716888211678038
[00:32:41.083] Server  ERROR   Backend returned 400
[00:32:41.112] Server  ERROR   Backend returned 400
[00:32:41.114] Server  ERROR   Backend returned 400
[00:32:41.222] Browser WARN    Inference API failed, using simulation 400
[00:32:41.222] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4550493274729407
[00:32:41.222] Browser WARN    Inference API failed, using simulation 400
[00:32:41.222] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.15890538600725823
[00:32:41.222] Browser WARN    Inference API failed, using simulation 400
[00:32:41.222] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.45163798147597634
[00:32:41.578] Server  ERROR   Backend returned 400
[00:32:41.603] Server  ERROR   Backend returned 400
[00:32:41.614] Server  ERROR   Backend returned 400
[00:32:41.719] Browser WARN    Inference API failed, using simulation 400
[00:32:41.719] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7670646643119952
[00:32:41.719] Browser WARN    Inference API failed, using simulation 400
[00:32:41.719] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.372142335033367
[00:32:41.719] Browser WARN    Inference API failed, using simulation 400
[00:32:41.719] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.15112030525481923
[00:32:42.081] Server  ERROR   Backend returned 400
[00:32:42.108] Server  ERROR   Backend returned 400
[00:32:42.125] Server  ERROR   Backend returned 400
[00:32:42.230] Browser WARN    Inference API failed, using simulation 400
[00:32:42.230] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8152220739451383
[00:32:42.230] Browser WARN    Inference API failed, using simulation 400
[00:32:42.230] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3724475983976065
[00:32:42.230] Browser WARN    Inference API failed, using simulation 400
[00:32:42.230] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3711746377998043
[00:32:42.576] Server  ERROR   Backend returned 400
[00:32:42.601] Server  ERROR   Backend returned 400
[00:32:42.615] Server  ERROR   Backend returned 400
[00:32:42.719] Browser WARN    Inference API failed, using simulation 400
[00:32:42.719] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7496582590440849
[00:32:42.719] Browser WARN    Inference API failed, using simulation 400
[00:32:42.719] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.25157501797370224
[00:32:42.719] Browser WARN    Inference API failed, using simulation 400
[00:32:42.719] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.19171967530031364
[00:32:43.079] Server  ERROR   Backend returned 400
[00:32:43.097] Server  ERROR   Backend returned 400
[00:32:43.111] Server  ERROR   Backend returned 400
[00:32:43.214] Browser WARN    Inference API failed, using simulation 400
[00:32:43.214] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.263627463881381
[00:32:43.214] Browser WARN    Inference API failed, using simulation 400
[00:32:43.214] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.0930362859532442
[00:32:43.214] Browser WARN    Inference API failed, using simulation 400
[00:32:43.214] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.23853790769975203
[00:32:43.578] Server  ERROR   Backend returned 400
[00:32:43.593] Server  ERROR   Backend returned 400
[00:32:43.605] Server  ERROR   Backend returned 400
[00:32:43.708] Browser WARN    Inference API failed, using simulation 400
[00:32:43.708] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.04447073807777713
[00:32:43.708] Browser WARN    Inference API failed, using simulation 400
[00:32:43.708] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4760444441494234
[00:32:43.708] Browser WARN    Inference API failed, using simulation 400
[00:32:43.708] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3991603381459935
[00:32:44.079] Server  ERROR   Backend returned 400
[00:32:44.097] Server  ERROR   Backend returned 400
[00:32:44.111] Server  ERROR   Backend returned 400
[00:32:44.214] Browser WARN    Inference API failed, using simulation 400
[00:32:44.214] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.49369120611789746
[00:32:44.214] Browser WARN    Inference API failed, using simulation 400
[00:32:44.214] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.06583875577288206
[00:32:44.214] Browser WARN    Inference API failed, using simulation 400
[00:32:44.214] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.22885043690494256
[00:32:44.582] Server  ERROR   Backend returned 400
[00:32:44.600] Server  ERROR   Backend returned 400
[00:32:44.613] Server  ERROR   Backend returned 400
[00:32:44.719] Browser WARN    Inference API failed, using simulation 400
[00:32:44.719] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2936819693020816
[00:32:44.719] Browser WARN    Inference API failed, using simulation 400
[00:32:44.719] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.35777430666284077
[00:32:44.719] Browser WARN    Inference API failed, using simulation 400
[00:32:44.719] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.29620586195485
[00:32:45.076] Server  ERROR   Backend returned 400
[00:32:45.097] Server  ERROR   Backend returned 400
[00:32:45.111] Server  ERROR   Backend returned 400
[00:32:45.215] Browser WARN    Inference API failed, using simulation 400
[00:32:45.215] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9255035627417949
[00:32:45.215] Browser WARN    Inference API failed, using simulation 400
[00:32:45.215] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.30015087846535343
[00:32:45.215] Browser WARN    Inference API failed, using simulation 400
[00:32:45.215] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.126920039558434
[00:32:45.577] Server  ERROR   Backend returned 400
[00:32:45.598] Server  ERROR   Backend returned 400
[00:32:45.616] Server  ERROR   Backend returned 400
[00:32:45.728] Browser WARN    Inference API failed, using simulation 400
[00:32:45.728] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9735686630945279
[00:32:45.728] Browser WARN    Inference API failed, using simulation 400
[00:32:45.728] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.851577494420417
[00:32:45.728] Browser WARN    Inference API failed, using simulation 400
[00:32:45.728] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.02776150483101192
[00:32:46.079] Server  ERROR   Backend returned 400
[00:32:46.107] Server  ERROR   Backend returned 400
[00:32:46.120] Server  ERROR   Backend returned 400
[00:32:46.225] Browser WARN    Inference API failed, using simulation 400
[00:32:46.226] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.37865706004786553
[00:32:46.226] Browser WARN    Inference API failed, using simulation 400
[00:32:46.226] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7116863031677912
[00:32:46.226] Browser WARN    Inference API failed, using simulation 400
[00:32:46.226] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3656382376009005
[00:32:46.586] Server  ERROR   Backend returned 400
[00:32:46.603] Server  ERROR   Backend returned 400
[00:32:46.630] Server  ERROR   Backend returned 400
[00:32:46.733] Browser WARN    Inference API failed, using simulation 400
[00:32:46.733] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.0936750590498917
[00:32:46.733] Browser WARN    Inference API failed, using simulation 400
[00:32:46.733] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7750190678405661
[00:32:46.733] Browser WARN    Inference API failed, using simulation 400
[00:32:46.733] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4832434125583609
[00:32:47.078] Server  ERROR   Backend returned 400
[00:32:47.099] Server  ERROR   Backend returned 400
[00:32:47.115] Server  ERROR   Backend returned 400
[00:32:47.221] Browser WARN    Inference API failed, using simulation 400
[00:32:47.221] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8038481523930278
[00:32:47.221] Browser WARN    Inference API failed, using simulation 400
[00:32:47.221] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.07677579657950018
[00:32:47.221] Browser WARN    Inference API failed, using simulation 400
[00:32:47.221] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8563871828345162
[00:32:47.579] Server  ERROR   Backend returned 400
[00:32:47.600] Server  ERROR   Backend returned 400
[00:32:47.613] Server  ERROR   Backend returned 400
[00:32:47.720] Browser WARN    Inference API failed, using simulation 400
[00:32:47.720] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.0533119239662479
[00:32:47.720] Browser WARN    Inference API failed, using simulation 400
[00:32:47.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.021584792427162436
[00:32:47.720] Browser WARN    Inference API failed, using simulation 400
[00:32:47.720] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8043915686515559
[00:32:48.079] Server  ERROR   Backend returned 400
[00:32:48.097] Server  ERROR   Backend returned 400
[00:32:48.109] Server  ERROR   Backend returned 400
[00:32:48.215] Browser WARN    Inference API failed, using simulation 400
[00:32:48.215] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9997275804112404
[00:32:48.215] Browser WARN    Inference API failed, using simulation 400
[00:32:48.215] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7021500755336559
[00:32:48.215] Browser WARN    Inference API failed, using simulation 400
[00:32:48.215] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.13879498142509827
[00:32:48.580] Server  ERROR   Backend returned 400
[00:32:48.596] Server  ERROR   Backend returned 400
[00:32:48.618] Server  ERROR   Backend returned 400
[00:32:48.723] Browser WARN    Inference API failed, using simulation 400
[00:32:48.723] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8257157832703905
[00:32:48.723] Browser WARN    Inference API failed, using simulation 400
[00:32:48.723] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7704850910339194
[00:32:48.723] Browser WARN    Inference API failed, using simulation 400
[00:32:48.723] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3150961433313546
[00:32:49.081] Server  ERROR   Backend returned 400
[00:32:49.099] Server  ERROR   Backend returned 400
[00:32:49.111] Server  ERROR   Backend returned 400
[00:32:49.215] Browser WARN    Inference API failed, using simulation 400
[00:32:49.215] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.040694193598045325
[00:32:49.215] Browser WARN    Inference API failed, using simulation 400
[00:32:49.215] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3842742413307191
[00:32:49.215] Browser WARN    Inference API failed, using simulation 400
[00:32:49.215] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.431202856935421
[00:32:49.577] Server  ERROR   Backend returned 400
[00:32:49.592] Server  ERROR   Backend returned 400
[00:32:49.604] Server  ERROR   Backend returned 400
[00:32:49.708] Browser WARN    Inference API failed, using simulation 400
[00:32:49.708] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1865203971390627
[00:32:49.708] Browser WARN    Inference API failed, using simulation 400
[00:32:49.708] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.03581036298092899
[00:32:49.708] Browser WARN    Inference API failed, using simulation 400
[00:32:49.708] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.264529963414215
[00:32:50.082] Server  ERROR   Backend returned 400
[00:32:50.099] Server  ERROR   Backend returned 400
[00:32:50.112] Server  ERROR   Backend returned 400
[00:32:50.217] Browser WARN    Inference API failed, using simulation 400
[00:32:50.217] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.16250044662791852
[00:32:50.217] Browser WARN    Inference API failed, using simulation 400
[00:32:50.217] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9192178587508972
[00:32:50.217] Browser WARN    Inference API failed, using simulation 400
[00:32:50.217] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.0834462859352616
[00:32:50.583] Server  ERROR   Backend returned 400
[00:32:50.604] Server  ERROR   Backend returned 400
[00:32:50.616] Server  ERROR   Backend returned 400
[00:32:50.729] Browser WARN    Inference API failed, using simulation 400
[00:32:50.729] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.06765433699901025
[00:32:50.729] Browser WARN    Inference API failed, using simulation 400
[00:32:50.729] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7656325058281491
[00:32:50.729] Browser WARN    Inference API failed, using simulation 400
[00:32:50.729] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7782027458532653
[00:32:51.129] Server  ERROR   Backend returned 400
[00:32:51.144] Server  ERROR   Backend returned 400
[00:32:51.146] Server  ERROR   Backend returned 400
[00:32:51.257] Browser WARN    Inference API failed, using simulation 400
[00:32:51.258] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9623544795555785
[00:32:51.258] Browser WARN    Inference API failed, using simulation 400
[00:32:51.258] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3382728798736807
[00:32:51.258] Browser WARN    Inference API failed, using simulation 400
[00:32:51.258] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8832326221261486
[00:32:51.588] Server  ERROR   Backend returned 400
[00:32:51.609] Server  ERROR   Backend returned 400
[00:32:51.634] Server  ERROR   Backend returned 400
[00:32:51.739] Browser WARN    Inference API failed, using simulation 400
[00:32:51.739] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.24108183586037257
[00:32:51.739] Browser WARN    Inference API failed, using simulation 400
[00:32:51.739] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9149134110022962
[00:32:51.739] Browser WARN    Inference API failed, using simulation 400
[00:32:51.739] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7886359900446416
[00:32:52.085] Server  ERROR   Backend returned 400
[00:32:52.117] Server  ERROR   Backend returned 400
[00:32:52.119] Server  ERROR   Backend returned 400
[00:32:52.229] Browser WARN    Inference API failed, using simulation 400
[00:32:52.229] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.14957409149483647
[00:32:52.229] Browser WARN    Inference API failed, using simulation 400
[00:32:52.229] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.030103964522508464
[00:32:52.229] Browser WARN    Inference API failed, using simulation 400
[00:32:52.229] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.10508497848850629
[00:32:52.586] Server  ERROR   Backend returned 400
[00:32:52.603] Server  ERROR   Backend returned 400
[00:32:52.621] Server  ERROR   Backend returned 400
[00:32:52.732] Browser WARN    Inference API failed, using simulation 400
[00:32:52.732] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.17876605468086942
[00:32:52.732] Browser WARN    Inference API failed, using simulation 400
[00:32:52.732] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.873020940537388
[00:32:52.732] Browser WARN    Inference API failed, using simulation 400
[00:32:52.732] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3592985501554554
[00:32:53.085] Server  ERROR   Backend returned 400
[00:32:53.105] Server  ERROR   Backend returned 400
[00:32:53.116] Server  ERROR   Backend returned 400
[00:32:53.219] Browser WARN    Inference API failed, using simulation 400
[00:32:53.219] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9693759716153288
[00:32:53.219] Browser WARN    Inference API failed, using simulation 400
[00:32:53.219] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4922283405449018
[00:32:53.219] Browser WARN    Inference API failed, using simulation 400
[00:32:53.219] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.10718443387061549
[00:32:53.596] Server  ERROR   Backend returned 400
[00:32:53.611] Server  ERROR   Backend returned 400
[00:32:53.632] Server  ERROR   Backend returned 400
[00:32:53.740] Browser WARN    Inference API failed, using simulation 400
[00:32:53.740] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9303193028472121
[00:32:53.740] Browser WARN    Inference API failed, using simulation 400
[00:32:53.740] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2182473725467346
[00:32:53.740] Browser WARN    Inference API failed, using simulation 400
[00:32:53.740] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.40263424857729074
[00:32:54.082] Server  ERROR   Backend returned 400
[00:32:54.112] Server  ERROR   Backend returned 400
[00:32:54.116] Server  ERROR   Backend returned 400
[00:32:54.222] Browser WARN    Inference API failed, using simulation 400
[00:32:54.222] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9341435292028175
[00:32:54.222] Browser WARN    Inference API failed, using simulation 400
[00:32:54.222] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4985066147302733
[00:32:54.222] Browser WARN    Inference API failed, using simulation 400
[00:32:54.222] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.34995448264276907
[00:32:54.584] Server  ERROR   Backend returned 400
[00:32:54.607] Server  ERROR   Backend returned 400
[00:32:54.619] Server  ERROR   Backend returned 400
[00:32:54.723] Browser WARN    Inference API failed, using simulation 400
[00:32:54.723] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.11084668225740701
[00:32:54.723] Browser WARN    Inference API failed, using simulation 400
[00:32:54.723] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.32126049228195225
[00:32:54.723] Browser WARN    Inference API failed, using simulation 400
[00:32:54.723] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.18475041464095404
[00:32:55.085] Server  ERROR   Backend returned 400
[00:32:55.098] Server  ERROR   Backend returned 400
[00:32:55.115] Server  ERROR   Backend returned 400
[00:32:55.232] Browser WARN    Inference API failed, using simulation 400
[00:32:55.232] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.17294633805273307
[00:32:55.232] Browser WARN    Inference API failed, using simulation 400
[00:32:55.232] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.25496666719379646
[00:32:55.232] Browser WARN    Inference API failed, using simulation 400
[00:32:55.232] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.957041036611225
[00:32:55.578] Server  ERROR   Backend returned 400
[00:32:55.599] Server  ERROR   Backend returned 400
[00:32:55.602] Server  ERROR   Backend returned 400
[00:32:55.710] Browser WARN    Inference API failed, using simulation 400
[00:32:55.710] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.15789884673854315
[00:32:55.710] Browser WARN    Inference API failed, using simulation 400
[00:32:55.710] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8466071392620854
[00:32:55.710] Browser WARN    Inference API failed, using simulation 400
[00:32:55.710] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4031097960114179
[00:32:56.077] Server  ERROR   Backend returned 400
[00:32:56.093] Server  ERROR   Backend returned 400
[00:32:56.111] Server  ERROR   Backend returned 400
[00:32:56.215] Browser WARN    Inference API failed, using simulation 400
[00:32:56.215] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.22451426672433028
[00:32:56.215] Browser WARN    Inference API failed, using simulation 400
[00:32:56.215] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7222609113075036
[00:32:56.215] Browser WARN    Inference API failed, using simulation 400
[00:32:56.215] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7049781906654322
[00:32:56.578] Server  ERROR   Backend returned 400
[00:32:56.593] Server  ERROR   Backend returned 400
[00:32:56.605] Server  ERROR   Backend returned 400
[00:32:56.711] Browser WARN    Inference API failed, using simulation 400
[00:32:56.711] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.37636317307161465
[00:32:56.711] Browser WARN    Inference API failed, using simulation 400
[00:32:56.711] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.12387870123376299
[00:32:56.711] Browser WARN    Inference API failed, using simulation 400
[00:32:56.711] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.22384191475898657
[00:32:57.083] Server  ERROR   Backend returned 400
[00:32:57.095] Server  ERROR   Backend returned 400
[00:32:57.110] Server  ERROR   Backend returned 400
[00:32:57.214] Browser WARN    Inference API failed, using simulation 400
[00:32:57.214] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8353301342456545
[00:32:57.214] Browser WARN    Inference API failed, using simulation 400
[00:32:57.214] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.43401404280226213
[00:32:57.214] Browser WARN    Inference API failed, using simulation 400
[00:32:57.214] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4570650510476235
[00:32:57.579] Server  ERROR   Backend returned 400
[00:32:57.595] Server  ERROR   Backend returned 400
[00:32:57.610] Server  ERROR   Backend returned 400
[00:32:57.717] Browser WARN    Inference API failed, using simulation 400
[00:32:57.717] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3568780431159999
[00:32:57.717] Browser WARN    Inference API failed, using simulation 400
[00:32:57.717] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1931951369994806
[00:32:57.717] Browser WARN    Inference API failed, using simulation 400
[00:32:57.717] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.15284263324857839
[00:32:58.086] Server  ERROR   Backend returned 400
[00:32:58.100] Server  ERROR   Backend returned 400
[00:32:58.112] Server  ERROR   Backend returned 400
[00:32:58.216] Browser WARN    Inference API failed, using simulation 400
[00:32:58.216] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.360097727370858
[00:32:58.216] Browser WARN    Inference API failed, using simulation 400
[00:32:58.216] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2333078646828482
[00:32:58.216] Browser WARN    Inference API failed, using simulation 400
[00:32:58.216] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.036204271995030624
[00:32:58.577] Server  ERROR   Backend returned 400
[00:32:58.590] Server  ERROR   Backend returned 400
[00:32:58.695] Browser WARN    Inference API failed, using simulation 400
[00:32:58.695] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.17998289974922804
[00:32:58.695] Browser WARN    Inference API failed, using simulation 400
[00:32:58.695] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.41761771778435386
[00:32:59.078] Server  ERROR   Backend returned 400
[00:32:59.093] Server  ERROR   Backend returned 400
[00:32:59.113] Server  ERROR   Backend returned 400
[00:32:59.220] Browser WARN    Inference API failed, using simulation 400
[00:32:59.220] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.08275190294584384
[00:32:59.220] Browser WARN    Inference API failed, using simulation 400
[00:32:59.220] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.11013416465435849
[00:32:59.220] Browser WARN    Inference API failed, using simulation 400
[00:32:59.220] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.031832530737402165
[00:32:59.579] Server  ERROR   Backend returned 400
[00:32:59.600] Server  ERROR   Backend returned 400
[00:32:59.613] Server  ERROR   Backend returned 400
[00:32:59.717] Browser WARN    Inference API failed, using simulation 400
[00:32:59.717] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7923176899796082
[00:32:59.717] Browser WARN    Inference API failed, using simulation 400
[00:32:59.717] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.20484985524103327
[00:32:59.717] Browser WARN    Inference API failed, using simulation 400
[00:32:59.717] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7894929282004108
[00:33:00.080] Server  ERROR   Backend returned 400
[00:33:00.106] Server  ERROR   Backend returned 400
[00:33:00.119] Server  ERROR   Backend returned 400
[00:33:00.233] Browser WARN    Inference API failed, using simulation 400
[00:33:00.233] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.41021621952782644
[00:33:00.233] Browser WARN    Inference API failed, using simulation 400
[00:33:00.233] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3003433053895928
[00:33:00.233] Browser WARN    Inference API failed, using simulation 400
[00:33:00.233] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.062382117314306096
[00:33:00.577] Server  ERROR   Backend returned 400
[00:33:00.594] Server  ERROR   Backend returned 400
[00:33:00.616] Server  ERROR   Backend returned 400
[00:33:00.721] Browser WARN    Inference API failed, using simulation 400
[00:33:00.721] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.856297306129611
[00:33:00.721] Browser WARN    Inference API failed, using simulation 400
[00:33:00.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.007947142203150293
[00:33:00.721] Browser WARN    Inference API failed, using simulation 400
[00:33:00.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.03156949213413823
[00:33:01.076] Server  ERROR   Backend returned 400
[00:33:01.104] Server  ERROR   Backend returned 400
[00:33:01.106] Server  ERROR   Backend returned 400
[00:33:01.215] Browser WARN    Inference API failed, using simulation 400
[00:33:01.215] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8528297623157315
[00:33:01.215] Browser WARN    Inference API failed, using simulation 400
[00:33:01.215] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.28385526930328486
[00:33:01.215] Browser WARN    Inference API failed, using simulation 400
[00:33:01.215] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.39345511605886563
[00:33:01.578] Server  ERROR   Backend returned 400
[00:33:01.599] Server  ERROR   Backend returned 400
[00:33:01.612] Server  ERROR   Backend returned 400
[00:33:01.716] Browser WARN    Inference API failed, using simulation 400
[00:33:01.716] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.13058493720392428
[00:33:01.716] Browser WARN    Inference API failed, using simulation 400
[00:33:01.716] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8405304377225153
[00:33:01.716] Browser WARN    Inference API failed, using simulation 400
[00:33:01.716] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.059553637229945555
[00:33:02.081] Server  ERROR   Backend returned 400
[00:33:02.102] Server  ERROR   Backend returned 400
[00:33:02.123] Server  ERROR   Backend returned 400
[00:33:02.230] Browser WARN    Inference API failed, using simulation 400
[00:33:02.230] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7675690575460254
[00:33:02.230] Browser WARN    Inference API failed, using simulation 400
[00:33:02.230] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.23701935615511177
[00:33:02.230] Browser WARN    Inference API failed, using simulation 400
[00:33:02.230] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4289392337814815
[00:33:02.586] Server  ERROR   Backend returned 400
[00:33:02.601] Server  ERROR   Backend returned 400
[00:33:02.612] Server  ERROR   Backend returned 400
[00:33:02.719] Browser WARN    Inference API failed, using simulation 400
[00:33:02.719] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1067285579506081
[00:33:02.719] Browser WARN    Inference API failed, using simulation 400
[00:33:02.719] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8583769063231569
[00:33:02.719] Browser WARN    Inference API failed, using simulation 400
[00:33:02.719] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8585193172188401
[00:33:03.077] Server  ERROR   Backend returned 400
[00:33:03.099] Server  ERROR   Backend returned 400
[00:33:03.112] Server  ERROR   Backend returned 400
[00:33:03.218] Browser WARN    Inference API failed, using simulation 400
[00:33:03.218] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1821283616661497
[00:33:03.218] Browser WARN    Inference API failed, using simulation 400
[00:33:03.218] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.13861145965023774
[00:33:03.218] Browser WARN    Inference API failed, using simulation 400
[00:33:03.218] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.37833409011649743
[00:33:03.577] Server  ERROR   Backend returned 400
[00:33:03.602] Server  ERROR   Backend returned 400
[00:33:03.615] Server  ERROR   Backend returned 400
[00:33:03.720] Browser WARN    Inference API failed, using simulation 400
[00:33:03.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2594433444960082
[00:33:03.720] Browser WARN    Inference API failed, using simulation 400
[00:33:03.720] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8300820759272742
[00:33:03.720] Browser WARN    Inference API failed, using simulation 400
[00:33:03.720] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.21072207075802812
[00:33:04.078] Server  ERROR   Backend returned 400
[00:33:04.094] Server  ERROR   Backend returned 400
[00:33:04.106] Server  ERROR   Backend returned 400
[00:33:04.210] Browser WARN    Inference API failed, using simulation 400
[00:33:04.210] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4350807768741378
[00:33:04.210] Browser WARN    Inference API failed, using simulation 400
[00:33:04.210] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.423244930337227
[00:33:04.210] Browser WARN    Inference API failed, using simulation 400
[00:33:04.210] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.40485910430584837
[00:33:04.585] Server  ERROR   Backend returned 400
[00:33:04.599] Server  ERROR   Backend returned 400
[00:33:04.625] Server  ERROR   Backend returned 400
[00:33:04.729] Browser WARN    Inference API failed, using simulation 400
[00:33:04.729] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.027315511914230928
[00:33:04.729] Browser WARN    Inference API failed, using simulation 400
[00:33:04.729] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.889264486474243
[00:33:04.729] Browser WARN    Inference API failed, using simulation 400
[00:33:04.729] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9755355749131805
[00:33:05.080] Server  ERROR   Backend returned 400
[00:33:05.098] Server  ERROR   Backend returned 400
[00:33:05.119] Server  ERROR   Backend returned 400
[00:33:05.224] Browser WARN    Inference API failed, using simulation 400
[00:33:05.224] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2969301592549422
[00:33:05.224] Browser WARN    Inference API failed, using simulation 400
[00:33:05.224] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.195099539024164
[00:33:05.224] Browser WARN    Inference API failed, using simulation 400
[00:33:05.224] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.34742014720113606
[00:33:05.576] Server  ERROR   Backend returned 400
[00:33:05.605] Server  ERROR   Backend returned 400
[00:33:05.608] Server  ERROR   Backend returned 400
[00:33:05.719] Browser WARN    Inference API failed, using simulation 400
[00:33:05.719] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.10530589664999801
[00:33:05.719] Browser WARN    Inference API failed, using simulation 400
[00:33:05.719] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4894460643219451
[00:33:05.719] Browser WARN    Inference API failed, using simulation 400
[00:33:05.719] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.44495956005939585
[00:33:06.080] Server  ERROR   Backend returned 400
[00:33:06.097] Server  ERROR   Backend returned 400
[00:33:06.115] Server  ERROR   Backend returned 400
[00:33:06.221] Browser WARN    Inference API failed, using simulation 400
[00:33:06.221] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.01653869302154881
[00:33:06.221] Browser WARN    Inference API failed, using simulation 400
[00:33:06.221] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8651995284348591
[00:33:06.221] Browser WARN    Inference API failed, using simulation 400
[00:33:06.221] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2583770664978494
[00:33:06.577] Server  ERROR   Backend returned 400
[00:33:06.610] Server  ERROR   Backend returned 400
[00:33:06.625] Server  ERROR   Backend returned 400
[00:33:06.741] Browser WARN    Inference API failed, using simulation 400
[00:33:06.741] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2946599305166054
[00:33:06.741] Browser WARN    Inference API failed, using simulation 400
[00:33:06.741] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.731771339145818
[00:33:06.741] Browser WARN    Inference API failed, using simulation 400
[00:33:06.741] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7215409637587986
[00:33:07.078] Server  ERROR   Backend returned 400
[00:33:07.097] Server  ERROR   Backend returned 400
[00:33:07.108] Server  ERROR   Backend returned 400
[00:33:07.213] Browser WARN    Inference API failed, using simulation 400
[00:33:07.213] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1451190990423198
[00:33:07.213] Browser WARN    Inference API failed, using simulation 400
[00:33:07.213] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8237319972809045
[00:33:07.213] Browser WARN    Inference API failed, using simulation 400
[00:33:07.213] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7322153596951883
[00:33:07.583] Server  ERROR   Backend returned 400
[00:33:07.599] Server  ERROR   Backend returned 400
[00:33:07.612] Server  ERROR   Backend returned 400
[00:33:07.716] Browser WARN    Inference API failed, using simulation 400
[00:33:07.716] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1752667470280137
[00:33:07.716] Browser WARN    Inference API failed, using simulation 400
[00:33:07.716] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4499377849144327
[00:33:07.716] Browser WARN    Inference API failed, using simulation 400
[00:33:07.716] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9595482680783239
[00:33:08.088] Server  ERROR   Backend returned 400
[00:33:08.103] Server  ERROR   Backend returned 400
[00:33:08.115] Server  ERROR   Backend returned 400
[00:33:08.230] Browser WARN    Inference API failed, using simulation 400
[00:33:08.230] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.24475748582080653
[00:33:08.230] Browser WARN    Inference API failed, using simulation 400
[00:33:08.230] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.45600614283426366
[00:33:08.230] Browser WARN    Inference API failed, using simulation 400
[00:33:08.230] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8883295157906508
[00:33:08.596] Server  ERROR   Backend returned 400
[00:33:08.609] Server  ERROR   Backend returned 400
[00:33:08.630] Server  ERROR   Backend returned 400
[00:33:08.733] Browser WARN    Inference API failed, using simulation 400
[00:33:08.733] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.01385409222036732
[00:33:08.733] Browser WARN    Inference API failed, using simulation 400
[00:33:08.733] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.36721330578383893
[00:33:08.733] Browser WARN    Inference API failed, using simulation 400
[00:33:08.733] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8455219085569194
[00:33:09.076] Server  ERROR   Backend returned 400
[00:33:09.091] Server  ERROR   Backend returned 400
[00:33:09.105] Server  ERROR   Backend returned 400
[00:33:09.211] Browser WARN    Inference API failed, using simulation 400
[00:33:09.211] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.47848692269945814
[00:33:09.211] Browser WARN    Inference API failed, using simulation 400
[00:33:09.211] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3165053275047275
[00:33:09.211] Browser WARN    Inference API failed, using simulation 400
[00:33:09.211] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.01187983497485845
[00:33:09.589] Server  ERROR   Backend returned 400
[00:33:09.605] Server  ERROR   Backend returned 400
[00:33:09.618] Server  ERROR   Backend returned 400
[00:33:09.722] Browser WARN    Inference API failed, using simulation 400
[00:33:09.722] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7051662704417855
[00:33:09.722] Browser WARN    Inference API failed, using simulation 400
[00:33:09.722] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9201103612395517
[00:33:09.722] Browser WARN    Inference API failed, using simulation 400
[00:33:09.722] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8614079711303956
[00:33:10.075] Server  ERROR   Backend returned 400
[00:33:10.092] Server  ERROR   Backend returned 400
[00:33:10.110] Server  ERROR   Backend returned 400
[00:33:10.221] Browser WARN    Inference API failed, using simulation 400
[00:33:10.221] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8681731207012613
[00:33:10.221] Browser WARN    Inference API failed, using simulation 400
[00:33:10.221] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9886796750692572
[00:33:10.221] Browser WARN    Inference API failed, using simulation 400
[00:33:10.221] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8765566788848959
[00:33:10.577] Server  ERROR   Backend returned 400
[00:33:10.603] Server  ERROR   Backend returned 400
[00:33:10.617] Server  ERROR   Backend returned 400
[00:33:10.722] Browser WARN    Inference API failed, using simulation 400
[00:33:10.722] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2598853784359334
[00:33:10.722] Browser WARN    Inference API failed, using simulation 400
[00:33:10.722] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8877294110080136
[00:33:10.722] Browser WARN    Inference API failed, using simulation 400
[00:33:10.722] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4937644053699802
[00:33:11.077] Server  ERROR   Backend returned 400
[00:33:11.093] Server  ERROR   Backend returned 400
[00:33:11.106] Server  ERROR   Backend returned 400
[00:33:11.212] Browser WARN    Inference API failed, using simulation 400
[00:33:11.212] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4145456210402565
[00:33:11.212] Browser WARN    Inference API failed, using simulation 400
[00:33:11.212] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4092008830967927
[00:33:11.212] Browser WARN    Inference API failed, using simulation 400
[00:33:11.212] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3149958862789634
[00:33:11.581] Server  ERROR   Backend returned 400
[00:33:11.599] Server  ERROR   Backend returned 400
[00:33:11.619] Server  ERROR   Backend returned 400
[00:33:11.724] Browser WARN    Inference API failed, using simulation 400
[00:33:11.724] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.24259907196911112
[00:33:11.724] Browser WARN    Inference API failed, using simulation 400
[00:33:11.724] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.01936188051975829
[00:33:11.724] Browser WARN    Inference API failed, using simulation 400
[00:33:11.724] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.26618381816928904
[00:33:12.077] Server  ERROR   Backend returned 400
[00:33:12.093] Server  ERROR   Backend returned 400
[00:33:12.114] Server  ERROR   Backend returned 400
[00:33:12.219] Browser WARN    Inference API failed, using simulation 400
[00:33:12.219] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3629788331732389
[00:33:12.219] Browser WARN    Inference API failed, using simulation 400
[00:33:12.219] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.38251164127737536
[00:33:12.219] Browser WARN    Inference API failed, using simulation 400
[00:33:12.219] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1681962906570903
[00:33:12.582] Server  ERROR   Backend returned 400
[00:33:12.598] Server  ERROR   Backend returned 400
[00:33:12.615] Server  ERROR   Backend returned 400
[00:33:12.719] Browser WARN    Inference API failed, using simulation 400
[00:33:12.719] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8430633837886189
[00:33:12.719] Browser WARN    Inference API failed, using simulation 400
[00:33:12.719] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8468036879824151
[00:33:12.719] Browser WARN    Inference API failed, using simulation 400
[00:33:12.719] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.0458485447884312
[00:33:13.084] Server  ERROR   Backend returned 400
[00:33:13.103] Server  ERROR   Backend returned 400
[00:33:13.118] Server  ERROR   Backend returned 400
[00:33:13.223] Browser WARN    Inference API failed, using simulation 400
[00:33:13.223] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8751664228686611
[00:33:13.223] Browser WARN    Inference API failed, using simulation 400
[00:33:13.223] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.08583095938214413
[00:33:13.223] Browser WARN    Inference API failed, using simulation 400
[00:33:13.223] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.867397189719361
[00:33:13.584] Server  ERROR   Backend returned 400
[00:33:13.598] Server  ERROR   Backend returned 400
[00:33:13.609] Server  ERROR   Backend returned 400
[00:33:13.712] Browser WARN    Inference API failed, using simulation 400
[00:33:13.713] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2895132687736758
[00:33:13.713] Browser WARN    Inference API failed, using simulation 400
[00:33:13.713] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.48480402367879555
[00:33:13.713] Browser WARN    Inference API failed, using simulation 400
[00:33:13.713] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9186595284425105
[00:33:14.078] Server  ERROR   Backend returned 400
[00:33:14.094] Server  ERROR   Backend returned 400
[00:33:14.111] Server  ERROR   Backend returned 400
[00:33:14.218] Browser WARN    Inference API failed, using simulation 400
[00:33:14.218] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.34064161451434516
[00:33:14.218] Browser WARN    Inference API failed, using simulation 400
[00:33:14.218] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9461554943298676
[00:33:14.218] Browser WARN    Inference API failed, using simulation 400
[00:33:14.218] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.22175084308027349
[00:33:14.576] Server  ERROR   Backend returned 400
[00:33:14.600] Server  ERROR   Backend returned 400
[00:33:14.605] Server  ERROR   Backend returned 400
[00:33:14.712] Browser WARN    Inference API failed, using simulation 400
[00:33:14.712] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.006325906819830696
[00:33:14.712] Browser WARN    Inference API failed, using simulation 400
[00:33:14.712] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4347827342101072
[00:33:14.712] Browser WARN    Inference API failed, using simulation 400
[00:33:14.712] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8633079741310054
[00:33:15.075] Server  ERROR   Backend returned 400
[00:33:15.100] Server  ERROR   Backend returned 400
[00:33:15.102] Server  ERROR   Backend returned 400
[00:33:15.219] Browser WARN    Inference API failed, using simulation 400
[00:33:15.219] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1407026348234413
[00:33:15.219] Browser WARN    Inference API failed, using simulation 400
[00:33:15.219] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8646351463465551
[00:33:15.219] Browser WARN    Inference API failed, using simulation 400
[00:33:15.219] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7338435303851693
[00:33:15.585] Server  ERROR   Backend returned 400
[00:33:15.613] Server  ERROR   Backend returned 400
[00:33:15.616] Server  ERROR   Backend returned 400
[00:33:15.726] Browser WARN    Inference API failed, using simulation 400
[00:33:15.726] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9189520232654591
[00:33:15.726] Browser WARN    Inference API failed, using simulation 400
[00:33:15.726] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.20458792906192008
[00:33:15.726] Browser WARN    Inference API failed, using simulation 400
[00:33:15.726] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8410291895186403
[00:33:16.089] Server  ERROR   Backend returned 400
[00:33:16.106] Server  ERROR   Backend returned 400
[00:33:16.131] Server  ERROR   Backend returned 400
[00:33:16.235] Browser WARN    Inference API failed, using simulation 400
[00:33:16.235] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.994063052935132
[00:33:16.235] Browser WARN    Inference API failed, using simulation 400
[00:33:16.235] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.14201302194012272
[00:33:16.235] Browser WARN    Inference API failed, using simulation 400
[00:33:16.235] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7724462181598606
[00:33:16.579] Server  ERROR   Backend returned 400
[00:33:16.619] Server  ERROR   Backend returned 400
[00:33:16.622] Server  ERROR   Backend returned 400
[00:33:16.729] Browser WARN    Inference API failed, using simulation 400
[00:33:16.729] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4919834234062308
[00:33:16.729] Browser WARN    Inference API failed, using simulation 400
[00:33:16.729] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.049444149896688716
[00:33:16.729] Browser WARN    Inference API failed, using simulation 400
[00:33:16.729] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9250729403181036
[00:33:17.084] Server  ERROR   Backend returned 400
[00:33:17.118] Server  ERROR   Backend returned 400
[00:33:17.121] Server  ERROR   Backend returned 400
[00:33:17.231] Browser WARN    Inference API failed, using simulation 400
[00:33:17.231] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.04744982099757683
[00:33:17.231] Browser WARN    Inference API failed, using simulation 400
[00:33:17.231] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.43631642127906317
[00:33:17.231] Browser WARN    Inference API failed, using simulation 400
[00:33:17.231] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7123357429011306
[00:33:17.583] Server  ERROR   Backend returned 400
[00:33:17.607] Server  ERROR   Backend returned 400
[00:33:17.611] Server  ERROR   Backend returned 400
[00:33:17.722] Browser WARN    Inference API failed, using simulation 400
[00:33:17.722] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.46072563127969296
[00:33:17.722] Browser WARN    Inference API failed, using simulation 400
[00:33:17.722] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9318183872596006
[00:33:17.722] Browser WARN    Inference API failed, using simulation 400
[00:33:17.722] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8766342955597556
[00:33:18.082] Server  ERROR   Backend returned 400
[00:33:18.105] Server  ERROR   Backend returned 400
[00:33:18.116] Server  ERROR   Backend returned 400
[00:33:18.220] Browser WARN    Inference API failed, using simulation 400
[00:33:18.220] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8857147409380335
[00:33:18.220] Browser WARN    Inference API failed, using simulation 400
[00:33:18.220] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8389337712100546
[00:33:18.220] Browser WARN    Inference API failed, using simulation 400
[00:33:18.220] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.46640245255571106
[00:33:18.583] Server  ERROR   Backend returned 400
[00:33:18.607] Server  ERROR   Backend returned 400
[00:33:18.617] Server  ERROR   Backend returned 400
[00:33:18.721] Browser WARN    Inference API failed, using simulation 400
[00:33:18.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.45916224769712616
[00:33:18.721] Browser WARN    Inference API failed, using simulation 400
[00:33:18.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.29388035261256545
[00:33:18.721] Browser WARN    Inference API failed, using simulation 400
[00:33:18.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4969499399713677
[00:33:19.089] Server  ERROR   Backend returned 400
[00:33:19.105] Server  ERROR   Backend returned 400
[00:33:19.117] Server  ERROR   Backend returned 400
[00:33:19.223] Browser WARN    Inference API failed, using simulation 400
[00:33:19.223] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.13981315344309275
[00:33:19.223] Browser WARN    Inference API failed, using simulation 400
[00:33:19.223] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.43543086433989187
[00:33:19.223] Browser WARN    Inference API failed, using simulation 400
[00:33:19.223] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.22149167478944753
[00:33:19.583] Server  ERROR   Backend returned 400
[00:33:19.605] Server  ERROR   Backend returned 400
[00:33:19.610] Server  ERROR   Backend returned 400
[00:33:19.721] Browser WARN    Inference API failed, using simulation 400
[00:33:19.721] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9243875947623088
[00:33:19.721] Browser WARN    Inference API failed, using simulation 400
[00:33:19.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.44141190346818016
[00:33:19.721] Browser WARN    Inference API failed, using simulation 400
[00:33:19.721] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.909071117345065
[00:33:20.075] Server  ERROR   Backend returned 400
[00:33:20.104] Server  ERROR   Backend returned 400
[00:33:20.107] Server  ERROR   Backend returned 400
[00:33:20.215] Browser WARN    Inference API failed, using simulation 400
[00:33:20.215] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1945446118333904
[00:33:20.215] Browser WARN    Inference API failed, using simulation 400
[00:33:20.215] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8366227186487658
[00:33:20.215] Browser WARN    Inference API failed, using simulation 400
[00:33:20.215] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.14073450710507734
[00:33:20.583] Server  ERROR   Backend returned 400
[00:33:20.599] Server  ERROR   Backend returned 400
[00:33:20.613] Server  ERROR   Backend returned 400
[00:33:20.717] Browser WARN    Inference API failed, using simulation 400
[00:33:20.717] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8463915090980427
[00:33:20.717] Browser WARN    Inference API failed, using simulation 400
[00:33:20.717] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.029883206234067594
[00:33:20.717] Browser WARN    Inference API failed, using simulation 400
[00:33:20.717] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7990274998149586
[00:33:21.075] Server  ERROR   Backend returned 400
[00:33:21.100] Server  ERROR   Backend returned 400
[00:33:21.111] Server  ERROR   Backend returned 400
[00:33:21.216] Browser WARN    Inference API failed, using simulation 400
[00:33:21.216] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.26956328662100004
[00:33:21.216] Browser WARN    Inference API failed, using simulation 400
[00:33:21.216] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3654031966001974
[00:33:21.216] Browser WARN    Inference API failed, using simulation 400
[00:33:21.216] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.46393372671811106
[00:33:21.576] Server  ERROR   Backend returned 400
[00:33:21.608] Server  ERROR   Backend returned 400
[00:33:21.611] Server  ERROR   Backend returned 400
[00:33:21.720] Browser WARN    Inference API failed, using simulation 400
[00:33:21.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.20857989262567633
[00:33:21.720] Browser WARN    Inference API failed, using simulation 400
[00:33:21.720] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3633347983805322
[00:33:21.720] Browser WARN    Inference API failed, using simulation 400
[00:33:21.720] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.041340178435203656
[00:33:22.077] Server  ERROR   Backend returned 400
[00:33:22.102] Server  ERROR   Backend returned 400
[00:33:22.103] Server  ERROR   Backend returned 400
[00:33:22.211] Browser WARN    Inference API failed, using simulation 400
[00:33:22.211] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3843218344850061
[00:33:22.211] Browser WARN    Inference API failed, using simulation 400
[00:33:22.211] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.48329830152391723
[00:33:22.211] Browser WARN    Inference API failed, using simulation 400
[00:33:22.211] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4385362947468036
[00:33:22.576] Server  ERROR   Backend returned 400
[00:33:22.599] Server  ERROR   Backend returned 400
[00:33:22.602] Server  ERROR   Backend returned 400
[00:33:22.710] Browser WARN    Inference API failed, using simulation 400
[00:33:22.710] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4925240706532338
[00:33:22.710] Browser WARN    Inference API failed, using simulation 400
[00:33:22.710] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.05754963600855317
[00:33:22.710] Browser WARN    Inference API failed, using simulation 400
[00:33:22.710] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.11645490889335663
[00:33:23.083] Server  ERROR   Backend returned 400
[00:33:23.099] Server  ERROR   Backend returned 400
[00:33:23.110] Server  ERROR   Backend returned 400
[00:33:23.214] Browser WARN    Inference API failed, using simulation 400
[00:33:23.214] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8754103823202981
[00:33:23.214] Browser WARN    Inference API failed, using simulation 400
[00:33:23.214] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.851633563588305
[00:33:23.214] Browser WARN    Inference API failed, using simulation 400
[00:33:23.214] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2632391984506627
[00:33:23.586] Server  ERROR   Backend returned 400
[00:33:23.618] Server  ERROR   Backend returned 400
[00:33:23.622] Server  ERROR   Backend returned 400
[00:33:23.731] Browser WARN    Inference API failed, using simulation 400
[00:33:23.731] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7773050382717835
[00:33:23.731] Browser WARN    Inference API failed, using simulation 400
[00:33:23.731] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.05127789539440797
[00:33:23.731] Browser WARN    Inference API failed, using simulation 400
[00:33:23.731] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.18338550814710003
[00:33:24.079] Server  ERROR   Backend returned 400
[00:33:24.105] Server  ERROR   Backend returned 400
[00:33:24.126] Server  ERROR   Backend returned 400
[00:33:24.232] Browser WARN    Inference API failed, using simulation 400
[00:33:24.233] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.36027709479144326
[00:33:24.233] Browser WARN    Inference API failed, using simulation 400
[00:33:24.233] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1392974164475091
[00:33:24.233] Browser WARN    Inference API failed, using simulation 400
[00:33:24.233] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.06764025422538422
[00:33:24.578] Server  ERROR   Backend returned 400
[00:33:24.596] Server  ERROR   Backend returned 400
[00:33:24.614] Server  ERROR   Backend returned 400
[00:33:24.719] Browser WARN    Inference API failed, using simulation 400
[00:33:24.719] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3752661977418719
[00:33:24.719] Browser WARN    Inference API failed, using simulation 400
[00:33:24.719] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.35208494988371103
[00:33:24.719] Browser WARN    Inference API failed, using simulation 400
[00:33:24.719] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.05152145445342071
[00:33:25.087] Server  ERROR   Backend returned 400
[00:33:25.106] Server  ERROR   Backend returned 400
[00:33:25.117] Server  ERROR   Backend returned 400
[00:33:25.220] Browser WARN    Inference API failed, using simulation 400
[00:33:25.220] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.48363617149798027
[00:33:25.220] Browser WARN    Inference API failed, using simulation 400
[00:33:25.220] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.41378863947910116
[00:33:25.220] Browser WARN    Inference API failed, using simulation 400
[00:33:25.220] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2904624234158759
[00:33:25.577] Server  ERROR   Backend returned 400
[00:33:25.610] Server  ERROR   Backend returned 400
[00:33:25.612] Server  ERROR   Backend returned 400
[00:33:25.721] Browser WARN    Inference API failed, using simulation 400
[00:33:25.721] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7796729724917093
[00:33:25.721] Browser WARN    Inference API failed, using simulation 400
[00:33:25.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1872035752985725
[00:33:25.721] Browser WARN    Inference API failed, using simulation 400
[00:33:25.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2814552216061657
[00:33:26.077] Server  ERROR   Backend returned 400
[00:33:26.106] Server  ERROR   Backend returned 400
[00:33:26.110] Server  ERROR   Backend returned 400
[00:33:26.218] Browser WARN    Inference API failed, using simulation 400
[00:33:26.218] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7306479187970981
[00:33:26.218] Browser WARN    Inference API failed, using simulation 400
[00:33:26.218] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.49816245963916733
[00:33:26.218] Browser WARN    Inference API failed, using simulation 400
[00:33:26.218] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.812541601057063
[00:33:26.577] Server  ERROR   Backend returned 400
[00:33:26.600] Server  ERROR   Backend returned 400
[00:33:26.611] Server  ERROR   Backend returned 400
[00:33:26.715] Browser WARN    Inference API failed, using simulation 400
[00:33:26.715] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.05848135625009471
[00:33:26.715] Browser WARN    Inference API failed, using simulation 400
[00:33:26.715] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9421106647808631
[00:33:26.715] Browser WARN    Inference API failed, using simulation 400
[00:33:26.715] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4877587117723236
[00:33:27.111] Server  ERROR   Backend returned 400
[00:33:27.171] Server  ERROR   Backend returned 400
[00:33:27.195] Server  ERROR   Backend returned 400
[00:33:27.300] Browser WARN    Inference API failed, using simulation 400
[00:33:27.300] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4647610721337404
[00:33:27.300] Browser WARN    Inference API failed, using simulation 400
[00:33:27.300] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.47274906931841976
[00:33:27.300] Browser WARN    Inference API failed, using simulation 400
[00:33:27.300] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7162206062901056
[00:33:27.584] Server  ERROR   Backend returned 400
[00:33:27.600] Server  ERROR   Backend returned 400
[00:33:27.619] Server  ERROR   Backend returned 400
[00:33:27.723] Browser WARN    Inference API failed, using simulation 400
[00:33:27.723] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8523563501398613
[00:33:27.723] Browser WARN    Inference API failed, using simulation 400
[00:33:27.723] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.43767355969711763
[00:33:27.723] Browser WARN    Inference API failed, using simulation 400
[00:33:27.723] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.08648921919121971
[00:33:28.078] Server  ERROR   Backend returned 400
[00:33:28.106] Server  ERROR   Backend returned 400
[00:33:28.111] Server  ERROR   Backend returned 400
[00:33:28.219] Browser WARN    Inference API failed, using simulation 400
[00:33:28.219] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8832711136216291
[00:33:28.219] Browser WARN    Inference API failed, using simulation 400
[00:33:28.219] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.39176485571528136
[00:33:28.219] Browser WARN    Inference API failed, using simulation 400
[00:33:28.219] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4991315861021561
[00:33:28.578] Server  ERROR   Backend returned 400
[00:33:28.602] Server  ERROR   Backend returned 400
[00:33:28.616] Server  ERROR   Backend returned 400
[00:33:28.719] Browser WARN    Inference API failed, using simulation 400
[00:33:28.719] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9848979451227703
[00:33:28.719] Browser WARN    Inference API failed, using simulation 400
[00:33:28.719] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.42397739888142993
[00:33:28.719] Browser WARN    Inference API failed, using simulation 400
[00:33:28.719] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4111891357675241
[00:33:29.077] Server  ERROR   Backend returned 400
[00:33:29.093] Server  ERROR   Backend returned 400
[00:33:29.104] Server  ERROR   Backend returned 400
[00:33:29.208] Browser WARN    Inference API failed, using simulation 400
[00:33:29.208] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.010961818208154839
[00:33:29.208] Browser WARN    Inference API failed, using simulation 400
[00:33:29.208] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7699383001372545
[00:33:29.208] Browser WARN    Inference API failed, using simulation 400
[00:33:29.208] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4904409427228446
[00:33:29.586] Server  ERROR   Backend returned 400
[00:33:29.612] Server  ERROR   Backend returned 400
[00:33:29.624] Server  ERROR   Backend returned 400
[00:33:29.728] Browser WARN    Inference API failed, using simulation 400
[00:33:29.728] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7901586341966794
[00:33:29.728] Browser WARN    Inference API failed, using simulation 400
[00:33:29.728] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.861458323838075
[00:33:29.728] Browser WARN    Inference API failed, using simulation 400
[00:33:29.728] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.05261904945321383
[00:33:30.089] Server  ERROR   Backend returned 400
[00:33:30.104] Server  ERROR   Backend returned 400
[00:33:30.124] Server  ERROR   Backend returned 400
[00:33:30.228] Browser WARN    Inference API failed, using simulation 400
[00:33:30.228] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.39318162067773277
[00:33:30.228] Browser WARN    Inference API failed, using simulation 400
[00:33:30.228] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8784862107196255
[00:33:30.228] Browser WARN    Inference API failed, using simulation 400
[00:33:30.228] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.48594948805050975
[00:33:30.576] Server  ERROR   Backend returned 400
[00:33:30.595] Server  ERROR   Backend returned 400
[00:33:30.607] Server  ERROR   Backend returned 400
[00:33:30.712] Browser WARN    Inference API failed, using simulation 400
[00:33:30.712] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7071346277581109
[00:33:30.712] Browser WARN    Inference API failed, using simulation 400
[00:33:30.712] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7284788482964556
[00:33:30.712] Browser WARN    Inference API failed, using simulation 400
[00:33:30.712] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2256645491351421
[00:33:31.086] Server  ERROR   Backend returned 400
[00:33:31.110] Server  ERROR   Backend returned 400
[00:33:31.124] Server  ERROR   Backend returned 400
[00:33:31.227] Browser WARN    Inference API failed, using simulation 400
[00:33:31.227] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2544352420803199
[00:33:31.227] Browser WARN    Inference API failed, using simulation 400
[00:33:31.227] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.34797468653600966
[00:33:31.227] Browser WARN    Inference API failed, using simulation 400
[00:33:31.227] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8544273208511429
[00:33:31.577] Server  ERROR   Backend returned 400
[00:33:31.608] Server  ERROR   Backend returned 400
[00:33:31.610] Server  ERROR   Backend returned 400
[00:33:31.719] Browser WARN    Inference API failed, using simulation 400
[00:33:31.719] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.14215039541368524
[00:33:31.719] Browser WARN    Inference API failed, using simulation 400
[00:33:31.719] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.18903871241004816
[00:33:31.719] Browser WARN    Inference API failed, using simulation 400
[00:33:31.719] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.27774894302653036
[00:33:32.077] Server  ERROR   Backend returned 400
[00:33:32.094] Server  ERROR   Backend returned 400
[00:33:32.114] Server  ERROR   Backend returned 400
[00:33:32.220] Browser WARN    Inference API failed, using simulation 400
[00:33:32.220] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7598388057340558
[00:33:32.220] Browser WARN    Inference API failed, using simulation 400
[00:33:32.220] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7579185817627796
[00:33:32.220] Browser WARN    Inference API failed, using simulation 400
[00:33:32.220] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.776213878060564
[00:33:32.580] Server  ERROR   Backend returned 400
[00:33:32.609] Server  ERROR   Backend returned 400
[00:33:32.622] Server  ERROR   Backend returned 400
[00:33:32.725] Browser WARN    Inference API failed, using simulation 400
[00:33:32.725] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.27187477684940403
[00:33:32.725] Browser WARN    Inference API failed, using simulation 400
[00:33:32.725] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.19269371569321858
[00:33:32.725] Browser WARN    Inference API failed, using simulation 400
[00:33:32.725] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.40694298199327844
[00:33:33.095] Server  ERROR   Backend returned 400
[00:33:33.132] Server  ERROR   Backend returned 400
[00:33:33.154] Server  ERROR   Backend returned 400
[00:33:33.259] Browser WARN    Inference API failed, using simulation 400
[00:33:33.259] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3229072112044849
[00:33:33.259] Browser WARN    Inference API failed, using simulation 400
[00:33:33.259] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.23742424166530962
[00:33:33.259] Browser WARN    Inference API failed, using simulation 400
[00:33:33.259] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.434540391680149
[00:33:33.583] Server  ERROR   Backend returned 400
[00:33:33.604] Server  ERROR   Backend returned 400
[00:33:33.617] Server  ERROR   Backend returned 400
[00:33:33.721] Browser WARN    Inference API failed, using simulation 400
[00:33:33.721] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8454010437661444
[00:33:33.721] Browser WARN    Inference API failed, using simulation 400
[00:33:33.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.44438129404650556
[00:33:33.721] Browser WARN    Inference API failed, using simulation 400
[00:33:33.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.09356257802514711
[00:33:34.080] Server  ERROR   Backend returned 400
[00:33:34.102] Server  ERROR   Backend returned 400
[00:33:34.116] Server  ERROR   Backend returned 400
[00:33:34.220] Browser WARN    Inference API failed, using simulation 400
[00:33:34.220] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.0902456323607177
[00:33:34.220] Browser WARN    Inference API failed, using simulation 400
[00:33:34.220] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4004602919385817
[00:33:34.220] Browser WARN    Inference API failed, using simulation 400
[00:33:34.220] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8412293193898241
[00:33:34.596] Server  ERROR   Backend returned 400
[00:33:34.611] Server  ERROR   Backend returned 400
[00:33:34.633] Server  ERROR   Backend returned 400
[00:33:34.736] Browser WARN    Inference API failed, using simulation 400
[00:33:34.736] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8040319271109102
[00:33:34.736] Browser WARN    Inference API failed, using simulation 400
[00:33:34.736] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.31696222864651313
[00:33:34.736] Browser WARN    Inference API failed, using simulation 400
[00:33:34.736] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.86387287335604
[00:33:35.078] Server  ERROR   Backend returned 400
[00:33:35.094] Server  ERROR   Backend returned 400
[00:33:35.105] Server  ERROR   Backend returned 400
[00:33:35.210] Browser WARN    Inference API failed, using simulation 400
[00:33:35.210] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.32780886549113747
[00:33:35.210] Browser WARN    Inference API failed, using simulation 400
[00:33:35.210] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.03507576061544765
[00:33:35.210] Browser WARN    Inference API failed, using simulation 400
[00:33:35.210] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9272337105486593
[00:33:35.575] Server  ERROR   Backend returned 400
[00:33:35.593] Server  ERROR   Backend returned 400
[00:33:35.605] Server  ERROR   Backend returned 400
[00:33:35.709] Browser WARN    Inference API failed, using simulation 400
[00:33:35.709] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.286000487985667
[00:33:35.709] Browser WARN    Inference API failed, using simulation 400
[00:33:35.709] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.35443898601372725
[00:33:35.709] Browser WARN    Inference API failed, using simulation 400
[00:33:35.709] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.11711295840813274
[00:33:36.084] Server  ERROR   Backend returned 400
[00:33:36.102] Server  ERROR   Backend returned 400
[00:33:36.114] Server  ERROR   Backend returned 400
[00:33:36.219] Browser WARN    Inference API failed, using simulation 400
[00:33:36.219] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.33388910754533996
[00:33:36.219] Browser WARN    Inference API failed, using simulation 400
[00:33:36.219] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.44701250138682463
[00:33:36.219] Browser WARN    Inference API failed, using simulation 400
[00:33:36.219] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2858471991258121
[00:33:36.581] Server  ERROR   Backend returned 400
[00:33:36.598] Server  ERROR   Backend returned 400
[00:33:36.616] Server  ERROR   Backend returned 400
[00:33:36.720] Browser WARN    Inference API failed, using simulation 400
[00:33:36.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.15958419024821746
[00:33:36.720] Browser WARN    Inference API failed, using simulation 400
[00:33:36.720] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2832685274059728
[00:33:36.720] Browser WARN    Inference API failed, using simulation 400
[00:33:36.720] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9020628645201174
[00:33:37.080] Server  ERROR   Backend returned 400
[00:33:37.105] Server  ERROR   Backend returned 400
[00:33:37.110] Server  ERROR   Backend returned 400
[00:33:37.218] Browser WARN    Inference API failed, using simulation 400
[00:33:37.218] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9347555522332527
[00:33:37.218] Browser WARN    Inference API failed, using simulation 400
[00:33:37.218] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.949456897181532
[00:33:37.218] Browser WARN    Inference API failed, using simulation 400
[00:33:37.218] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.053386800841943916
[00:33:37.582] Server  ERROR   Backend returned 400
[00:33:37.599] Server  ERROR   Backend returned 400
[00:33:37.613] Server  ERROR   Backend returned 400
[00:33:37.718] Browser WARN    Inference API failed, using simulation 400
[00:33:37.718] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3231866676701374
[00:33:37.718] Browser WARN    Inference API failed, using simulation 400
[00:33:37.718] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.05330693290784144
[00:33:37.718] Browser WARN    Inference API failed, using simulation 400
[00:33:37.718] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9970768665014906
[00:33:38.081] Server  ERROR   Backend returned 400
[00:33:38.102] Server  ERROR   Backend returned 400
[00:33:38.116] Server  ERROR   Backend returned 400
[00:33:38.220] Browser WARN    Inference API failed, using simulation 400
[00:33:38.220] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.49097579357144233
[00:33:38.220] Browser WARN    Inference API failed, using simulation 400
[00:33:38.220] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2639502761027635
[00:33:38.220] Browser WARN    Inference API failed, using simulation 400
[00:33:38.220] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9451239863443899
[00:33:38.588] Server  ERROR   Backend returned 400
[00:33:38.612] Server  ERROR   Backend returned 400
[00:33:38.631] Server  ERROR   Backend returned 400
[00:33:38.736] Browser WARN    Inference API failed, using simulation 400
[00:33:38.736] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.28586670520678714
[00:33:38.736] Browser WARN    Inference API failed, using simulation 400
[00:33:38.736] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1305234925468437
[00:33:38.736] Browser WARN    Inference API failed, using simulation 400
[00:33:38.736] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2713426541996988
[00:33:39.079] Server  ERROR   Backend returned 400
[00:33:39.100] Server  ERROR   Backend returned 400
[00:33:39.111] Server  ERROR   Backend returned 400
[00:33:39.219] Browser WARN    Inference API failed, using simulation 400
[00:33:39.219] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.07684092449489655
[00:33:39.219] Browser WARN    Inference API failed, using simulation 400
[00:33:39.219] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.40147835527607734
[00:33:39.219] Browser WARN    Inference API failed, using simulation 400
[00:33:39.219] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3347082239955487
[00:33:39.587] Server  ERROR   Backend returned 400
[00:33:39.603] Server  ERROR   Backend returned 400
[00:33:39.615] Server  ERROR   Backend returned 400
[00:33:39.720] Browser WARN    Inference API failed, using simulation 400
[00:33:39.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.25781454398269976
[00:33:39.720] Browser WARN    Inference API failed, using simulation 400
[00:33:39.720] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.43259882385049236
[00:33:39.720] Browser WARN    Inference API failed, using simulation 400
[00:33:39.720] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7143042364531718
[00:33:40.082] Server  ERROR   Backend returned 400
[00:33:40.099] Server  ERROR   Backend returned 400
[00:33:40.112] Server  ERROR   Backend returned 400
[00:33:40.218] Browser WARN    Inference API failed, using simulation 400
[00:33:40.218] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4704516745005822
[00:33:40.218] Browser WARN    Inference API failed, using simulation 400
[00:33:40.218] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9489210358488982
[00:33:40.218] Browser WARN    Inference API failed, using simulation 400
[00:33:40.218] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.42811096223871514
[00:33:40.585] Server  ERROR   Backend returned 400
[00:33:40.605] Server  ERROR   Backend returned 400
[00:33:40.617] Server  ERROR   Backend returned 400
[00:33:40.727] Browser WARN    Inference API failed, using simulation 400
[00:33:40.727] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8263346510811755
[00:33:40.727] Browser WARN    Inference API failed, using simulation 400
[00:33:40.727] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.025628195609213478
[00:33:40.727] Browser WARN    Inference API failed, using simulation 400
[00:33:40.727] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.08482145806323649
[00:33:41.079] Server  ERROR   Backend returned 400
[00:33:41.099] Server  ERROR   Backend returned 400
[00:33:41.112] Server  ERROR   Backend returned 400
[00:33:41.218] Browser WARN    Inference API failed, using simulation 400
[00:33:41.218] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.027490046252483746
[00:33:41.218] Browser WARN    Inference API failed, using simulation 400
[00:33:41.218] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1144059249348307
[00:33:41.218] Browser WARN    Inference API failed, using simulation 400
[00:33:41.218] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.40849054089874653
[00:33:41.593] Server  ERROR   Backend returned 400
[00:33:41.608] Server  ERROR   Backend returned 400
[00:33:41.639] Server  ERROR   Backend returned 400
[00:33:41.742] Browser WARN    Inference API failed, using simulation 400
[00:33:41.742] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.15394147309970563
[00:33:41.742] Browser WARN    Inference API failed, using simulation 400
[00:33:41.742] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2640732108989485
[00:33:41.742] Browser WARN    Inference API failed, using simulation 400
[00:33:41.742] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.11214460573851376
[00:33:42.082] Server  ERROR   Backend returned 400
[00:33:42.105] Server  ERROR   Backend returned 400
[00:33:42.111] Server  ERROR   Backend returned 400
[00:33:42.216] Browser WARN    Inference API failed, using simulation 400
[00:33:42.216] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8102055066533429
[00:33:42.216] Browser WARN    Inference API failed, using simulation 400
[00:33:42.216] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8857897551531382
[00:33:42.216] Browser WARN    Inference API failed, using simulation 400
[00:33:42.216] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.39706907284492093
[00:33:42.581] Server  ERROR   Backend returned 400
[00:33:42.601] Server  ERROR   Backend returned 400
[00:33:42.620] Server  ERROR   Backend returned 400
[00:33:42.724] Browser WARN    Inference API failed, using simulation 400
[00:33:42.724] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.36956917137377104
[00:33:42.724] Browser WARN    Inference API failed, using simulation 400
[00:33:42.724] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4718947749415276
[00:33:42.724] Browser WARN    Inference API failed, using simulation 400
[00:33:42.724] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.37093778673342903
[00:33:43.081] Server  ERROR   Backend returned 400
[00:33:43.109] Server  ERROR   Backend returned 400
[00:33:43.112] Server  ERROR   Backend returned 400
[00:33:43.221] Browser WARN    Inference API failed, using simulation 400
[00:33:43.221] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8338569726517903
[00:33:43.221] Browser WARN    Inference API failed, using simulation 400
[00:33:43.221] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.11421719337393965
[00:33:43.221] Browser WARN    Inference API failed, using simulation 400
[00:33:43.221] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8238533692501204
[00:33:43.588] Server  ERROR   Backend returned 400
[00:33:43.609] Server  ERROR   Backend returned 400
[00:33:43.646] Server  ERROR   Backend returned 400
[00:33:43.749] Browser WARN    Inference API failed, using simulation 400
[00:33:43.749] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7294281275782539
[00:33:43.749] Browser WARN    Inference API failed, using simulation 400
[00:33:43.749] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.28963382069044435
[00:33:43.749] Browser WARN    Inference API failed, using simulation 400
[00:33:43.750] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.02641008128852712
[00:33:44.078] Server  ERROR   Backend returned 400
[00:33:44.094] Server  ERROR   Backend returned 400
[00:33:44.103] Server  ERROR   Backend returned 400
[00:33:44.208] Browser WARN    Inference API failed, using simulation 400
[00:33:44.208] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.37784781168857207
[00:33:44.208] Browser WARN    Inference API failed, using simulation 400
[00:33:44.208] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.18622560139640693
[00:33:44.208] Browser WARN    Inference API failed, using simulation 400
[00:33:44.208] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.39728832299481137
[00:33:44.577] Server  ERROR   Backend returned 400
[00:33:44.599] Server  ERROR   Backend returned 400
[00:33:44.611] Server  ERROR   Backend returned 400
[00:33:44.715] Browser WARN    Inference API failed, using simulation 400
[00:33:44.715] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9243545233962583
[00:33:44.715] Browser WARN    Inference API failed, using simulation 400
[00:33:44.715] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.32710770662144734
[00:33:44.715] Browser WARN    Inference API failed, using simulation 400
[00:33:44.715] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.328219490591295
[00:33:45.076] Server  ERROR   Backend returned 400
[00:33:45.098] Server  ERROR   Backend returned 400
[00:33:45.111] Server  ERROR   Backend returned 400
[00:33:45.215] Browser WARN    Inference API failed, using simulation 400
[00:33:45.215] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7843404003898863
[00:33:45.215] Browser WARN    Inference API failed, using simulation 400
[00:33:45.215] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9964758920113235
[00:33:45.215] Browser WARN    Inference API failed, using simulation 400
[00:33:45.215] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2532098751055758
[00:33:45.579] Server  ERROR   Backend returned 400
[00:33:45.596] Server  ERROR   Backend returned 400
[00:33:45.608] Server  ERROR   Backend returned 400
[00:33:45.711] Browser WARN    Inference API failed, using simulation 400
[00:33:45.711] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.20529010535850967
[00:33:45.711] Browser WARN    Inference API failed, using simulation 400
[00:33:45.711] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4710544853674494
[00:33:45.711] Browser WARN    Inference API failed, using simulation 400
[00:33:45.711] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7550895390783292
[00:33:46.082] Server  ERROR   Backend returned 400
[00:33:46.100] Server  ERROR   Backend returned 400
[00:33:46.113] Server  ERROR   Backend returned 400
[00:33:46.218] Browser WARN    Inference API failed, using simulation 400
[00:33:46.218] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8907819121019596
[00:33:46.218] Browser WARN    Inference API failed, using simulation 400
[00:33:46.218] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8617256168667597
[00:33:46.218] Browser WARN    Inference API failed, using simulation 400
[00:33:46.218] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8804159622600121
[00:33:46.576] Server  ERROR   Backend returned 400
[00:33:46.597] Server  ERROR   Backend returned 400
[00:33:46.611] Server  ERROR   Backend returned 400
[00:33:46.716] Browser WARN    Inference API failed, using simulation 400
[00:33:46.716] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.16390586756688663
[00:33:46.716] Browser WARN    Inference API failed, using simulation 400
[00:33:46.716] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7650570266779455
[00:33:46.716] Browser WARN    Inference API failed, using simulation 400
[00:33:46.716] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.12016624290773492
[00:33:47.085] Server  ERROR   Backend returned 400
[00:33:47.113] Server  ERROR   Backend returned 400
[00:33:47.129] Server  ERROR   Backend returned 400
[00:33:47.232] Browser WARN    Inference API failed, using simulation 400
[00:33:47.232] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3957962840998836
[00:33:47.232] Browser WARN    Inference API failed, using simulation 400
[00:33:47.232] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.07944855536298445
[00:33:47.232] Browser WARN    Inference API failed, using simulation 400
[00:33:47.232] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.46239511683533463
[00:33:47.584] Server  ERROR   Backend returned 400
[00:33:47.600] Server  ERROR   Backend returned 400
[00:33:47.617] Server  ERROR   Backend returned 400
[00:33:47.733] Browser WARN    Inference API failed, using simulation 400
[00:33:47.733] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8871734496109593
[00:33:47.733] Browser WARN    Inference API failed, using simulation 400
[00:33:47.733] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3595316002295718
[00:33:47.733] Browser WARN    Inference API failed, using simulation 400
[00:33:47.733] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.21521117837940024
[00:33:48.081] Server  ERROR   Backend returned 400
[00:33:48.105] Server  ERROR   Backend returned 400
[00:33:48.113] Server  ERROR   Backend returned 400
[00:33:48.220] Browser WARN    Inference API failed, using simulation 400
[00:33:48.220] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.44282068080797105
[00:33:48.220] Browser WARN    Inference API failed, using simulation 400
[00:33:48.220] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8387607656106357
[00:33:48.220] Browser WARN    Inference API failed, using simulation 400
[00:33:48.220] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.08443208575517602
[00:33:48.583] Server  ERROR   Backend returned 400
[00:33:48.605] Server  ERROR   Backend returned 400
[00:33:48.618] Server  ERROR   Backend returned 400
[00:33:48.722] Browser WARN    Inference API failed, using simulation 400
[00:33:48.722] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.01807141147718666
[00:33:48.722] Browser WARN    Inference API failed, using simulation 400
[00:33:48.722] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4450696636530559
[00:33:48.722] Browser WARN    Inference API failed, using simulation 400
[00:33:48.722] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1801205210595081
[00:33:49.094] Server  ERROR   Backend returned 400
[00:33:49.114] Server  ERROR   Backend returned 400
[00:33:49.134] Server  ERROR   Backend returned 400
[00:33:49.248] Browser WARN    Inference API failed, using simulation 400
[00:33:49.248] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2612610951314806
[00:33:49.248] Browser WARN    Inference API failed, using simulation 400
[00:33:49.248] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.40584272750619244
[00:33:49.248] Browser WARN    Inference API failed, using simulation 400
[00:33:49.248] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3533327140699202
[00:33:49.586] Server  ERROR   Backend returned 400
[00:33:49.602] Server  ERROR   Backend returned 400
[00:33:49.613] Server  ERROR   Backend returned 400
[00:33:49.717] Browser WARN    Inference API failed, using simulation 400
[00:33:49.717] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.22239961762814853
[00:33:49.717] Browser WARN    Inference API failed, using simulation 400
[00:33:49.717] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9687937222491869
[00:33:49.717] Browser WARN    Inference API failed, using simulation 400
[00:33:49.717] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9317667077937207
[00:33:50.096] Server  ERROR   Backend returned 400
[00:33:50.109] Server  ERROR   Backend returned 400
[00:33:50.131] Server  ERROR   Backend returned 400
[00:33:50.242] Browser WARN    Inference API failed, using simulation 400
[00:33:50.242] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.25347847322415556
[00:33:50.242] Browser WARN    Inference API failed, using simulation 400
[00:33:50.242] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.24591525286089372
[00:33:50.242] Browser WARN    Inference API failed, using simulation 400
[00:33:50.242] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1335625159327734
[00:33:50.586] Server  ERROR   Backend returned 400
[00:33:50.610] Server  ERROR   Backend returned 400
[00:33:50.612] Server  ERROR   Backend returned 400
[00:33:50.725] Browser WARN    Inference API failed, using simulation 400
[00:33:50.725] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9517479698587552
[00:33:50.725] Browser WARN    Inference API failed, using simulation 400
[00:33:50.725] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3910158615160426
[00:33:50.725] Browser WARN    Inference API failed, using simulation 400
[00:33:50.725] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.37678787848279516
[00:33:51.095] Server  ERROR   Backend returned 400
[00:33:51.111] Server  ERROR   Backend returned 400
[00:33:51.124] Server  ERROR   Backend returned 400
[00:33:51.228] Browser WARN    Inference API failed, using simulation 400
[00:33:51.228] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3084697649653173
[00:33:51.228] Browser WARN    Inference API failed, using simulation 400
[00:33:51.228] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9586416274047455
[00:33:51.228] Browser WARN    Inference API failed, using simulation 400
[00:33:51.228] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.48713446330863663
[00:33:51.609] Server  ERROR   Backend returned 400
[00:33:51.631] Server  ERROR   Backend returned 400
[00:33:51.634] Server  ERROR   Backend returned 400
[00:33:51.746] Browser WARN    Inference API failed, using simulation 400
[00:33:51.746] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4657511806297333
[00:33:51.746] Browser WARN    Inference API failed, using simulation 400
[00:33:51.747] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3687053692794637
[00:33:51.747] Browser WARN    Inference API failed, using simulation 400
[00:33:51.747] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.0034294567820631894
[00:33:52.075] Server  ERROR   Backend returned 400
[00:33:52.097] Server  ERROR   Backend returned 400
[00:33:52.110] Server  ERROR   Backend returned 400
[00:33:52.215] Browser WARN    Inference API failed, using simulation 400
[00:33:52.215] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.40276820307907757
[00:33:52.215] Browser WARN    Inference API failed, using simulation 400
[00:33:52.215] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.25314454576559636
[00:33:52.215] Browser WARN    Inference API failed, using simulation 400
[00:33:52.215] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.004396220881590529
[00:33:52.575] Server  ERROR   Backend returned 400
[00:33:52.592] Server  ERROR   Backend returned 400
[00:33:52.610] Server  ERROR   Backend returned 400
[00:33:52.718] Browser WARN    Inference API failed, using simulation 400
[00:33:52.718] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.40574698215760824
[00:33:52.718] Browser WARN    Inference API failed, using simulation 400
[00:33:52.718] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.42147827001163024
[00:33:52.718] Browser WARN    Inference API failed, using simulation 400
[00:33:52.718] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1859165987137742
[00:33:53.077] Server  ERROR   Backend returned 400
[00:33:53.113] Server  ERROR   Backend returned 400
[00:33:53.116] Server  ERROR   Backend returned 400
[00:33:53.226] Browser WARN    Inference API failed, using simulation 400
[00:33:53.226] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.14742556706326454
[00:33:53.226] Browser WARN    Inference API failed, using simulation 400
[00:33:53.226] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2260449197525794
[00:33:53.226] Browser WARN    Inference API failed, using simulation 400
[00:33:53.226] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.12583971198661792
[00:33:53.577] Server  ERROR   Backend returned 400
[00:33:53.601] Server  ERROR   Backend returned 400
[00:33:53.614] Server  ERROR   Backend returned 400
[00:33:53.718] Browser WARN    Inference API failed, using simulation 400
[00:33:53.718] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.13511629872747583
[00:33:53.718] Browser WARN    Inference API failed, using simulation 400
[00:33:53.718] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4323419663474232
[00:33:53.718] Browser WARN    Inference API failed, using simulation 400
[00:33:53.718] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8901176397982314
[00:33:54.080] Server  ERROR   Backend returned 400
[00:33:54.100] Server  ERROR   Backend returned 400
[00:33:54.110] Server  ERROR   Backend returned 400
[00:33:54.214] Browser WARN    Inference API failed, using simulation 400
[00:33:54.214] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.09541332230067268
[00:33:54.214] Browser WARN    Inference API failed, using simulation 400
[00:33:54.214] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7706681402628066
[00:33:54.214] Browser WARN    Inference API failed, using simulation 400
[00:33:54.214] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9871801645480965
[00:33:54.575] Server  ERROR   Backend returned 400
[00:33:54.601] Server  ERROR   Backend returned 400
[00:33:54.603] Server  ERROR   Backend returned 400
[00:33:54.711] Browser WARN    Inference API failed, using simulation 400
[00:33:54.711] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7898932038629722
[00:33:54.711] Browser WARN    Inference API failed, using simulation 400
[00:33:54.711] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.18109436080690972
[00:33:54.711] Browser WARN    Inference API failed, using simulation 400
[00:33:54.711] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.006942091610163681
[00:33:55.079] Server  ERROR   Backend returned 400
[00:33:55.099] Server  ERROR   Backend returned 400
[00:33:55.117] Server  ERROR   Backend returned 400
[00:33:55.220] Browser WARN    Inference API failed, using simulation 400
[00:33:55.220] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.33360768800441803
[00:33:55.220] Browser WARN    Inference API failed, using simulation 400
[00:33:55.220] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.11971088133827024
[00:33:55.220] Browser WARN    Inference API failed, using simulation 400
[00:33:55.220] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4421062027598648
[00:33:55.583] Server  ERROR   Backend returned 400
[00:33:55.601] Server  ERROR   Backend returned 400
[00:33:55.614] Server  ERROR   Backend returned 400
[00:33:55.717] Browser WARN    Inference API failed, using simulation 400
[00:33:55.717] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3168646111296102
[00:33:55.717] Browser WARN    Inference API failed, using simulation 400
[00:33:55.717] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.12882592681810362
[00:33:55.717] Browser WARN    Inference API failed, using simulation 400
[00:33:55.717] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3432786152898917
[00:33:56.078] Server  ERROR   Backend returned 400
[00:33:56.095] Server  ERROR   Backend returned 400
[00:33:56.113] Server  ERROR   Backend returned 400
[00:33:56.218] Browser WARN    Inference API failed, using simulation 400
[00:33:56.218] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1236024816339018
[00:33:56.218] Browser WARN    Inference API failed, using simulation 400
[00:33:56.218] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7639619979622534
[00:33:56.218] Browser WARN    Inference API failed, using simulation 400
[00:33:56.218] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.40126994783452163
[00:33:56.577] Server  ERROR   Backend returned 400
[00:33:56.599] Server  ERROR   Backend returned 400
[00:33:56.615] Server  ERROR   Backend returned 400
[00:33:56.729] Browser WARN    Inference API failed, using simulation 400
[00:33:56.729] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.33384516986134455
[00:33:56.729] Browser WARN    Inference API failed, using simulation 400
[00:33:56.729] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.04843320422545405
[00:33:56.729] Browser WARN    Inference API failed, using simulation 400
[00:33:56.729] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.12693325123592347
[00:33:57.086] Server  ERROR   Backend returned 400
[00:33:57.102] Server  ERROR   Backend returned 400
[00:33:57.120] Server  ERROR   Backend returned 400
[00:33:57.233] Browser WARN    Inference API failed, using simulation 400
[00:33:57.233] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4708567277275119
[00:33:57.233] Browser WARN    Inference API failed, using simulation 400
[00:33:57.233] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.31017156104371724
[00:33:57.233] Browser WARN    Inference API failed, using simulation 400
[00:33:57.233] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.30579716916341554
[00:33:57.577] Server  ERROR   Backend returned 400
[00:33:57.593] Server  ERROR   Backend returned 400
[00:33:57.611] Server  ERROR   Backend returned 400
[00:33:57.716] Browser WARN    Inference API failed, using simulation 400
[00:33:57.716] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8006609885151794
[00:33:57.716] Browser WARN    Inference API failed, using simulation 400
[00:33:57.716] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.11980778542094095
[00:33:57.716] Browser WARN    Inference API failed, using simulation 400
[00:33:57.716] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.31224533185436204
[00:33:58.077] Server  ERROR   Backend returned 400
[00:33:58.098] Server  ERROR   Backend returned 400
[00:33:58.119] Server  ERROR   Backend returned 400
[00:33:58.223] Browser WARN    Inference API failed, using simulation 400
[00:33:58.223] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.16072197119954013
[00:33:58.223] Browser WARN    Inference API failed, using simulation 400
[00:33:58.223] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2613501689023496
[00:33:58.223] Browser WARN    Inference API failed, using simulation 400
[00:33:58.223] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.27373608445291003
[00:33:58.577] Server  ERROR   Backend returned 400
[00:33:58.611] Server  ERROR   Backend returned 400
[00:33:58.612] Server  ERROR   Backend returned 400
[00:33:58.733] Browser WARN    Inference API failed, using simulation 400
[00:33:58.733] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.04365021814511094
[00:33:58.733] Browser WARN    Inference API failed, using simulation 400
[00:33:58.733] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.18997661512961783
[00:33:58.733] Browser WARN    Inference API failed, using simulation 400
[00:33:58.733] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8481318323432244
[00:33:59.088] Server  ERROR   Backend returned 400
[00:33:59.119] Server  ERROR   Backend returned 400
[00:33:59.122] Server  ERROR   Backend returned 400
[00:33:59.231] Browser WARN    Inference API failed, using simulation 400
[00:33:59.231] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8410876977411621
[00:33:59.231] Browser WARN    Inference API failed, using simulation 400
[00:33:59.231] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9886903555645851
[00:33:59.231] Browser WARN    Inference API failed, using simulation 400
[00:33:59.231] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.37494895332118194
[00:33:59.578] Server  ERROR   Backend returned 400
[00:33:59.597] Server  ERROR   Backend returned 400
[00:33:59.610] Server  ERROR   Backend returned 400
[00:33:59.715] Browser WARN    Inference API failed, using simulation 400
[00:33:59.715] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.13492094196915566
[00:33:59.715] Browser WARN    Inference API failed, using simulation 400
[00:33:59.715] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4070022365286882
[00:33:59.715] Browser WARN    Inference API failed, using simulation 400
[00:33:59.715] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8852788386296202
[00:34:00.082] Server  ERROR   Backend returned 400
[00:34:00.099] Server  ERROR   Backend returned 400
[00:34:00.113] Server  ERROR   Backend returned 400
[00:34:00.217] Browser WARN    Inference API failed, using simulation 400
[00:34:00.217] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3180154308109934
[00:34:00.217] Browser WARN    Inference API failed, using simulation 400
[00:34:00.217] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2583434299864494
[00:34:00.217] Browser WARN    Inference API failed, using simulation 400
[00:34:00.217] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7075652473387382
[00:34:00.579] Server  ERROR   Backend returned 400
[00:34:00.598] Server  ERROR   Backend returned 400
[00:34:00.611] Server  ERROR   Backend returned 400
[00:34:00.714] Browser WARN    Inference API failed, using simulation 400
[00:34:00.714] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.46147462314786825
[00:34:00.714] Browser WARN    Inference API failed, using simulation 400
[00:34:00.714] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.480675876665293
[00:34:00.714] Browser WARN    Inference API failed, using simulation 400
[00:34:00.714] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7772079789013828
[00:34:01.084] Server  ERROR   Backend returned 400
[00:34:01.099] Server  ERROR   Backend returned 400
[00:34:01.116] Server  ERROR   Backend returned 400
[00:34:01.219] Browser WARN    Inference API failed, using simulation 400
[00:34:01.219] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8215070795038416
[00:34:01.219] Browser WARN    Inference API failed, using simulation 400
[00:34:01.219] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.22390906148519885
[00:34:01.219] Browser WARN    Inference API failed, using simulation 400
[00:34:01.219] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4686572966091636
[00:34:01.582] Server  ERROR   Backend returned 400
[00:34:01.611] Server  ERROR   Backend returned 400
[00:34:01.613] Server  ERROR   Backend returned 400
[00:34:01.735] Browser WARN    Inference API failed, using simulation 400
[00:34:01.735] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.19232602607423815
[00:34:01.735] Browser WARN    Inference API failed, using simulation 400
[00:34:01.735] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.26309400415970763
[00:34:01.735] Browser WARN    Inference API failed, using simulation 400
[00:34:01.735] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.14716320500427893
[00:34:02.081] Server  ERROR   Backend returned 400
[00:34:02.104] Server  ERROR   Backend returned 400
[00:34:02.135] Server  ERROR   Backend returned 400
[00:34:02.239] Browser WARN    Inference API failed, using simulation 400
[00:34:02.239] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8868257532819017
[00:34:02.239] Browser WARN    Inference API failed, using simulation 400
[00:34:02.239] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3792656280660678
[00:34:02.239] Browser WARN    Inference API failed, using simulation 400
[00:34:02.239] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4967825846072269
[00:34:02.581] Server  ERROR   Backend returned 400
[00:34:02.606] Server  ERROR   Backend returned 400
[00:34:02.609] Server  ERROR   Backend returned 400
[00:34:02.719] Browser WARN    Inference API failed, using simulation 400
[00:34:02.719] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.43541501615056827
[00:34:02.719] Browser WARN    Inference API failed, using simulation 400
[00:34:02.719] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4942291473445254
[00:34:02.719] Browser WARN    Inference API failed, using simulation 400
[00:34:02.719] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1778535643128002
[00:34:03.077] Server  ERROR   Backend returned 400
[00:34:03.110] Server  ERROR   Backend returned 400
[00:34:03.111] Server  ERROR   Backend returned 400
[00:34:03.221] Browser WARN    Inference API failed, using simulation 400
[00:34:03.221] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9527042140092222
[00:34:03.221] Browser WARN    Inference API failed, using simulation 400
[00:34:03.221] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7163314518824436
[00:34:03.221] Browser WARN    Inference API failed, using simulation 400
[00:34:03.221] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9050355920004187
[00:34:03.577] Server  ERROR   Backend returned 400
[00:34:03.629] Server  ERROR   Backend returned 400
[00:34:03.633] Server  ERROR   Backend returned 400
[00:34:03.744] Browser WARN    Inference API failed, using simulation 400
[00:34:03.744] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.19165474850785047
[00:34:03.744] Browser WARN    Inference API failed, using simulation 400
[00:34:03.744] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.23769176030720512
[00:34:03.744] Browser WARN    Inference API failed, using simulation 400
[00:34:03.744] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.379920245928583
[00:34:04.078] Server  ERROR   Backend returned 400
[00:34:04.096] Server  ERROR   Backend returned 400
[00:34:04.114] Server  ERROR   Backend returned 400
[00:34:04.221] Browser WARN    Inference API failed, using simulation 400
[00:34:04.221] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.004712925057007766
[00:34:04.221] Browser WARN    Inference API failed, using simulation 400
[00:34:04.221] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9558579384697576
[00:34:04.221] Browser WARN    Inference API failed, using simulation 400
[00:34:04.221] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8133096733935125
[00:34:04.582] Server  ERROR   Backend returned 400
[00:34:04.607] Server  ERROR   Backend returned 400
[00:34:04.610] Server  ERROR   Backend returned 400
[00:34:04.718] Browser WARN    Inference API failed, using simulation 400
[00:34:04.718] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1359702031075189
[00:34:04.718] Browser WARN    Inference API failed, using simulation 400
[00:34:04.718] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1959184985289762
[00:34:04.718] Browser WARN    Inference API failed, using simulation 400
[00:34:04.718] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7630528126611075
[00:34:05.084] Server  ERROR   Backend returned 400
[00:34:05.110] Server  ERROR   Backend returned 400
[00:34:05.113] Server  ERROR   Backend returned 400
[00:34:05.226] Browser WARN    Inference API failed, using simulation 400
[00:34:05.226] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8494572189386578
[00:34:05.226] Browser WARN    Inference API failed, using simulation 400
[00:34:05.226] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8670478846866276
[00:34:05.226] Browser WARN    Inference API failed, using simulation 400
[00:34:05.226] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9954518862577495
[00:34:05.576] Server  ERROR   Backend returned 400
[00:34:05.609] Server  ERROR   Backend returned 400
[00:34:05.612] Server  ERROR   Backend returned 400
[00:34:05.734] Browser WARN    Inference API failed, using simulation 400
[00:34:05.734] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.0653620401424797
[00:34:05.734] Browser WARN    Inference API failed, using simulation 400
[00:34:05.734] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9308549959881732
[00:34:05.734] Browser WARN    Inference API failed, using simulation 400
[00:34:05.734] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9043613910932687
[00:34:06.076] Server  ERROR   Backend returned 400
[00:34:06.106] Server  ERROR   Backend returned 400
[00:34:06.109] Server  ERROR   Backend returned 400
[00:34:06.218] Browser WARN    Inference API failed, using simulation 400
[00:34:06.218] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4069856910103058
[00:34:06.218] Browser WARN    Inference API failed, using simulation 400
[00:34:06.218] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.47662019808499445
[00:34:06.218] Browser WARN    Inference API failed, using simulation 400
[00:34:06.218] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.08076123142085645
[00:34:06.575] Server  ERROR   Backend returned 400
[00:34:06.600] Server  ERROR   Backend returned 400
[00:34:06.602] Server  ERROR   Backend returned 400
[00:34:06.712] Browser WARN    Inference API failed, using simulation 400
[00:34:06.712] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.43857246172597475
[00:34:06.712] Browser WARN    Inference API failed, using simulation 400
[00:34:06.712] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4203760936408354
[00:34:06.712] Browser WARN    Inference API failed, using simulation 400
[00:34:06.712] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9694972842621636
[00:34:07.082] Server  ERROR   Backend returned 400
[00:34:07.105] Server  ERROR   Backend returned 400
[00:34:07.113] Server  ERROR   Backend returned 400
[00:34:07.218] Browser WARN    Inference API failed, using simulation 400
[00:34:07.218] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.16499164107349562
[00:34:07.218] Browser WARN    Inference API failed, using simulation 400
[00:34:07.218] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7530326226848617
[00:34:07.218] Browser WARN    Inference API failed, using simulation 400
[00:34:07.218] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.15893348930492324
[00:34:07.575] Server  ERROR   Backend returned 400
[00:34:07.591] Server  ERROR   Backend returned 400
[00:34:07.604] Server  ERROR   Backend returned 400
[00:34:07.708] Browser WARN    Inference API failed, using simulation 400
[00:34:07.708] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.18382215569194293
[00:34:07.708] Browser WARN    Inference API failed, using simulation 400
[00:34:07.708] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8013247132111002
[00:34:07.708] Browser WARN    Inference API failed, using simulation 400
[00:34:07.708] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.30136989924593044
[00:34:08.078] Server  ERROR   Backend returned 400
[00:34:08.094] Server  ERROR   Backend returned 400
[00:34:08.105] Server  ERROR   Backend returned 400
[00:34:08.212] Browser WARN    Inference API failed, using simulation 400
[00:34:08.212] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8084801676768586
[00:34:08.212] Browser WARN    Inference API failed, using simulation 400
[00:34:08.212] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9103475286463081
[00:34:08.212] Browser WARN    Inference API failed, using simulation 400
[00:34:08.212] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.06773414378761233
[00:34:08.584] Server  ERROR   Backend returned 400
[00:34:08.600] Server  ERROR   Backend returned 400
[00:34:08.614] Server  ERROR   Backend returned 400
[00:34:08.718] Browser WARN    Inference API failed, using simulation 400
[00:34:08.718] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.44992564185425526
[00:34:08.718] Browser WARN    Inference API failed, using simulation 400
[00:34:08.718] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7859525804481505
[00:34:08.718] Browser WARN    Inference API failed, using simulation 400
[00:34:08.718] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3092399723619341
[00:34:09.082] Server  ERROR   Backend returned 400
[00:34:09.099] Server  ERROR   Backend returned 400
[00:34:09.134] Server  ERROR   Backend returned 400
[00:34:09.239] Browser WARN    Inference API failed, using simulation 400
[00:34:09.239] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.0208597013407385
[00:34:09.239] Browser WARN    Inference API failed, using simulation 400
[00:34:09.239] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.14910292795906377
[00:34:09.239] Browser WARN    Inference API failed, using simulation 400
[00:34:09.239] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7966476504881789
[00:34:09.580] Server  ERROR   Backend returned 400
[00:34:09.611] Server  ERROR   Backend returned 400
[00:34:09.613] Server  ERROR   Backend returned 400
[00:34:09.722] Browser WARN    Inference API failed, using simulation 400
[00:34:09.722] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.35389827816428976
[00:34:09.722] Browser WARN    Inference API failed, using simulation 400
[00:34:09.722] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.28417528946341813
[00:34:09.722] Browser WARN    Inference API failed, using simulation 400
[00:34:09.722] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2990676529001374
[00:34:10.076] Server  ERROR   Backend returned 400
[00:34:10.098] Server  ERROR   Backend returned 400
[00:34:10.113] Server  ERROR   Backend returned 400
[00:34:10.217] Browser WARN    Inference API failed, using simulation 400
[00:34:10.217] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7058704510233269
[00:34:10.217] Browser WARN    Inference API failed, using simulation 400
[00:34:10.217] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.42268203826026135
[00:34:10.217] Browser WARN    Inference API failed, using simulation 400
[00:34:10.217] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.45765066374836455
[00:34:10.582] Server  ERROR   Backend returned 400
[00:34:10.599] Server  ERROR   Backend returned 400
[00:34:10.610] Server  ERROR   Backend returned 400
[00:34:10.726] Browser WARN    Inference API failed, using simulation 400
[00:34:10.726] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3263226110335144
[00:34:10.726] Browser WARN    Inference API failed, using simulation 400
[00:34:10.726] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.33294798930933994
[00:34:10.726] Browser WARN    Inference API failed, using simulation 400
[00:34:10.726] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7618823779003746
[00:34:11.076] Server  ERROR   Backend returned 400
[00:34:11.108] Server  ERROR   Backend returned 400
[00:34:11.110] Server  ERROR   Backend returned 400
[00:34:11.223] Browser WARN    Inference API failed, using simulation 400
[00:34:11.223] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.33010250255454243
[00:34:11.223] Browser WARN    Inference API failed, using simulation 400
[00:34:11.223] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8307515397589881
[00:34:11.223] Browser WARN    Inference API failed, using simulation 400
[00:34:11.223] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.42190387123611367
[00:34:11.575] Server  ERROR   Backend returned 400
[00:34:11.595] Server  ERROR   Backend returned 400
[00:34:11.609] Server  ERROR   Backend returned 400
[00:34:11.713] Browser WARN    Inference API failed, using simulation 400
[00:34:11.713] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.21328388661022973
[00:34:11.713] Browser WARN    Inference API failed, using simulation 400
[00:34:11.713] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4593237820089294
[00:34:11.713] Browser WARN    Inference API failed, using simulation 400
[00:34:11.713] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2151791864996091
[00:34:12.082] Server  ERROR   Backend returned 400
[00:34:12.098] Server  ERROR   Backend returned 400
[00:34:12.111] Server  ERROR   Backend returned 400
[00:34:12.214] Browser WARN    Inference API failed, using simulation 400
[00:34:12.214] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8472629684063688
[00:34:12.214] Browser WARN    Inference API failed, using simulation 400
[00:34:12.214] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.10770393333684036
[00:34:12.214] Browser WARN    Inference API failed, using simulation 400
[00:34:12.214] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1617535355794732
[00:34:12.579] Server  ERROR   Backend returned 400
[00:34:12.597] Server  ERROR   Backend returned 400
[00:34:12.616] Server  ERROR   Backend returned 400
[00:34:12.720] Browser WARN    Inference API failed, using simulation 400
[00:34:12.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.41259861990254504
[00:34:12.720] Browser WARN    Inference API failed, using simulation 400
[00:34:12.720] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1164226421753648
[00:34:12.720] Browser WARN    Inference API failed, using simulation 400
[00:34:12.720] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.048547199115888506
[00:34:13.081] Server  ERROR   Backend returned 400
[00:34:13.100] Server  ERROR   Backend returned 400
[00:34:13.112] Server  ERROR   Backend returned 400
[00:34:13.216] Browser WARN    Inference API failed, using simulation 400
[00:34:13.216] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.04444947030388352
[00:34:13.216] Browser WARN    Inference API failed, using simulation 400
[00:34:13.216] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.005101520278419036
[00:34:13.216] Browser WARN    Inference API failed, using simulation 400
[00:34:13.216] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9836840577512063
[00:34:13.576] Server  ERROR   Backend returned 400
[00:34:13.600] Server  ERROR   Backend returned 400
[00:34:13.603] Server  ERROR   Backend returned 400
[00:34:13.712] Browser WARN    Inference API failed, using simulation 400
[00:34:13.712] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4423635606323098
[00:34:13.712] Browser WARN    Inference API failed, using simulation 400
[00:34:13.712] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.08936648911459039
[00:34:13.712] Browser WARN    Inference API failed, using simulation 400
[00:34:13.712] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.48286801534350265
[00:34:14.077] Server  ERROR   Backend returned 400
[00:34:14.101] Server  ERROR   Backend returned 400
[00:34:14.116] Server  ERROR   Backend returned 400
[00:34:14.224] Browser WARN    Inference API failed, using simulation 400
[00:34:14.224] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.02856097630129706
[00:34:14.224] Browser WARN    Inference API failed, using simulation 400
[00:34:14.224] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.880518810499678
[00:34:14.224] Browser WARN    Inference API failed, using simulation 400
[00:34:14.224] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7073582684447636
[00:34:14.588] Server  ERROR   Backend returned 400
[00:34:14.600] Server  ERROR   Backend returned 400
[00:34:14.619] Server  ERROR   Backend returned 400
[00:34:14.724] Browser WARN    Inference API failed, using simulation 400
[00:34:14.724] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.47855643721917696
[00:34:14.724] Browser WARN    Inference API failed, using simulation 400
[00:34:14.724] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.19174739119483863
[00:34:14.724] Browser WARN    Inference API failed, using simulation 400
[00:34:14.724] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.13043105053285992
[00:34:15.077] Server  ERROR   Backend returned 400
[00:34:15.095] Server  ERROR   Backend returned 400
[00:34:15.106] Server  ERROR   Backend returned 400
[00:34:15.241] Browser WARN    Inference API failed, using simulation 400
[00:34:15.241] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3662720296159049
[00:34:15.241] Browser WARN    Inference API failed, using simulation 400
[00:34:15.241] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3640609736705717
[00:34:15.241] Browser WARN    Inference API failed, using simulation 400
[00:34:15.241] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.44926132678749386
[00:34:15.584] Server  ERROR   Backend returned 400
[00:34:15.599] Server  ERROR   Backend returned 400
[00:34:15.609] Server  ERROR   Backend returned 400
[00:34:15.713] Browser WARN    Inference API failed, using simulation 400
[00:34:15.713] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.05145149190820597
[00:34:15.713] Browser WARN    Inference API failed, using simulation 400
[00:34:15.713] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7762915645631473
[00:34:15.713] Browser WARN    Inference API failed, using simulation 400
[00:34:15.713] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8304003016744015
[00:34:16.077] Server  ERROR   Backend returned 400
[00:34:16.090] Server  ERROR   Backend returned 400
[00:34:16.101] Server  ERROR   Backend returned 400
[00:34:16.206] Browser WARN    Inference API failed, using simulation 400
[00:34:16.206] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3984829326332488
[00:34:16.206] Browser WARN    Inference API failed, using simulation 400
[00:34:16.206] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.25379603724746796
[00:34:16.206] Browser WARN    Inference API failed, using simulation 400
[00:34:16.206] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3506305249610776
[00:34:16.579] Server  ERROR   Backend returned 400
[00:34:16.594] Server  ERROR   Backend returned 400
[00:34:16.605] Server  ERROR   Backend returned 400
[00:34:16.715] Browser WARN    Inference API failed, using simulation 400
[00:34:16.715] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1131972754624353
[00:34:16.715] Browser WARN    Inference API failed, using simulation 400
[00:34:16.715] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8662661263766458
[00:34:16.715] Browser WARN    Inference API failed, using simulation 400
[00:34:16.715] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.11849717900049173
[00:34:17.080] Server  ERROR   Backend returned 400
[00:34:17.099] Server  ERROR   Backend returned 400
[00:34:17.109] Server  ERROR   Backend returned 400
[00:34:17.213] Browser WARN    Inference API failed, using simulation 400
[00:34:17.213] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7169042585712416
[00:34:17.213] Browser WARN    Inference API failed, using simulation 400
[00:34:17.213] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4509257770237149
[00:34:17.213] Browser WARN    Inference API failed, using simulation 400
[00:34:17.213] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.07139741831415242
[00:34:17.588] Server  ERROR   Backend returned 400
[00:34:17.615] Server  ERROR   Backend returned 400
[00:34:17.626] Server  ERROR   Backend returned 400
[00:34:17.730] Browser WARN    Inference API failed, using simulation 400
[00:34:17.730] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.480149987194287
[00:34:17.730] Browser WARN    Inference API failed, using simulation 400
[00:34:17.730] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.20453057147545656
[00:34:17.730] Browser WARN    Inference API failed, using simulation 400
[00:34:17.730] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4778089394451097
[00:34:18.077] Server  ERROR   Backend returned 400
[00:34:18.099] Server  ERROR   Backend returned 400
[00:34:18.112] Server  ERROR   Backend returned 400
[00:34:18.216] Browser WARN    Inference API failed, using simulation 400
[00:34:18.216] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3441545441364535
[00:34:18.216] Browser WARN    Inference API failed, using simulation 400
[00:34:18.216] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8120487048831938
[00:34:18.216] Browser WARN    Inference API failed, using simulation 400
[00:34:18.216] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.17441957379491585
[00:34:18.578] Server  ERROR   Backend returned 400
[00:34:18.593] Server  ERROR   Backend returned 400
[00:34:18.605] Server  ERROR   Backend returned 400
[00:34:18.711] Browser WARN    Inference API failed, using simulation 400
[00:34:18.711] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3419530142312478
[00:34:18.711] Browser WARN    Inference API failed, using simulation 400
[00:34:18.711] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3638775576841293
[00:34:18.711] Browser WARN    Inference API failed, using simulation 400
[00:34:18.711] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.37432328409719345
[00:34:19.093] Server  ERROR   Backend returned 400
[00:34:19.110] Server  ERROR   Backend returned 400
[00:34:19.128] Server  ERROR   Backend returned 400
[00:34:19.239] Browser WARN    Inference API failed, using simulation 400
[00:34:19.239] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.23944103274799172
[00:34:19.239] Browser WARN    Inference API failed, using simulation 400
[00:34:19.239] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2585684123205363
[00:34:19.239] Browser WARN    Inference API failed, using simulation 400
[00:34:19.239] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.16984653114553094
[00:34:19.584] Server  ERROR   Backend returned 400
[00:34:19.595] Server  ERROR   Backend returned 400
[00:34:19.608] Server  ERROR   Backend returned 400
[00:34:19.712] Browser WARN    Inference API failed, using simulation 400
[00:34:19.712] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.41264329480406825
[00:34:19.712] Browser WARN    Inference API failed, using simulation 400
[00:34:19.712] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.11403574896351443
[00:34:19.712] Browser WARN    Inference API failed, using simulation 400
[00:34:19.712] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7531010882947602
[00:34:20.081] Server  ERROR   Backend returned 400
[00:34:20.098] Server  ERROR   Backend returned 400
[00:34:20.108] Server  ERROR   Backend returned 400
[00:34:20.211] Browser WARN    Inference API failed, using simulation 400
[00:34:20.211] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1413020043817581
[00:34:20.211] Browser WARN    Inference API failed, using simulation 400
[00:34:20.211] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.18232216496172732
[00:34:20.211] Browser WARN    Inference API failed, using simulation 400
[00:34:20.211] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7776809964645424
[00:34:20.580] Server  ERROR   Backend returned 400
[00:34:20.594] Server  ERROR   Backend returned 400
[00:34:20.606] Server  ERROR   Backend returned 400
[00:34:20.710] Browser WARN    Inference API failed, using simulation 400
[00:34:20.711] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.43377468911229394
[00:34:20.711] Browser WARN    Inference API failed, using simulation 400
[00:34:20.711] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4602076638220167
[00:34:20.711] Browser WARN    Inference API failed, using simulation 400
[00:34:20.711] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.33496204009186786
[00:34:21.093] Server  ERROR   Backend returned 400
[00:34:21.106] Server  ERROR   Backend returned 400
[00:34:21.119] Server  ERROR   Backend returned 400
[00:34:21.232] Browser WARN    Inference API failed, using simulation 400
[00:34:21.232] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.09512567821278017
[00:34:21.232] Browser WARN    Inference API failed, using simulation 400
[00:34:21.232] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8339979094367
[00:34:21.232] Browser WARN    Inference API failed, using simulation 400
[00:34:21.232] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4038363379274059
[00:34:21.581] Server  ERROR   Backend returned 400
[00:34:21.598] Server  ERROR   Backend returned 400
[00:34:21.611] Server  ERROR   Backend returned 400
[00:34:21.716] Browser WARN    Inference API failed, using simulation 400
[00:34:21.716] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1451197335849045
[00:34:21.716] Browser WARN    Inference API failed, using simulation 400
[00:34:21.716] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.747947592139066
[00:34:21.716] Browser WARN    Inference API failed, using simulation 400
[00:34:21.716] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7136095006121168
[00:34:22.076] Server  ERROR   Backend returned 400
[00:34:22.102] Server  ERROR   Backend returned 400
[00:34:22.108] Server  ERROR   Backend returned 400
[00:34:22.215] Browser WARN    Inference API failed, using simulation 400
[00:34:22.215] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9119486226166793
[00:34:22.215] Browser WARN    Inference API failed, using simulation 400
[00:34:22.215] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.46511899449115957
[00:34:22.215] Browser WARN    Inference API failed, using simulation 400
[00:34:22.215] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.03794458100963799
[00:34:22.576] Server  ERROR   Backend returned 400
[00:34:22.594] Server  ERROR   Backend returned 400
[00:34:22.607] Server  ERROR   Backend returned 400
[00:34:22.712] Browser WARN    Inference API failed, using simulation 400
[00:34:22.712] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.42900340667331516
[00:34:22.712] Browser WARN    Inference API failed, using simulation 400
[00:34:22.712] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.19170309618058434
[00:34:22.712] Browser WARN    Inference API failed, using simulation 400
[00:34:22.712] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.022495346482449663
[00:34:23.078] Server  ERROR   Backend returned 400
[00:34:23.098] Server  ERROR   Backend returned 400
[00:34:23.110] Server  ERROR   Backend returned 400
[00:34:23.214] Browser WARN    Inference API failed, using simulation 400
[00:34:23.214] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9367747082290983
[00:34:23.214] Browser WARN    Inference API failed, using simulation 400
[00:34:23.214] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.20154149882454336
[00:34:23.214] Browser WARN    Inference API failed, using simulation 400
[00:34:23.214] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4270298047708405
[00:34:23.576] Server  ERROR   Backend returned 400
[00:34:23.601] Server  ERROR   Backend returned 400
[00:34:23.613] Server  ERROR   Backend returned 400
[00:34:23.727] Browser WARN    Inference API failed, using simulation 400
[00:34:23.727] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4767847868407952
[00:34:23.727] Browser WARN    Inference API failed, using simulation 400
[00:34:23.727] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3021618912896696
[00:34:23.727] Browser WARN    Inference API failed, using simulation 400
[00:34:23.727] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4438985564134702
[00:34:24.090] Server  ERROR   Backend returned 400
[00:34:24.133] Server  ERROR   Backend returned 400
[00:34:24.137] Server  ERROR   Backend returned 400
[00:34:24.245] Browser WARN    Inference API failed, using simulation 400
[00:34:24.245] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7916966903388131
[00:34:24.245] Browser WARN    Inference API failed, using simulation 400
[00:34:24.245] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9952590000655412
[00:34:24.245] Browser WARN    Inference API failed, using simulation 400
[00:34:24.245] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2373798570861878
[00:34:24.579] Server  ERROR   Backend returned 400
[00:34:24.602] Server  ERROR   Backend returned 400
[00:34:24.615] Server  ERROR   Backend returned 400
[00:34:24.720] Browser WARN    Inference API failed, using simulation 400
[00:34:24.720] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8467907648103796
[00:34:24.720] Browser WARN    Inference API failed, using simulation 400
[00:34:24.720] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7214937490139668
[00:34:24.720] Browser WARN    Inference API failed, using simulation 400
[00:34:24.720] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.02378978325805775
[00:34:25.079] Server  ERROR   Backend returned 400
[00:34:25.092] Server  ERROR   Backend returned 400
[00:34:25.104] Server  ERROR   Backend returned 400
[00:34:25.208] Browser WARN    Inference API failed, using simulation 400
[00:34:25.208] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7572011239587393
[00:34:25.208] Browser WARN    Inference API failed, using simulation 400
[00:34:25.208] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9903189294696552
[00:34:25.208] Browser WARN    Inference API failed, using simulation 400
[00:34:25.208] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4212318833163238
[00:34:25.583] Server  ERROR   Backend returned 400
[00:34:25.598] Server  ERROR   Backend returned 400
[00:34:25.612] Server  ERROR   Backend returned 400
[00:34:25.718] Browser WARN    Inference API failed, using simulation 400
[00:34:25.718] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7265892822670081
[00:34:25.718] Browser WARN    Inference API failed, using simulation 400
[00:34:25.718] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.49959922854239824
[00:34:25.718] Browser WARN    Inference API failed, using simulation 400
[00:34:25.718] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.46311925831145745
[00:34:26.080] Server  ERROR   Backend returned 400
[00:34:26.096] Server  ERROR   Backend returned 400
[00:34:26.109] Server  ERROR   Backend returned 400
[00:34:26.212] Browser WARN    Inference API failed, using simulation 400
[00:34:26.212] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.11721427823650138
[00:34:26.212] Browser WARN    Inference API failed, using simulation 400
[00:34:26.212] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.024306375584331352
[00:34:26.212] Browser WARN    Inference API failed, using simulation 400
[00:34:26.212] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.41833365285516294
[00:34:26.581] Server  ERROR   Backend returned 400
[00:34:26.599] Server  ERROR   Backend returned 400
[00:34:26.611] Server  ERROR   Backend returned 400
[00:34:26.714] Browser WARN    Inference API failed, using simulation 400
[00:34:26.714] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.025591826036505116
[00:34:26.714] Browser WARN    Inference API failed, using simulation 400
[00:34:26.714] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.35641153206075343
[00:34:26.714] Browser WARN    Inference API failed, using simulation 400
[00:34:26.714] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2714888218727517
[00:34:27.091] Server  ERROR   Backend returned 400
[00:34:27.150] Server  ERROR   Backend returned 400
[00:34:27.257] Browser WARN    Inference API failed, using simulation 400
[00:34:27.257] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.0995363372065689
[00:34:27.257] Browser WARN    Inference API failed, using simulation 400
[00:34:27.257] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.14047044594707164
[00:34:27.577] Server  ERROR   Backend returned 400
[00:34:27.599] Server  ERROR   Backend returned 400
[00:34:27.613] Server  ERROR   Backend returned 400
[00:34:27.718] Browser WARN    Inference API failed, using simulation 400
[00:34:27.718] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2586812742745224
[00:34:27.718] Browser WARN    Inference API failed, using simulation 400
[00:34:27.718] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.47452988840254257
[00:34:27.718] Browser WARN    Inference API failed, using simulation 400
[00:34:27.718] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3735167433380362
[00:34:28.085] Server  ERROR   Backend returned 400
[00:34:28.097] Server  ERROR   Backend returned 400
[00:34:28.110] Server  ERROR   Backend returned 400
[00:34:28.225] Browser WARN    Inference API failed, using simulation 400
[00:34:28.225] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.937605383318285
[00:34:28.225] Browser WARN    Inference API failed, using simulation 400
[00:34:28.225] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9434338062171914
[00:34:28.225] Browser WARN    Inference API failed, using simulation 400
[00:34:28.225] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.06065971834175676
[00:34:28.584] Server  ERROR   Backend returned 400
[00:34:28.607] Server  ERROR   Backend returned 400
[00:34:28.620] Server  ERROR   Backend returned 400
[00:34:28.725] Browser WARN    Inference API failed, using simulation 400
[00:34:28.725] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.46721632063180607
[00:34:28.725] Browser WARN    Inference API failed, using simulation 400
[00:34:28.725] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8018494710469275
[00:34:28.725] Browser WARN    Inference API failed, using simulation 400
[00:34:28.725] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3188359360549095
[00:34:29.082] Server  ERROR   Backend returned 400
[00:34:29.101] Server  ERROR   Backend returned 400
[00:34:29.118] Server  ERROR   Backend returned 400
[00:34:29.223] Browser WARN    Inference API failed, using simulation 400
[00:34:29.223] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8128605984542252
[00:34:29.223] Browser WARN    Inference API failed, using simulation 400
[00:34:29.223] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.0328102832595748
[00:34:29.223] Browser WARN    Inference API failed, using simulation 400
[00:34:29.223] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.11741438419624356
[00:34:29.578] Server  ERROR   Backend returned 400
[00:34:29.603] Server  ERROR   Backend returned 400
[00:34:29.617] Server  ERROR   Backend returned 400
[00:34:29.720] Browser WARN    Inference API failed, using simulation 400
[00:34:29.720] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7986368430880679
[00:34:29.720] Browser WARN    Inference API failed, using simulation 400
[00:34:29.720] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.0640607456429968
[00:34:29.720] Browser WARN    Inference API failed, using simulation 400
[00:34:29.720] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3955703503297
[00:34:30.084] Server  ERROR   Backend returned 400
[00:34:30.105] Server  ERROR   Backend returned 400
[00:34:30.123] Server  ERROR   Backend returned 400
[00:34:30.226] Browser WARN    Inference API failed, using simulation 400
[00:34:30.226] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8667170251322128
[00:34:30.226] Browser WARN    Inference API failed, using simulation 400
[00:34:30.226] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.01756606601425653
[00:34:30.226] Browser WARN    Inference API failed, using simulation 400
[00:34:30.226] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.815800704159692
[00:34:30.586] Server  ERROR   Backend returned 400
[00:34:30.603] Server  ERROR   Backend returned 400
[00:34:30.616] Server  ERROR   Backend returned 400
[00:34:30.730] Browser WARN    Inference API failed, using simulation 400
[00:34:30.730] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2999387731024529
[00:34:30.730] Browser WARN    Inference API failed, using simulation 400
[00:34:30.730] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.86794293418984
[00:34:30.730] Browser WARN    Inference API failed, using simulation 400
[00:34:30.730] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3137118629147773
[00:34:31.075] Server  ERROR   Backend returned 400
[00:34:31.101] Server  ERROR   Backend returned 400
[00:34:31.112] Server  ERROR   Backend returned 400
[00:34:31.218] Browser WARN    Inference API failed, using simulation 400
[00:34:31.218] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.48773771539214816
[00:34:31.218] Browser WARN    Inference API failed, using simulation 400
[00:34:31.218] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.11270682227530032
[00:34:31.218] Browser WARN    Inference API failed, using simulation 400
[00:34:31.218] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4989393869120681
[00:34:31.577] Server  ERROR   Backend returned 400
[00:34:31.602] Server  ERROR   Backend returned 400
[00:34:31.616] Server  ERROR   Backend returned 400
[00:34:31.722] Browser WARN    Inference API failed, using simulation 400
[00:34:31.722] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9919624666552761
[00:34:31.722] Browser WARN    Inference API failed, using simulation 400
[00:34:31.722] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8875136848063306
[00:34:31.722] Browser WARN    Inference API failed, using simulation 400
[00:34:31.722] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.32638526239998084
[00:34:32.080] Server  ERROR   Backend returned 400
[00:34:32.108] Server  ERROR   Backend returned 400
[00:34:32.111] Server  ERROR   Backend returned 400
[00:34:32.220] Browser WARN    Inference API failed, using simulation 400
[00:34:32.220] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8904115236514835
[00:34:32.220] Browser WARN    Inference API failed, using simulation 400
[00:34:32.220] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8024388397228335
[00:34:32.221] Browser WARN    Inference API failed, using simulation 400
[00:34:32.221] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4452489343849349
[00:34:32.579] Server  ERROR   Backend returned 400
[00:34:32.603] Server  ERROR   Backend returned 400
[00:34:32.617] Server  ERROR   Backend returned 400
[00:34:32.720] Browser WARN    Inference API failed, using simulation 400
[00:34:32.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3311497261883193
[00:34:32.720] Browser WARN    Inference API failed, using simulation 400
[00:34:32.720] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9129326246340503
[00:34:32.720] Browser WARN    Inference API failed, using simulation 400
[00:34:32.720] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.24738745318923777
[00:34:33.077] Server  ERROR   Backend returned 400
[00:34:33.103] Server  ERROR   Backend returned 400
[00:34:33.114] Server  ERROR   Backend returned 400
[00:34:33.218] Browser WARN    Inference API failed, using simulation 400
[00:34:33.218] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9217380471917144
[00:34:33.218] Browser WARN    Inference API failed, using simulation 400
[00:34:33.218] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.15183266733481343
[00:34:33.218] Browser WARN    Inference API failed, using simulation 400
[00:34:33.218] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2163196122454129
[00:34:33.579] Server  ERROR   Backend returned 400
[00:34:33.597] Server  ERROR   Backend returned 400
[00:34:33.611] Server  ERROR   Backend returned 400
[00:34:33.715] Browser WARN    Inference API failed, using simulation 400
[00:34:33.715] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8321960055089814
[00:34:33.715] Browser WARN    Inference API failed, using simulation 400
[00:34:33.715] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3417005809928436
[00:34:33.715] Browser WARN    Inference API failed, using simulation 400
[00:34:33.715] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3169038889732575
[00:34:34.077] Server  ERROR   Backend returned 400
[00:34:34.097] Server  ERROR   Backend returned 400
[00:34:34.109] Server  ERROR   Backend returned 400
[00:34:34.213] Browser WARN    Inference API failed, using simulation 400
[00:34:34.213] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4581943770214813
[00:34:34.213] Browser WARN    Inference API failed, using simulation 400
[00:34:34.213] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9087070605147306
[00:34:34.213] Browser WARN    Inference API failed, using simulation 400
[00:34:34.213] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.043735442599233176
[00:34:34.588] Server  ERROR   Backend returned 400
[00:34:34.605] Server  ERROR   Backend returned 400
[00:34:34.617] Server  ERROR   Backend returned 400
[00:34:34.720] Browser WARN    Inference API failed, using simulation 400
[00:34:34.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.18711569550561552
[00:34:34.720] Browser WARN    Inference API failed, using simulation 400
[00:34:34.720] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9069323738049899
[00:34:34.720] Browser WARN    Inference API failed, using simulation 400
[00:34:34.720] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7104330704151018
[00:34:35.080] Server  ERROR   Backend returned 400
[00:34:35.099] Server  ERROR   Backend returned 400
[00:34:35.116] Server  ERROR   Backend returned 400
[00:34:35.220] Browser WARN    Inference API failed, using simulation 400
[00:34:35.220] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.43401658545531685
[00:34:35.220] Browser WARN    Inference API failed, using simulation 400
[00:34:35.220] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.41534830945959883
[00:34:35.220] Browser WARN    Inference API failed, using simulation 400
[00:34:35.220] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9784567149641934
[00:34:35.576] Server  ERROR   Backend returned 400
[00:34:35.598] Server  ERROR   Backend returned 400
[00:34:35.612] Server  ERROR   Backend returned 400
[00:34:35.718] Browser WARN    Inference API failed, using simulation 400
[00:34:35.718] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.19274215256293026
[00:34:35.718] Browser WARN    Inference API failed, using simulation 400
[00:34:35.718] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7886750906108106
[00:34:35.718] Browser WARN    Inference API failed, using simulation 400
[00:34:35.718] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.32230233032135647
[00:34:36.078] Server  ERROR   Backend returned 400
[00:34:36.094] Server  ERROR   Backend returned 400
[00:34:36.111] Server  ERROR   Backend returned 400
[00:34:36.216] Browser WARN    Inference API failed, using simulation 400
[00:34:36.216] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.022113249944881452
[00:34:36.216] Browser WARN    Inference API failed, using simulation 400
[00:34:36.216] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9746124522003149
[00:34:36.216] Browser WARN    Inference API failed, using simulation 400
[00:34:36.216] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.04571099561883735
[00:34:36.578] Server  ERROR   Backend returned 400
[00:34:36.595] Server  ERROR   Backend returned 400
[00:34:36.614] Server  ERROR   Backend returned 400
[00:34:36.719] Browser WARN    Inference API failed, using simulation 400
[00:34:36.719] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.08814835231384899
[00:34:36.719] Browser WARN    Inference API failed, using simulation 400
[00:34:36.719] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9598730677009673
[00:34:36.719] Browser WARN    Inference API failed, using simulation 400
[00:34:36.719] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4033383789887707
[00:34:37.090] Server  ERROR   Backend returned 400
[00:34:37.107] Server  ERROR   Backend returned 400
[00:34:37.140] Server  ERROR   Backend returned 400
[00:34:37.244] Browser WARN    Inference API failed, using simulation 400
[00:34:37.244] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7949837110739184
[00:34:37.244] Browser WARN    Inference API failed, using simulation 400
[00:34:37.244] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.07246412956047976
[00:34:37.244] Browser WARN    Inference API failed, using simulation 400
[00:34:37.244] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.14869065793217062
[00:34:37.588] Server  ERROR   Backend returned 400
[00:34:37.606] Server  ERROR   Backend returned 400
[00:34:37.612] Server  ERROR   Backend returned 400
[00:34:37.721] Browser WARN    Inference API failed, using simulation 400
[00:34:37.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.13622210753436353
[00:34:37.721] Browser WARN    Inference API failed, using simulation 400
[00:34:37.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.27460180824482644
[00:34:37.721] Browser WARN    Inference API failed, using simulation 400
[00:34:37.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3830640923770204
[00:34:38.075] Server  ERROR   Backend returned 400
[00:34:38.090] Server  ERROR   Backend returned 400
[00:34:38.103] Server  ERROR   Backend returned 400
[00:34:38.206] Browser WARN    Inference API failed, using simulation 400
[00:34:38.206] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3100335389202016
[00:34:38.206] Browser WARN    Inference API failed, using simulation 400
[00:34:38.206] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.43668987390529035
[00:34:38.206] Browser WARN    Inference API failed, using simulation 400
[00:34:38.206] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.057139998656922764
[00:34:38.579] Server  ERROR   Backend returned 400
[00:34:38.600] Server  ERROR   Backend returned 400
[00:34:38.612] Server  ERROR   Backend returned 400
[00:34:38.717] Browser WARN    Inference API failed, using simulation 400
[00:34:38.717] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8349458108651853
[00:34:38.717] Browser WARN    Inference API failed, using simulation 400
[00:34:38.717] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.09494769437605288
[00:34:38.717] Browser WARN    Inference API failed, using simulation 400
[00:34:38.717] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.014937134833206744
[00:34:39.079] Server  ERROR   Backend returned 400
[00:34:39.094] Server  ERROR   Backend returned 400
[00:34:39.106] Server  ERROR   Backend returned 400
[00:34:39.210] Browser WARN    Inference API failed, using simulation 400
[00:34:39.210] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.25892493992150173
[00:34:39.210] Browser WARN    Inference API failed, using simulation 400
[00:34:39.210] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1230263070902673
[00:34:39.210] Browser WARN    Inference API failed, using simulation 400
[00:34:39.210] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8308892882436367
[00:34:39.576] Server  ERROR   Backend returned 400
[00:34:39.593] Server  ERROR   Backend returned 400
[00:34:39.604] Server  ERROR   Backend returned 400
[00:34:39.709] Browser WARN    Inference API failed, using simulation 400
[00:34:39.709] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.15489148790631008
[00:34:39.709] Browser WARN    Inference API failed, using simulation 400
[00:34:39.709] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4712072899735406
[00:34:39.709] Browser WARN    Inference API failed, using simulation 400
[00:34:39.709] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.41172626299576853
[00:34:40.076] Server  ERROR   Backend returned 400
[00:34:40.110] Server  ERROR   Backend returned 400
[00:34:40.112] Server  ERROR   Backend returned 400
[00:34:40.221] Browser WARN    Inference API failed, using simulation 400
[00:34:40.221] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.055222786243198096
[00:34:40.221] Browser WARN    Inference API failed, using simulation 400
[00:34:40.221] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7469509843666923
[00:34:40.221] Browser WARN    Inference API failed, using simulation 400
[00:34:40.221] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4288254440153721
[00:34:40.576] Server  ERROR   Backend returned 400
[00:34:40.595] Server  ERROR   Backend returned 400
[00:34:40.607] Server  ERROR   Backend returned 400
[00:34:40.711] Browser WARN    Inference API failed, using simulation 400
[00:34:40.711] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.02338052154933623
[00:34:40.711] Browser WARN    Inference API failed, using simulation 400
[00:34:40.711] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.43677121178376227
[00:34:40.711] Browser WARN    Inference API failed, using simulation 400
[00:34:40.711] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2725886576733094
[00:34:41.076] Server  ERROR   Backend returned 400
[00:34:41.102] Server  ERROR   Backend returned 400
[00:34:41.104] Server  ERROR   Backend returned 400
[00:34:41.213] Browser WARN    Inference API failed, using simulation 400
[00:34:41.213] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9765970066308658
[00:34:41.213] Browser WARN    Inference API failed, using simulation 400
[00:34:41.213] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9402331963465228
[00:34:41.213] Browser WARN    Inference API failed, using simulation 400
[00:34:41.213] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9708637550964664
[00:34:41.583] Server  ERROR   Backend returned 400
[00:34:41.613] Server  ERROR   Backend returned 400
[00:34:41.616] Server  ERROR   Backend returned 400
[00:34:41.724] Browser WARN    Inference API failed, using simulation 400
[00:34:41.724] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9230666376958397
[00:34:41.724] Browser WARN    Inference API failed, using simulation 400
[00:34:41.724] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3100318570070851
[00:34:41.724] Browser WARN    Inference API failed, using simulation 400
[00:34:41.724] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.41192632849515076
[00:34:42.081] Server  ERROR   Backend returned 400
[00:34:42.102] Server  ERROR   Backend returned 400
[00:34:42.114] Server  ERROR   Backend returned 400
[00:34:42.231] Browser WARN    Inference API failed, using simulation 400
[00:34:42.231] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1869246181709932
[00:34:42.231] Browser WARN    Inference API failed, using simulation 400
[00:34:42.231] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.0436348650933166
[00:34:42.231] Browser WARN    Inference API failed, using simulation 400
[00:34:42.231] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4324261751161679
[00:34:42.576] Server  ERROR   Backend returned 400
[00:34:42.609] Server  ERROR   Backend returned 400
[00:34:42.611] Server  ERROR   Backend returned 400
[00:34:42.719] Browser WARN    Inference API failed, using simulation 400
[00:34:42.719] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9371419489170165
[00:34:42.719] Browser WARN    Inference API failed, using simulation 400
[00:34:42.719] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3744389974659718
[00:34:42.719] Browser WARN    Inference API failed, using simulation 400
[00:34:42.719] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9993735012644733
[00:34:43.076] Server  ERROR   Backend returned 400
[00:34:43.108] Server  ERROR   Backend returned 400
[00:34:43.110] Server  ERROR   Backend returned 400
[00:34:43.218] Browser WARN    Inference API failed, using simulation 400
[00:34:43.218] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.09686994607445343
[00:34:43.218] Browser WARN    Inference API failed, using simulation 400
[00:34:43.218] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.29513800493237247
[00:34:43.218] Browser WARN    Inference API failed, using simulation 400
[00:34:43.218] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1857689800128261
[00:34:43.583] Server  ERROR   Backend returned 400
[00:34:43.608] Server  ERROR   Backend returned 400
[00:34:43.610] Server  ERROR   Backend returned 400
[00:34:43.721] Browser WARN    Inference API failed, using simulation 400
[00:34:43.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.16596275555118578
[00:34:43.721] Browser WARN    Inference API failed, using simulation 400
[00:34:43.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4190966813942847
[00:34:43.721] Browser WARN    Inference API failed, using simulation 400
[00:34:43.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3402054942407374
[00:34:44.077] Server  ERROR   Backend returned 400
[00:34:44.109] Server  ERROR   Backend returned 400
[00:34:44.111] Server  ERROR   Backend returned 400
[00:34:44.222] Browser WARN    Inference API failed, using simulation 400
[00:34:44.222] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7013651359320849
[00:34:44.222] Browser WARN    Inference API failed, using simulation 400
[00:34:44.222] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.03461632445074847
[00:34:44.222] Browser WARN    Inference API failed, using simulation 400
[00:34:44.222] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9819146291535612
[00:34:44.588] Server  ERROR   Backend returned 400
[00:34:44.602] Server  ERROR   Backend returned 400
[00:34:44.621] Server  ERROR   Backend returned 400
[00:34:44.724] Browser WARN    Inference API failed, using simulation 400
[00:34:44.724] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.13586010849816993
[00:34:44.724] Browser WARN    Inference API failed, using simulation 400
[00:34:44.724] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.12293277738053371
[00:34:44.724] Browser WARN    Inference API failed, using simulation 400
[00:34:44.724] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.017914911115374132
[00:34:45.089] Server  ERROR   Backend returned 400
[00:34:45.105] Server  ERROR   Backend returned 400
[00:34:45.116] Server  ERROR   Backend returned 400
[00:34:45.220] Browser WARN    Inference API failed, using simulation 400
[00:34:45.220] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.06738948384599208
[00:34:45.220] Browser WARN    Inference API failed, using simulation 400
[00:34:45.220] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.007496813381487055
[00:34:45.220] Browser WARN    Inference API failed, using simulation 400
[00:34:45.220] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.47163668240905804
[00:34:45.588] Server  ERROR   Backend returned 400
[00:34:45.603] Server  ERROR   Backend returned 400
[00:34:45.617] Server  ERROR   Backend returned 400
[00:34:45.722] Browser WARN    Inference API failed, using simulation 400
[00:34:45.722] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.711153539560821
[00:34:45.722] Browser WARN    Inference API failed, using simulation 400
[00:34:45.722] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8911922775982927
[00:34:45.722] Browser WARN    Inference API failed, using simulation 400
[00:34:45.722] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.33978583629428016
[00:34:46.080] Server  ERROR   Backend returned 400
[00:34:46.098] Server  ERROR   Backend returned 400
[00:34:46.115] Server  ERROR   Backend returned 400
[00:34:46.221] Browser WARN    Inference API failed, using simulation 400
[00:34:46.221] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.12891359049174822
[00:34:46.221] Browser WARN    Inference API failed, using simulation 400
[00:34:46.221] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.14003659358441567
[00:34:46.221] Browser WARN    Inference API failed, using simulation 400
[00:34:46.221] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.037382618333936246
[00:34:46.574] Server  ERROR   Backend returned 400
[00:34:46.598] Server  ERROR   Backend returned 400
[00:34:46.601] Server  ERROR   Backend returned 400
[00:34:46.710] Browser WARN    Inference API failed, using simulation 400
[00:34:46.710] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8284719478656983
[00:34:46.710] Browser WARN    Inference API failed, using simulation 400
[00:34:46.710] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4619328685367642
[00:34:46.710] Browser WARN    Inference API failed, using simulation 400
[00:34:46.710] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.20767985251895138
[00:34:47.077] Server  ERROR   Backend returned 400
[00:34:47.101] Server  ERROR   Backend returned 400
[00:34:47.104] Server  ERROR   Backend returned 400
[00:34:47.212] Browser WARN    Inference API failed, using simulation 400
[00:34:47.212] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.09799845160287585
[00:34:47.212] Browser WARN    Inference API failed, using simulation 400
[00:34:47.212] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9051222586903037
[00:34:47.212] Browser WARN    Inference API failed, using simulation 400
[00:34:47.212] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3761654734892875
[00:34:47.578] Server  ERROR   Backend returned 400
[00:34:47.609] Server  ERROR   Backend returned 400
[00:34:47.638] Server  ERROR   Backend returned 400
[00:34:47.742] Browser WARN    Inference API failed, using simulation 400
[00:34:47.742] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.0790110996181922
[00:34:47.742] Browser WARN    Inference API failed, using simulation 400
[00:34:47.742] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.34998774861675463
[00:34:47.742] Browser WARN    Inference API failed, using simulation 400
[00:34:47.742] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.03667953540388613
[00:34:48.077] Server  ERROR   Backend returned 400
[00:34:48.103] Server  ERROR   Backend returned 400
[00:34:48.105] Server  ERROR   Backend returned 400
[00:34:48.219] Browser WARN    Inference API failed, using simulation 400
[00:34:48.219] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.755310782589232
[00:34:48.219] Browser WARN    Inference API failed, using simulation 400
[00:34:48.219] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1659336533196718
[00:34:48.219] Browser WARN    Inference API failed, using simulation 400
[00:34:48.219] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8820489842643817
[00:34:48.580] Server  ERROR   Backend returned 400
[00:34:48.615] Server  ERROR   Backend returned 400
[00:34:48.617] Server  ERROR   Backend returned 400
[00:34:48.726] Browser WARN    Inference API failed, using simulation 400
[00:34:48.726] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3564545987270842
[00:34:48.726] Browser WARN    Inference API failed, using simulation 400
[00:34:48.726] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3578324900025755
[00:34:48.726] Browser WARN    Inference API failed, using simulation 400
[00:34:48.726] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.06483686809558692
[00:34:49.081] Server  ERROR   Backend returned 400
[00:34:49.098] Server  ERROR   Backend returned 400
[00:34:49.110] Server  ERROR   Backend returned 400
[00:34:49.214] Browser WARN    Inference API failed, using simulation 400
[00:34:49.214] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.894532321417775
[00:34:49.214] Browser WARN    Inference API failed, using simulation 400
[00:34:49.214] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.07603874748126976
[00:34:49.214] Browser WARN    Inference API failed, using simulation 400
[00:34:49.214] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9724852535484023
[00:34:49.577] Server  ERROR   Backend returned 400
[00:34:49.610] Server  ERROR   Backend returned 400
[00:34:49.613] Server  ERROR   Backend returned 400
[00:34:49.721] Browser WARN    Inference API failed, using simulation 400
[00:34:49.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3516944187822199
[00:34:49.721] Browser WARN    Inference API failed, using simulation 400
[00:34:49.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.20424752321139183
[00:34:49.721] Browser WARN    Inference API failed, using simulation 400
[00:34:49.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.21553384532898695
[00:34:50.076] Server  ERROR   Backend returned 400
[00:34:50.100] Server  ERROR   Backend returned 400
[00:34:50.113] Server  ERROR   Backend returned 400
[00:34:50.216] Browser WARN    Inference API failed, using simulation 400
[00:34:50.216] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7910329325129908
[00:34:50.216] Browser WARN    Inference API failed, using simulation 400
[00:34:50.216] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9435317403915072
[00:34:50.216] Browser WARN    Inference API failed, using simulation 400
[00:34:50.216] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.44070092445696935
[00:34:50.590] Server  ERROR   Backend returned 400
[00:34:50.629] Server  ERROR   Backend returned 400
[00:34:50.633] Server  ERROR   Backend returned 400
[00:34:50.745] Browser WARN    Inference API failed, using simulation 400
[00:34:50.745] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.17500811438861608
[00:34:50.745] Browser WARN    Inference API failed, using simulation 400
[00:34:50.745] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2822320645952686
[00:34:50.745] Browser WARN    Inference API failed, using simulation 400
[00:34:50.745] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.08740558339135723
[00:34:51.086] Server  ERROR   Backend returned 400
[00:34:51.114] Server  ERROR   Backend returned 400
[00:34:51.130] Server  ERROR   Backend returned 400
[00:34:51.247] Browser WARN    Inference API failed, using simulation 400
[00:34:51.247] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.0821445821460523
[00:34:51.247] Browser WARN    Inference API failed, using simulation 400
[00:34:51.248] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.18147502659065656
[00:34:51.248] Browser WARN    Inference API failed, using simulation 400
[00:34:51.248] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.819927687090531
[00:34:51.596] Server  ERROR   Backend returned 400
[00:34:51.628] Server  ERROR   Backend returned 400
[00:34:51.646] Server  ERROR   Backend returned 400
[00:34:51.762] Browser WARN    Inference API failed, using simulation 400
[00:34:51.762] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7796181499265319
[00:34:51.762] Browser WARN    Inference API failed, using simulation 400
[00:34:51.762] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.025257225521053306
[00:34:51.762] Browser WARN    Inference API failed, using simulation 400
[00:34:51.762] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4840898269785788
[00:34:52.097] Server  ERROR   Backend returned 400
[00:34:52.145] Server  ERROR   Backend returned 400
[00:34:52.151] Server  ERROR   Backend returned 400
[00:34:52.262] Browser WARN    Inference API failed, using simulation 400
[00:34:52.263] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9017568491705783
[00:34:52.263] Browser WARN    Inference API failed, using simulation 400
[00:34:52.263] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.770614528248627
[00:34:52.263] Browser WARN    Inference API failed, using simulation 400
[00:34:52.263] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9175718566523949
[00:34:52.585] Server  ERROR   Backend returned 400
[00:34:52.606] Server  ERROR   Backend returned 400
[00:34:52.630] Server  ERROR   Backend returned 400
[00:34:52.746] Browser WARN    Inference API failed, using simulation 400
[00:34:52.746] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.38734879349683893
[00:34:52.746] Browser WARN    Inference API failed, using simulation 400
[00:34:52.746] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.22409419539802605
[00:34:52.746] Browser WARN    Inference API failed, using simulation 400
[00:34:52.746] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.18781343207480822
[00:34:53.095] Server  ERROR   Backend returned 400
[00:34:53.133] Server  ERROR   Backend returned 400
[00:34:53.149] Server  ERROR   Backend returned 400
[00:34:53.254] Browser WARN    Inference API failed, using simulation 400
[00:34:53.254] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.08463202101106859
[00:34:53.254] Browser WARN    Inference API failed, using simulation 400
[00:34:53.254] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.05357468253694875
[00:34:53.254] Browser WARN    Inference API failed, using simulation 400
[00:34:53.254] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1285037320168803
[00:34:53.589] Server  ERROR   Backend returned 400
[00:34:53.645] Server  ERROR   Backend returned 400
[00:34:53.651] Server  ERROR   Backend returned 400
[00:34:53.773] Browser WARN    Inference API failed, using simulation 400
[00:34:53.773] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8376664396173744
[00:34:53.773] Browser WARN    Inference API failed, using simulation 400
[00:34:53.773] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3383407418309564
[00:34:53.774] Browser WARN    Inference API failed, using simulation 400
[00:34:53.774] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.16997475390158356
[00:34:54.101] Server  ERROR   Backend returned 400
[00:34:54.125] Server  ERROR   Backend returned 400
[00:34:54.160] Server  ERROR   Backend returned 400
[00:34:54.265] Browser WARN    Inference API failed, using simulation 400
[00:34:54.265] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2700009155760806
[00:34:54.265] Browser WARN    Inference API failed, using simulation 400
[00:34:54.265] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.01113755213108708
[00:34:54.265] Browser WARN    Inference API failed, using simulation 400
[00:34:54.265] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8930215154985857
[00:34:54.599] Server  ERROR   Backend returned 400
[00:34:54.630] Server  ERROR   Backend returned 400
[00:34:54.658] Server  ERROR   Backend returned 400
[00:34:54.762] Browser WARN    Inference API failed, using simulation 400
[00:34:54.762] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9691269796847657
[00:34:54.762] Browser WARN    Inference API failed, using simulation 400
[00:34:54.762] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.21134694681902338
[00:34:54.762] Browser WARN    Inference API failed, using simulation 400
[00:34:54.762] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9142798433678869
[00:34:55.088] Server  ERROR   Backend returned 400
[00:34:55.113] Server  ERROR   Backend returned 400
[00:34:55.128] Server  ERROR   Backend returned 400
[00:34:55.243] Browser WARN    Inference API failed, using simulation 400
[00:34:55.243] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9155159711367203
[00:34:55.243] Browser WARN    Inference API failed, using simulation 400
[00:34:55.243] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.24494480237512517
[00:34:55.243] Browser WARN    Inference API failed, using simulation 400
[00:34:55.243] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.09007506440919305
[00:34:55.576] Server  ERROR   Backend returned 400
[00:34:55.605] Server  ERROR   Backend returned 400
[00:34:55.621] Server  ERROR   Backend returned 400
[00:34:55.725] Browser WARN    Inference API failed, using simulation 400
[00:34:55.725] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3278391953336266
[00:34:55.725] Browser WARN    Inference API failed, using simulation 400
[00:34:55.725] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4955613625953489
[00:34:55.725] Browser WARN    Inference API failed, using simulation 400
[00:34:55.725] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4536089257762688
[00:34:56.081] Server  ERROR   Backend returned 400
[00:34:56.098] Server  ERROR   Backend returned 400
[00:34:56.115] Server  ERROR   Backend returned 400
[00:34:56.221] Browser WARN    Inference API failed, using simulation 400
[00:34:56.221] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9572641702932259
[00:34:56.221] Browser WARN    Inference API failed, using simulation 400
[00:34:56.221] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1464169220344726
[00:34:56.221] Browser WARN    Inference API failed, using simulation 400
[00:34:56.221] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7281557914309639
[00:34:56.577] Server  ERROR   Backend returned 400
[00:34:56.605] Server  ERROR   Backend returned 400
[00:34:56.610] Server  ERROR   Backend returned 400
[00:34:56.721] Browser WARN    Inference API failed, using simulation 400
[00:34:56.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4782168227087536
[00:34:56.721] Browser WARN    Inference API failed, using simulation 400
[00:34:56.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.38000812913676196
[00:34:56.721] Browser WARN    Inference API failed, using simulation 400
[00:34:56.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1705868666308103
[00:34:57.083] Server  ERROR   Backend returned 400
[00:34:57.101] Server  ERROR   Backend returned 400
[00:34:57.119] Server  ERROR   Backend returned 400
[00:34:57.224] Browser WARN    Inference API failed, using simulation 400
[00:34:57.224] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9034881638204302
[00:34:57.224] Browser WARN    Inference API failed, using simulation 400
[00:34:57.224] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8871664923273095
[00:34:57.224] Browser WARN    Inference API failed, using simulation 400
[00:34:57.224] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7485259828476285
[00:34:57.581] Server  ERROR   Backend returned 400
[00:34:57.600] Server  ERROR   Backend returned 400
[00:34:57.618] Server  ERROR   Backend returned 400
[00:34:57.722] Browser WARN    Inference API failed, using simulation 400
[00:34:57.722] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.22669582926539128
[00:34:57.722] Browser WARN    Inference API failed, using simulation 400
[00:34:57.722] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7331285169062349
[00:34:57.722] Browser WARN    Inference API failed, using simulation 400
[00:34:57.722] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7234074430627485
[00:34:58.085] Server  ERROR   Backend returned 400
[00:34:58.119] Server  ERROR   Backend returned 400
[00:34:58.133] Server  ERROR   Backend returned 400
[00:34:58.258] Browser WARN    Inference API failed, using simulation 400
[00:34:58.258] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8339168722988004
[00:34:58.258] Browser WARN    Inference API failed, using simulation 400
[00:34:58.258] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.12147075509083927
[00:34:58.258] Browser WARN    Inference API failed, using simulation 400
[00:34:58.258] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.45951359456034624
[00:34:58.596] Server  ERROR   Backend returned 400
[00:34:58.614] Server  ERROR   Backend returned 400
[00:34:58.631] Server  ERROR   Backend returned 400
[00:34:58.735] Browser WARN    Inference API failed, using simulation 400
[00:34:58.735] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.19857235759824376
[00:34:58.735] Browser WARN    Inference API failed, using simulation 400
[00:34:58.735] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2914760630666973
[00:34:58.735] Browser WARN    Inference API failed, using simulation 400
[00:34:58.735] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2776810909615264
[00:34:59.082] Server  ERROR   Backend returned 400
[00:34:59.100] Server  ERROR   Backend returned 400
[00:34:59.112] Server  ERROR   Backend returned 400
[00:34:59.216] Browser WARN    Inference API failed, using simulation 400
[00:34:59.216] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4961685303399432
[00:34:59.216] Browser WARN    Inference API failed, using simulation 400
[00:34:59.216] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3162278050576544
[00:34:59.216] Browser WARN    Inference API failed, using simulation 400
[00:34:59.216] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8235419250716084
[00:34:59.581] Server  ERROR   Backend returned 400
[00:34:59.614] Server  ERROR   Backend returned 400
[00:34:59.618] Server  ERROR   Backend returned 400
[00:34:59.741] Browser WARN    Inference API failed, using simulation 400
[00:34:59.741] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.19603746167298086
[00:34:59.741] Browser WARN    Inference API failed, using simulation 400
[00:34:59.741] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.06198138700185124
[00:34:59.741] Browser WARN    Inference API failed, using simulation 400
[00:34:59.741] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.23362516756576485
[00:35:00.087] Server  ERROR   Backend returned 400
[00:35:00.116] Server  ERROR   Backend returned 400
[00:35:00.130] Server  ERROR   Backend returned 400
[00:35:00.234] Browser WARN    Inference API failed, using simulation 400
[00:35:00.234] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8353838715843853
[00:35:00.234] Browser WARN    Inference API failed, using simulation 400
[00:35:00.234] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.18522209071031498
[00:35:00.234] Browser WARN    Inference API failed, using simulation 400
[00:35:00.234] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8682552253181977
[00:35:00.595] Server  ERROR   Backend returned 400
[00:35:00.622] Server  ERROR   Backend returned 400
[00:35:00.639] Server  ERROR   Backend returned 400
[00:35:00.745] Browser WARN    Inference API failed, using simulation 400
[00:35:00.745] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9730573479371531
[00:35:00.745] Browser WARN    Inference API failed, using simulation 400
[00:35:00.745] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3731376875768728
[00:35:00.745] Browser WARN    Inference API failed, using simulation 400
[00:35:00.746] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4396226561355659
[00:35:01.090] Server  ERROR   Backend returned 400
[00:35:01.112] Server  ERROR   Backend returned 400
[00:35:01.132] Server  ERROR   Backend returned 400
[00:35:01.237] Browser WARN    Inference API failed, using simulation 400
[00:35:01.237] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7285807533375953
[00:35:01.237] Browser WARN    Inference API failed, using simulation 400
[00:35:01.237] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.08872252156441424
[00:35:01.237] Browser WARN    Inference API failed, using simulation 400
[00:35:01.237] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.29148323848809354
[00:35:01.587] Server  ERROR   Backend returned 400
[00:35:01.603] Server  ERROR   Backend returned 400
[00:35:01.638] Server  ERROR   Backend returned 400
[00:35:01.743] Browser WARN    Inference API failed, using simulation 400
[00:35:01.743] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2673634665356976
[00:35:01.743] Browser WARN    Inference API failed, using simulation 400
[00:35:01.743] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.34340786236717186
[00:35:01.743] Browser WARN    Inference API failed, using simulation 400
[00:35:01.743] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7789662518713742
[00:35:02.112] Server  ERROR   Backend returned 400
[00:35:02.141] Server  ERROR   Backend returned 400
[00:35:02.158] Server  ERROR   Backend returned 400
[00:35:02.263] Browser WARN    Inference API failed, using simulation 400
[00:35:02.263] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.44338345811559043
[00:35:02.263] Browser WARN    Inference API failed, using simulation 400
[00:35:02.263] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1429725191872364
[00:35:02.263] Browser WARN    Inference API failed, using simulation 400
[00:35:02.263] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8663291640618455
[00:35:02.585] Server  ERROR   Backend returned 400
[00:35:02.617] Server  ERROR   Backend returned 400
[00:35:02.633] Server  ERROR   Backend returned 400
[00:35:02.738] Browser WARN    Inference API failed, using simulation 400
[00:35:02.738] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.43935895214230286
[00:35:02.738] Browser WARN    Inference API failed, using simulation 400
[00:35:02.738] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.17206017232314602
[00:35:02.738] Browser WARN    Inference API failed, using simulation 400
[00:35:02.738] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.14936216878404346
[00:35:03.086] Server  ERROR   Backend returned 400
[00:35:03.102] Server  ERROR   Backend returned 400
[00:35:03.134] Server  ERROR   Backend returned 400
[00:35:03.239] Browser WARN    Inference API failed, using simulation 400
[00:35:03.239] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.44402481524717596
[00:35:03.239] Browser WARN    Inference API failed, using simulation 400
[00:35:03.239] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.13941803208846426
[00:35:03.239] Browser WARN    Inference API failed, using simulation 400
[00:35:03.239] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.19653617769368387
[00:35:03.639] Server  ERROR   Backend returned 400
[00:35:03.767] Server  ERROR   Backend returned 400
[00:35:03.811] Browser WARN    Inference API failed, using simulation 400
[00:35:03.812] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7202405003059944
[00:35:03.813] Server  ERROR   Backend returned 400
[00:35:03.917] Browser WARN    Inference API failed, using simulation 400
[00:35:03.917] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7824750364875969
[00:35:03.917] Browser WARN    Inference API failed, using simulation 400
[00:35:03.917] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3000984311599692
[00:35:04.085] Server  ERROR   Backend returned 400
[00:35:04.129] Server  ERROR   Backend returned 400
[00:35:04.134] Server  ERROR   Backend returned 400
[00:35:04.245] Browser WARN    Inference API failed, using simulation 400
[00:35:04.245] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.44613994949089725
[00:35:04.245] Browser WARN    Inference API failed, using simulation 400
[00:35:04.245] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.019122740324080456
[00:35:04.245] Browser WARN    Inference API failed, using simulation 400
[00:35:04.245] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.008520335860474959
[00:35:04.605] Server  ERROR   Backend returned 400
[00:35:04.629] Server  ERROR   Backend returned 400
[00:35:04.643] Server  ERROR   Backend returned 400
[00:35:04.749] Browser WARN    Inference API failed, using simulation 400
[00:35:04.749] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4926614836694829
[00:35:04.749] Browser WARN    Inference API failed, using simulation 400
[00:35:04.749] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.07899378182983768
[00:35:04.749] Browser WARN    Inference API failed, using simulation 400
[00:35:04.749] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.45880447418246584
[00:35:05.081] Server  ERROR   Backend returned 400
[00:35:05.118] Server  ERROR   Backend returned 400
[00:35:05.124] Server  ERROR   Backend returned 400
[00:35:05.245] Browser WARN    Inference API failed, using simulation 400
[00:35:05.246] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.08052191630731204
[00:35:05.246] Browser WARN    Inference API failed, using simulation 400
[00:35:05.246] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7027099794698717
[00:35:05.246] Browser WARN    Inference API failed, using simulation 400
[00:35:05.246] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8817340184994337
[00:35:05.585] Server  ERROR   Backend returned 400
[00:35:05.605] Server  ERROR   Backend returned 400
[00:35:05.625] Server  ERROR   Backend returned 400
[00:35:05.729] Browser WARN    Inference API failed, using simulation 400
[00:35:05.729] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3659151495825585
[00:35:05.729] Browser WARN    Inference API failed, using simulation 400
[00:35:05.729] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4612229160814602
[00:35:05.729] Browser WARN    Inference API failed, using simulation 400
[00:35:05.729] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.05252676521975308
[00:35:06.082] Server  ERROR   Backend returned 400
[00:35:06.114] Server  ERROR   Backend returned 400
[00:35:06.117] Server  ERROR   Backend returned 400
[00:35:06.227] Browser WARN    Inference API failed, using simulation 400
[00:35:06.227] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7283401792875058
[00:35:06.227] Browser WARN    Inference API failed, using simulation 400
[00:35:06.227] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9296417030221847
[00:35:06.227] Browser WARN    Inference API failed, using simulation 400
[00:35:06.227] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9529269575196949
[00:35:06.591] Server  ERROR   Backend returned 400
[00:35:06.616] Server  ERROR   Backend returned 400
[00:35:06.633] Server  ERROR   Backend returned 400
[00:35:06.740] Browser WARN    Inference API failed, using simulation 400
[00:35:06.740] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.07944850260252362
[00:35:06.740] Browser WARN    Inference API failed, using simulation 400
[00:35:06.740] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.24556524094174453
[00:35:06.740] Browser WARN    Inference API failed, using simulation 400
[00:35:06.740] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.05645696103407
[00:35:07.102] Server  ERROR   Backend returned 400
[00:35:07.123] Server  ERROR   Backend returned 400
[00:35:07.137] Server  ERROR   Backend returned 400
[00:35:07.242] Browser WARN    Inference API failed, using simulation 400
[00:35:07.242] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4159235405147538
[00:35:07.242] Browser WARN    Inference API failed, using simulation 400
[00:35:07.242] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.044536812609262955
[00:35:07.242] Browser WARN    Inference API failed, using simulation 400
[00:35:07.242] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2876948448470669
[00:35:07.584] Server  ERROR   Backend returned 400
[00:35:07.611] Server  ERROR   Backend returned 400
[00:35:07.614] Server  ERROR   Backend returned 400
[00:35:07.731] Browser WARN    Inference API failed, using simulation 400
[00:35:07.731] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8527002738117978
[00:35:07.731] Browser WARN    Inference API failed, using simulation 400
[00:35:07.731] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3429906962809962
[00:35:07.731] Browser WARN    Inference API failed, using simulation 400
[00:35:07.731] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8075509892384635
[00:35:08.084] Server  ERROR   Backend returned 400
[00:35:08.123] Server  ERROR   Backend returned 400
[00:35:08.126] Server  ERROR   Backend returned 400
[00:35:08.234] Browser WARN    Inference API failed, using simulation 400
[00:35:08.234] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.20884063991704843
[00:35:08.234] Browser WARN    Inference API failed, using simulation 400
[00:35:08.234] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2987106296345764
[00:35:08.234] Browser WARN    Inference API failed, using simulation 400
[00:35:08.234] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3480381089096202
[00:35:08.590] Server  ERROR   Backend returned 400
[00:35:08.613] Server  ERROR   Backend returned 400
[00:35:08.638] Server  ERROR   Backend returned 400
[00:35:08.741] Browser WARN    Inference API failed, using simulation 400
[00:35:08.741] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.10849032191166769
[00:35:08.741] Browser WARN    Inference API failed, using simulation 400
[00:35:08.741] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.20774090488569286
[00:35:08.741] Browser WARN    Inference API failed, using simulation 400
[00:35:08.741] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4437327110833424
[00:35:09.094] Server  ERROR   Backend returned 400
[00:35:09.113] Server  ERROR   Backend returned 400
[00:35:09.128] Server  ERROR   Backend returned 400
[00:35:09.232] Browser WARN    Inference API failed, using simulation 400
[00:35:09.233] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9476307129930637
[00:35:09.233] Browser WARN    Inference API failed, using simulation 400
[00:35:09.233] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.42591111386885017
[00:35:09.233] Browser WARN    Inference API failed, using simulation 400
[00:35:09.233] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.42378753907544936
[00:35:09.585] Server  ERROR   Backend returned 400
[00:35:09.618] Server  ERROR   Backend returned 400
[00:35:09.650] Server  ERROR   Backend returned 400
[00:35:09.759] Browser WARN    Inference API failed, using simulation 400
[00:35:09.759] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.08437165786503126
[00:35:09.759] Browser WARN    Inference API failed, using simulation 400
[00:35:09.759] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.34258379960411095
[00:35:09.759] Browser WARN    Inference API failed, using simulation 400
[00:35:09.759] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8281691364132411
[00:35:10.090] Server  ERROR   Backend returned 400
[00:35:10.117] Server  ERROR   Backend returned 400
[00:35:10.131] Server  ERROR   Backend returned 400
[00:35:10.250] Browser WARN    Inference API failed, using simulation 400
[00:35:10.250] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8552547649074382
[00:35:10.250] Browser WARN    Inference API failed, using simulation 400
[00:35:10.250] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.05479410730536799
[00:35:10.250] Browser WARN    Inference API failed, using simulation 400
[00:35:10.250] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.21381100647926599
[00:35:10.613] Server  ERROR   Backend returned 400
[00:35:10.634] Server  ERROR   Backend returned 400
[00:35:10.653] Server  ERROR   Backend returned 400
[00:35:10.757] Browser WARN    Inference API failed, using simulation 400
[00:35:10.757] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9173085202093283
[00:35:10.757] Browser WARN    Inference API failed, using simulation 400
[00:35:10.757] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3267832213606762
[00:35:10.757] Browser WARN    Inference API failed, using simulation 400
[00:35:10.757] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.45459928300318325
[00:35:11.088] Server  ERROR   Backend returned 400
[00:35:11.118] Server  ERROR   Backend returned 400
[00:35:11.122] Server  ERROR   Backend returned 400
[00:35:11.231] Browser WARN    Inference API failed, using simulation 400
[00:35:11.232] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.26534399274071496
[00:35:11.232] Browser WARN    Inference API failed, using simulation 400
[00:35:11.232] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1711665312186873
[00:35:11.232] Browser WARN    Inference API failed, using simulation 400
[00:35:11.232] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.41573409561739144
[00:35:11.582] Server  ERROR   Backend returned 400
[00:35:11.611] Server  ERROR   Backend returned 400
[00:35:11.625] Server  ERROR   Backend returned 400
[00:35:11.734] Browser WARN    Inference API failed, using simulation 400
[00:35:11.734] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.49322178527326827
[00:35:11.734] Browser WARN    Inference API failed, using simulation 400
[00:35:11.734] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.16099896770588945
[00:35:11.734] Browser WARN    Inference API failed, using simulation 400
[00:35:11.734] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4279484033354151
[00:35:12.093] Server  ERROR   Backend returned 400
[00:35:12.112] Server  ERROR   Backend returned 400
[00:35:12.125] Server  ERROR   Backend returned 400
[00:35:12.230] Browser WARN    Inference API failed, using simulation 400
[00:35:12.230] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.09152090123251849
[00:35:12.230] Browser WARN    Inference API failed, using simulation 400
[00:35:12.230] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.17778026393366397
[00:35:12.230] Browser WARN    Inference API failed, using simulation 400
[00:35:12.230] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.17425009790080065
[00:35:12.584] Server  ERROR   Backend returned 400
[00:35:12.619] Server  ERROR   Backend returned 400
[00:35:12.623] Server  ERROR   Backend returned 400
[00:35:12.733] Browser WARN    Inference API failed, using simulation 400
[00:35:12.733] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3668120946137151
[00:35:12.733] Browser WARN    Inference API failed, using simulation 400
[00:35:12.733] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.37458202661688866
[00:35:12.733] Browser WARN    Inference API failed, using simulation 400
[00:35:12.733] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.02197885970613267
[00:35:13.083] Server  ERROR   Backend returned 400
[00:35:13.106] Server  ERROR   Backend returned 400
[00:35:13.129] Server  ERROR   Backend returned 400
[00:35:13.233] Browser WARN    Inference API failed, using simulation 400
[00:35:13.233] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7424958954530847
[00:35:13.233] Browser WARN    Inference API failed, using simulation 400
[00:35:13.233] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.40011227295119906
[00:35:13.233] Browser WARN    Inference API failed, using simulation 400
[00:35:13.233] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.02743208062473096
[00:35:13.581] Server  ERROR   Backend returned 400
[00:35:13.611] Server  ERROR   Backend returned 400
[00:35:13.627] Server  ERROR   Backend returned 400
[00:35:13.731] Browser WARN    Inference API failed, using simulation 400
[00:35:13.731] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1413416321449567
[00:35:13.731] Browser WARN    Inference API failed, using simulation 400
[00:35:13.731] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.15635618861733835
[00:35:13.731] Browser WARN    Inference API failed, using simulation 400
[00:35:13.731] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3938487196024709
[00:35:14.080] Server  ERROR   Backend returned 400
[00:35:14.114] Server  ERROR   Backend returned 400
[00:35:14.116] Server  ERROR   Backend returned 400
[00:35:14.225] Browser WARN    Inference API failed, using simulation 400
[00:35:14.225] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9055264433586344
[00:35:14.225] Browser WARN    Inference API failed, using simulation 400
[00:35:14.225] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7589762926178782
[00:35:14.225] Browser WARN    Inference API failed, using simulation 400
[00:35:14.225] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2511171053701336
[00:35:14.582] Server  ERROR   Backend returned 400
[00:35:14.607] Server  ERROR   Backend returned 400
[00:35:14.619] Server  ERROR   Backend returned 400
[00:35:14.725] Browser WARN    Inference API failed, using simulation 400
[00:35:14.725] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7681462104765218
[00:35:14.726] Browser WARN    Inference API failed, using simulation 400
[00:35:14.726] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3947943488988818
[00:35:14.726] Browser WARN    Inference API failed, using simulation 400
[00:35:14.726] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3078438977433968
[00:35:15.082] Server  ERROR   Backend returned 400
[00:35:15.103] Server  ERROR   Backend returned 400
[00:35:15.125] Server  ERROR   Backend returned 400
[00:35:15.229] Browser WARN    Inference API failed, using simulation 400
[00:35:15.229] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.39657195652574695
[00:35:15.229] Browser WARN    Inference API failed, using simulation 400
[00:35:15.229] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.08489515039760104
[00:35:15.229] Browser WARN    Inference API failed, using simulation 400
[00:35:15.229] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.13238837178119156
[00:35:15.593] Server  ERROR   Backend returned 400
[00:35:15.611] Server  ERROR   Backend returned 400
[00:35:15.629] Server  ERROR   Backend returned 400
[00:35:15.733] Browser WARN    Inference API failed, using simulation 400
[00:35:15.733] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.12449177308468229
[00:35:15.733] Browser WARN    Inference API failed, using simulation 400
[00:35:15.733] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.07640210226427413
[00:35:15.733] Browser WARN    Inference API failed, using simulation 400
[00:35:15.733] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8276578717890278
[00:35:16.085] Server  ERROR   Backend returned 400
[00:35:16.104] Server  ERROR   Backend returned 400
[00:35:16.118] Server  ERROR   Backend returned 400
[00:35:16.221] Browser WARN    Inference API failed, using simulation 400
[00:35:16.221] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7466169289028597
[00:35:16.221] Browser WARN    Inference API failed, using simulation 400
[00:35:16.221] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.42711322309685346
[00:35:16.221] Browser WARN    Inference API failed, using simulation 400
[00:35:16.221] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1254976985696455
[00:35:16.589] Server  ERROR   Backend returned 400
[00:35:16.611] Server  ERROR   Backend returned 400
[00:35:16.627] Server  ERROR   Backend returned 400
[00:35:16.730] Browser WARN    Inference API failed, using simulation 400
[00:35:16.730] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9826054804199353
[00:35:16.730] Browser WARN    Inference API failed, using simulation 400
[00:35:16.730] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9678129571899861
[00:35:16.730] Browser WARN    Inference API failed, using simulation 400
[00:35:16.730] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.18642393150509456
[00:35:17.094] Server  ERROR   Backend returned 400
[00:35:17.116] Server  ERROR   Backend returned 400
[00:35:17.139] Server  ERROR   Backend returned 400
[00:35:17.243] Browser WARN    Inference API failed, using simulation 400
[00:35:17.243] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.34084969789834985
[00:35:17.243] Browser WARN    Inference API failed, using simulation 400
[00:35:17.243] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.028627138661550366
[00:35:17.243] Browser WARN    Inference API failed, using simulation 400
[00:35:17.243] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8914322670232047
[00:35:17.582] Server  ERROR   Backend returned 400
[00:35:17.602] Server  ERROR   Backend returned 400
[00:35:17.623] Server  ERROR   Backend returned 400
[00:35:17.727] Browser WARN    Inference API failed, using simulation 400
[00:35:17.727] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9603123855028521
[00:35:17.727] Browser WARN    Inference API failed, using simulation 400
[00:35:17.727] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4906399225364068
[00:35:17.727] Browser WARN    Inference API failed, using simulation 400
[00:35:17.727] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2507078587064729
[00:35:18.103] Server  ERROR   Backend returned 400
[00:35:18.133] Server  ERROR   Backend returned 400
[00:35:18.140] Server  ERROR   Backend returned 400
[00:35:18.245] Browser WARN    Inference API failed, using simulation 400
[00:35:18.245] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7263001105097087
[00:35:18.245] Browser WARN    Inference API failed, using simulation 400
[00:35:18.245] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.11927554971624832
[00:35:18.245] Browser WARN    Inference API failed, using simulation 400
[00:35:18.245] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8293304523993131
[00:35:18.582] Server  ERROR   Backend returned 400
[00:35:18.600] Server  ERROR   Backend returned 400
[00:35:18.619] Server  ERROR   Backend returned 400
[00:35:18.726] Browser WARN    Inference API failed, using simulation 400
[00:35:18.726] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.08252202257577551
[00:35:18.726] Browser WARN    Inference API failed, using simulation 400
[00:35:18.726] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9126078313981538
[00:35:18.726] Browser WARN    Inference API failed, using simulation 400
[00:35:18.726] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7682856707973558
[00:35:19.085] Server  ERROR   Backend returned 400
[00:35:19.113] Server  ERROR   Backend returned 400
[00:35:19.116] Server  ERROR   Backend returned 400
[00:35:19.226] Browser WARN    Inference API failed, using simulation 400
[00:35:19.226] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.21409046210997312
[00:35:19.226] Browser WARN    Inference API failed, using simulation 400
[00:35:19.226] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4444071984760907
[00:35:19.226] Browser WARN    Inference API failed, using simulation 400
[00:35:19.226] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1918355374936298
[00:35:19.598] Server  ERROR   Backend returned 400
[00:35:19.617] Server  ERROR   Backend returned 400
[00:35:19.632] Server  ERROR   Backend returned 400
[00:35:19.738] Browser WARN    Inference API failed, using simulation 400
[00:35:19.738] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8642671931314257
[00:35:19.738] Browser WARN    Inference API failed, using simulation 400
[00:35:19.738] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.09277469479247424
[00:35:19.738] Browser WARN    Inference API failed, using simulation 400
[00:35:19.738] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.10607144512648548
[00:35:20.082] Server  ERROR   Backend returned 400
[00:35:20.116] Server  ERROR   Backend returned 400
[00:35:20.128] Server  ERROR   Backend returned 400
[00:35:20.238] Browser WARN    Inference API failed, using simulation 400
[00:35:20.238] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.44494203341757993
[00:35:20.238] Browser WARN    Inference API failed, using simulation 400
[00:35:20.238] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7283649811643832
[00:35:20.238] Browser WARN    Inference API failed, using simulation 400
[00:35:20.238] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.828187673852942
[00:35:20.580] Server  ERROR   Backend returned 400
[00:35:20.615] Server  ERROR   Backend returned 400
[00:35:20.617] Server  ERROR   Backend returned 400
[00:35:20.726] Browser WARN    Inference API failed, using simulation 400
[00:35:20.726] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.26993167439683646
[00:35:20.726] Browser WARN    Inference API failed, using simulation 400
[00:35:20.726] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4262191231858096
[00:35:20.726] Browser WARN    Inference API failed, using simulation 400
[00:35:20.726] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3107500201457845
[00:35:21.082] Server  ERROR   Backend returned 400
[00:35:21.110] Server  ERROR   Backend returned 400
[00:35:21.131] Server  ERROR   Backend returned 400
[00:35:21.259] Browser WARN    Inference API failed, using simulation 400
[00:35:21.259] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2364622556367919
[00:35:21.259] Browser WARN    Inference API failed, using simulation 400
[00:35:21.259] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4537032298643053
[00:35:21.259] Browser WARN    Inference API failed, using simulation 400
[00:35:21.259] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.044047335316272807
[00:35:21.583] Server  ERROR   Backend returned 400
[00:35:21.614] Server  ERROR   Backend returned 400
[00:35:21.627] Server  ERROR   Backend returned 400
[00:35:21.736] Browser WARN    Inference API failed, using simulation 400
[00:35:21.736] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.893706009201255
[00:35:21.736] Browser WARN    Inference API failed, using simulation 400
[00:35:21.736] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8428039359029966
[00:35:21.736] Browser WARN    Inference API failed, using simulation 400
[00:35:21.736] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2951399810374496
[00:35:22.088] Server  ERROR   Backend returned 400
[00:35:22.104] Server  ERROR   Backend returned 400
[00:35:22.128] Server  ERROR   Backend returned 400
[00:35:22.232] Browser WARN    Inference API failed, using simulation 400
[00:35:22.232] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.44786302656359794
[00:35:22.232] Browser WARN    Inference API failed, using simulation 400
[00:35:22.232] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8778390886529295
[00:35:22.232] Browser WARN    Inference API failed, using simulation 400
[00:35:22.232] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4380691916966507
[00:35:22.582] Server  ERROR   Backend returned 400
[00:35:22.601] Server  ERROR   Backend returned 400
[00:35:22.614] Server  ERROR   Backend returned 400
[00:35:22.718] Browser WARN    Inference API failed, using simulation 400
[00:35:22.718] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.0062570964411627905
[00:35:22.718] Browser WARN    Inference API failed, using simulation 400
[00:35:22.718] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.36386309876505324
[00:35:22.718] Browser WARN    Inference API failed, using simulation 400
[00:35:22.718] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7705295156135848
[00:35:23.082] Server  ERROR   Backend returned 400
[00:35:23.103] Server  ERROR   Backend returned 400
[00:35:23.117] Server  ERROR   Backend returned 400
[00:35:23.220] Browser WARN    Inference API failed, using simulation 400
[00:35:23.220] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.06078722512476842
[00:35:23.220] Browser WARN    Inference API failed, using simulation 400
[00:35:23.220] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8791468816374395
[00:35:23.220] Browser WARN    Inference API failed, using simulation 400
[00:35:23.220] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9194271138500112
[00:35:23.580] Server  ERROR   Backend returned 400
[00:35:23.617] Server  ERROR   Backend returned 400
[00:35:23.619] Server  ERROR   Backend returned 400
[00:35:23.731] Browser WARN    Inference API failed, using simulation 400
[00:35:23.731] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.27127633698793274
[00:35:23.731] Browser WARN    Inference API failed, using simulation 400
[00:35:23.731] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.15330414595401076
[00:35:23.731] Browser WARN    Inference API failed, using simulation 400
[00:35:23.731] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7007328953524442
[00:35:24.088] Server  ERROR   Backend returned 400
[00:35:24.111] Server  ERROR   Backend returned 400
[00:35:24.125] Server  ERROR   Backend returned 400
[00:35:24.238] Browser WARN    Inference API failed, using simulation 400
[00:35:24.238] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4830899835617727
[00:35:24.238] Browser WARN    Inference API failed, using simulation 400
[00:35:24.238] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.24768396954053723
[00:35:24.238] Browser WARN    Inference API failed, using simulation 400
[00:35:24.238] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7171265983707631
[00:35:24.588] Server  ERROR   Backend returned 400
[00:35:24.608] Server  ERROR   Backend returned 400
[00:35:24.630] Server  ERROR   Backend returned 400
[00:35:24.734] Browser WARN    Inference API failed, using simulation 400
[00:35:24.734] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.12318532022439144
[00:35:24.734] Browser WARN    Inference API failed, using simulation 400
[00:35:24.734] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.08775281928659434
[00:35:24.734] Browser WARN    Inference API failed, using simulation 400
[00:35:24.734] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.18724580516088046
[00:35:25.094] Server  ERROR   Backend returned 400
[00:35:25.117] Server  ERROR   Backend returned 400
[00:35:25.131] Server  ERROR   Backend returned 400
[00:35:25.236] Browser WARN    Inference API failed, using simulation 400
[00:35:25.236] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9008762086840241
[00:35:25.236] Browser WARN    Inference API failed, using simulation 400
[00:35:25.236] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.43675380775240547
[00:35:25.236] Browser WARN    Inference API failed, using simulation 400
[00:35:25.236] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9494951781237705
[00:35:25.583] Server  ERROR   Backend returned 400
[00:35:25.611] Server  ERROR   Backend returned 400
[00:35:25.624] Server  ERROR   Backend returned 400
[00:35:25.728] Browser WARN    Inference API failed, using simulation 400
[00:35:25.728] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.11977375550455172
[00:35:25.728] Browser WARN    Inference API failed, using simulation 400
[00:35:25.728] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.32253857567850597
[00:35:25.728] Browser WARN    Inference API failed, using simulation 400
[00:35:25.728] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2926485234679204
[00:35:26.076] Server  ERROR   Backend returned 400
[00:35:26.102] Server  ERROR   Backend returned 400
[00:35:26.104] Server  ERROR   Backend returned 400
[00:35:26.229] Browser WARN    Inference API failed, using simulation 400
[00:35:26.229] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.27485016847182875
[00:35:26.229] Browser WARN    Inference API failed, using simulation 400
[00:35:26.229] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4625082865956596
[00:35:26.229] Browser WARN    Inference API failed, using simulation 400
[00:35:26.229] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.16413867591103587
[00:35:26.581] Server  ERROR   Backend returned 400
[00:35:26.608] Server  ERROR   Backend returned 400
[00:35:26.629] Server  ERROR   Backend returned 400
[00:35:26.732] Browser WARN    Inference API failed, using simulation 400
[00:35:26.732] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9015916100569001
[00:35:26.732] Browser WARN    Inference API failed, using simulation 400
[00:35:26.732] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2761172579253732
[00:35:26.732] Browser WARN    Inference API failed, using simulation 400
[00:35:26.732] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.13052125525184743
[00:35:27.084] Server  ERROR   Backend returned 400
[00:35:27.116] Server  ERROR   Backend returned 400
[00:35:27.121] Server  ERROR   Backend returned 400
[00:35:27.230] Browser WARN    Inference API failed, using simulation 400
[00:35:27.230] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9206766781527941
[00:35:27.230] Browser WARN    Inference API failed, using simulation 400
[00:35:27.230] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.08635061914135916
[00:35:27.230] Browser WARN    Inference API failed, using simulation 400
[00:35:27.230] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9772027259265443
[00:35:27.579] Server  ERROR   Backend returned 400
[00:35:27.617] Server  ERROR   Backend returned 400
[00:35:27.621] Server  ERROR   Backend returned 400
[00:35:27.731] Browser WARN    Inference API failed, using simulation 400
[00:35:27.731] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1757945370532326
[00:35:27.731] Browser WARN    Inference API failed, using simulation 400
[00:35:27.731] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.18524385769686058
[00:35:27.731] Browser WARN    Inference API failed, using simulation 400
[00:35:27.731] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.40739679782170246
[00:35:28.078] Server  ERROR   Backend returned 400
[00:35:28.108] Server  ERROR   Backend returned 400
[00:35:28.110] Server  ERROR   Backend returned 400
[00:35:28.219] Browser WARN    Inference API failed, using simulation 400
[00:35:28.219] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3674518240389035
[00:35:28.219] Browser WARN    Inference API failed, using simulation 400
[00:35:28.219] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7682192192645486
[00:35:28.219] Browser WARN    Inference API failed, using simulation 400
[00:35:28.219] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4495286406617869
[00:35:28.581] Server  ERROR   Backend returned 400
[00:35:28.606] Server  ERROR   Backend returned 400
[00:35:28.609] Server  ERROR   Backend returned 400
[00:35:28.718] Browser WARN    Inference API failed, using simulation 400
[00:35:28.718] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.23344884228811524
[00:35:28.718] Browser WARN    Inference API failed, using simulation 400
[00:35:28.718] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8296454024080075
[00:35:28.718] Browser WARN    Inference API failed, using simulation 400
[00:35:28.718] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9633958787716062
[00:35:29.081] Server  ERROR   Backend returned 400
[00:35:29.103] Server  ERROR   Backend returned 400
[00:35:29.121] Server  ERROR   Backend returned 400
[00:35:29.224] Browser WARN    Inference API failed, using simulation 400
[00:35:29.224] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2507163326503186
[00:35:29.224] Browser WARN    Inference API failed, using simulation 400
[00:35:29.224] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.306304137020148
[00:35:29.224] Browser WARN    Inference API failed, using simulation 400
[00:35:29.224] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.42478971641349655
[00:35:29.582] Server  ERROR   Backend returned 400
[00:35:29.613] Server  ERROR   Backend returned 400
[00:35:29.624] Server  ERROR   Backend returned 400
[00:35:29.730] Browser WARN    Inference API failed, using simulation 400
[00:35:29.730] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3055459975793697
[00:35:29.730] Browser WARN    Inference API failed, using simulation 400
[00:35:29.730] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9325326059966654
[00:35:29.730] Browser WARN    Inference API failed, using simulation 400
[00:35:29.730] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8245233379853162
[00:35:30.080] Server  ERROR   Backend returned 400
[00:35:30.128] Server  ERROR   Backend returned 400
[00:35:30.134] Server  ERROR   Backend returned 400
[00:35:30.245] Browser WARN    Inference API failed, using simulation 400
[00:35:30.245] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.730432442919418
[00:35:30.245] Browser WARN    Inference API failed, using simulation 400
[00:35:30.245] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.12287479778420474
[00:35:30.245] Browser WARN    Inference API failed, using simulation 400
[00:35:30.245] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8618831370701795
[00:35:30.580] Server  ERROR   Backend returned 400
[00:35:30.617] Server  ERROR   Backend returned 400
[00:35:30.619] Server  ERROR   Backend returned 400
[00:35:30.729] Browser WARN    Inference API failed, using simulation 400
[00:35:30.729] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.23591288123731513
[00:35:30.729] Browser WARN    Inference API failed, using simulation 400
[00:35:30.729] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3044959404106416
[00:35:30.729] Browser WARN    Inference API failed, using simulation 400
[00:35:30.729] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.25420650566585923
[00:35:31.078] Server  ERROR   Backend returned 400
[00:35:31.099] Server  ERROR   Backend returned 400
[00:35:31.118] Server  ERROR   Backend returned 400
[00:35:31.222] Browser WARN    Inference API failed, using simulation 400
[00:35:31.222] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.20397037984264355
[00:35:31.222] Browser WARN    Inference API failed, using simulation 400
[00:35:31.222] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9705513763898745
[00:35:31.222] Browser WARN    Inference API failed, using simulation 400
[00:35:31.222] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2609278409710427
[00:35:31.597] Server  ERROR   Backend returned 400
[00:35:31.618] Server  ERROR   Backend returned 400
[00:35:31.636] Server  ERROR   Backend returned 400
[00:35:31.740] Browser WARN    Inference API failed, using simulation 400
[00:35:31.740] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.44777873514364486
[00:35:31.740] Browser WARN    Inference API failed, using simulation 400
[00:35:31.740] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.364899719174586
[00:35:31.740] Browser WARN    Inference API failed, using simulation 400
[00:35:31.740] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8147400975985652
[00:35:32.085] Server  ERROR   Backend returned 400
[00:35:32.105] Server  ERROR   Backend returned 400
[00:35:32.118] Server  ERROR   Backend returned 400
[00:35:32.222] Browser WARN    Inference API failed, using simulation 400
[00:35:32.222] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8496008228992235
[00:35:32.222] Browser WARN    Inference API failed, using simulation 400
[00:35:32.222] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8711097721520329
[00:35:32.222] Browser WARN    Inference API failed, using simulation 400
[00:35:32.222] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8024669796651049
[00:35:32.581] Server  ERROR   Backend returned 400
[00:35:32.611] Server  ERROR   Backend returned 400
[00:35:32.624] Server  ERROR   Backend returned 400
[00:35:32.728] Browser WARN    Inference API failed, using simulation 400
[00:35:32.728] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8107974541716798
[00:35:32.728] Browser WARN    Inference API failed, using simulation 400
[00:35:32.728] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.42514945519945685
[00:35:32.728] Browser WARN    Inference API failed, using simulation 400
[00:35:32.728] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.043758741410752144
[00:35:33.086] Server  ERROR   Backend returned 400
[00:35:33.105] Server  ERROR   Backend returned 400
[00:35:33.118] Server  ERROR   Backend returned 400
[00:35:33.222] Browser WARN    Inference API failed, using simulation 400
[00:35:33.222] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.48552725638240357
[00:35:33.222] Browser WARN    Inference API failed, using simulation 400
[00:35:33.222] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.18588855359757261
[00:35:33.222] Browser WARN    Inference API failed, using simulation 400
[00:35:33.222] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.36304894930814186
[00:35:33.584] Server  ERROR   Backend returned 400
[00:35:33.605] Server  ERROR   Backend returned 400
[00:35:33.621] Server  ERROR   Backend returned 400
[00:35:33.724] Browser WARN    Inference API failed, using simulation 400
[00:35:33.724] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2186993918286816
[00:35:33.724] Browser WARN    Inference API failed, using simulation 400
[00:35:33.724] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.00319812043532014
[00:35:33.724] Browser WARN    Inference API failed, using simulation 400
[00:35:33.724] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8693833595282122
[00:35:34.076] Server  ERROR   Backend returned 400
[00:35:34.102] Server  ERROR   Backend returned 400
[00:35:34.116] Server  ERROR   Backend returned 400
[00:35:34.220] Browser WARN    Inference API failed, using simulation 400
[00:35:34.220] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.15263727227369395
[00:35:34.220] Browser WARN    Inference API failed, using simulation 400
[00:35:34.220] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.36198453980995077
[00:35:34.220] Browser WARN    Inference API failed, using simulation 400
[00:35:34.220] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4737501788750015
[00:35:34.580] Server  ERROR   Backend returned 400
[00:35:34.598] Server  ERROR   Backend returned 400
[00:35:34.612] Server  ERROR   Backend returned 400
[00:35:34.719] Browser WARN    Inference API failed, using simulation 400
[00:35:34.719] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.25387303217886165
[00:35:34.719] Browser WARN    Inference API failed, using simulation 400
[00:35:34.719] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9527118249123955
[00:35:34.719] Browser WARN    Inference API failed, using simulation 400
[00:35:34.719] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9462502065684362
[00:35:35.086] Server  ERROR   Backend returned 400
[00:35:35.114] Server  ERROR   Backend returned 400
[00:35:35.120] Server  ERROR   Backend returned 400
[00:35:35.226] Browser WARN    Inference API failed, using simulation 400
[00:35:35.226] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.40419777830607573
[00:35:35.226] Browser WARN    Inference API failed, using simulation 400
[00:35:35.226] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.020286384396450696
[00:35:35.226] Browser WARN    Inference API failed, using simulation 400
[00:35:35.226] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.48851809536511726
[00:35:35.582] Server  ERROR   Backend returned 400
[00:35:35.615] Server  ERROR   Backend returned 400
[00:35:35.634] Server  ERROR   Backend returned 400
[00:35:35.739] Browser WARN    Inference API failed, using simulation 400
[00:35:35.739] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.34161434939054214
[00:35:35.739] Browser WARN    Inference API failed, using simulation 400
[00:35:35.739] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.032026418607287555
[00:35:35.739] Browser WARN    Inference API failed, using simulation 400
[00:35:35.739] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4303713112226949
[00:35:36.084] Server  ERROR   Backend returned 400
[00:35:36.104] Server  ERROR   Backend returned 400
[00:35:36.118] Server  ERROR   Backend returned 400
[00:35:36.222] Browser WARN    Inference API failed, using simulation 400
[00:35:36.222] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.021498256365956758
[00:35:36.222] Browser WARN    Inference API failed, using simulation 400
[00:35:36.222] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7928862335023278
[00:35:36.222] Browser WARN    Inference API failed, using simulation 400
[00:35:36.222] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.03256365279452522
[00:35:36.583] Server  ERROR   Backend returned 400
[00:35:36.603] Server  ERROR   Backend returned 400
[00:35:36.614] Server  ERROR   Backend returned 400
[00:35:36.726] Browser WARN    Inference API failed, using simulation 400
[00:35:36.726] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.44666737506226784
[00:35:36.726] Browser WARN    Inference API failed, using simulation 400
[00:35:36.726] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8691690710227994
[00:35:36.726] Browser WARN    Inference API failed, using simulation 400
[00:35:36.726] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.768508552770375
[00:35:37.082] Server  ERROR   Backend returned 400
[00:35:37.101] Server  ERROR   Backend returned 400
[00:35:37.112] Server  ERROR   Backend returned 400
[00:35:37.217] Browser WARN    Inference API failed, using simulation 400
[00:35:37.217] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.012194358864775712
[00:35:37.217] Browser WARN    Inference API failed, using simulation 400
[00:35:37.217] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8352472719021851
[00:35:37.217] Browser WARN    Inference API failed, using simulation 400
[00:35:37.217] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.36945329766508056
[00:35:37.580] Server  ERROR   Backend returned 400
[00:35:37.598] Server  ERROR   Backend returned 400
[00:35:37.612] Server  ERROR   Backend returned 400
[00:35:37.731] Browser WARN    Inference API failed, using simulation 400
[00:35:37.731] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.12949609385408634
[00:35:37.731] Browser WARN    Inference API failed, using simulation 400
[00:35:37.731] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7074347424389179
[00:35:37.731] Browser WARN    Inference API failed, using simulation 400
[00:35:37.731] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9435706149383074
[00:35:38.078] Server  ERROR   Backend returned 400
[00:35:38.103] Server  ERROR   Backend returned 400
[00:35:38.116] Server  ERROR   Backend returned 400
[00:35:38.219] Browser WARN    Inference API failed, using simulation 400
[00:35:38.219] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8074792899159056
[00:35:38.219] Browser WARN    Inference API failed, using simulation 400
[00:35:38.219] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1857481749107659
[00:35:38.219] Browser WARN    Inference API failed, using simulation 400
[00:35:38.219] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.30846542664418186
[00:35:38.583] Server  ERROR   Backend returned 400
[00:35:38.610] Server  ERROR   Backend returned 400
[00:35:38.624] Server  ERROR   Backend returned 400
[00:35:38.729] Browser WARN    Inference API failed, using simulation 400
[00:35:38.729] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.292813367407445
[00:35:38.729] Browser WARN    Inference API failed, using simulation 400
[00:35:38.729] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.22207401988750913
[00:35:38.729] Browser WARN    Inference API failed, using simulation 400
[00:35:38.729] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.28495421129643694
[00:35:39.097] Server  ERROR   Backend returned 400
[00:35:39.121] Server  ERROR   Backend returned 400
[00:35:39.137] Server  ERROR   Backend returned 400
[00:35:39.243] Browser WARN    Inference API failed, using simulation 400
[00:35:39.243] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7760915728701332
[00:35:39.243] Browser WARN    Inference API failed, using simulation 400
[00:35:39.243] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8946524497604498
[00:35:39.243] Browser WARN    Inference API failed, using simulation 400
[00:35:39.243] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8075816027980035
[00:35:39.579] Server  ERROR   Backend returned 400
[00:35:39.604] Server  ERROR   Backend returned 400
[00:35:39.620] Server  ERROR   Backend returned 400
[00:35:39.725] Browser WARN    Inference API failed, using simulation 400
[00:35:39.725] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1397967836501181
[00:35:39.725] Browser WARN    Inference API failed, using simulation 400
[00:35:39.725] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.0832195780827551
[00:35:39.725] Browser WARN    Inference API failed, using simulation 400
[00:35:39.725] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.20587086004468452
[00:35:40.095] Server  ERROR   Backend returned 400
[00:35:40.113] Server  ERROR   Backend returned 400
[00:35:40.127] Server  ERROR   Backend returned 400
[00:35:40.232] Browser WARN    Inference API failed, using simulation 400
[00:35:40.232] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4634280737493983
[00:35:40.232] Browser WARN    Inference API failed, using simulation 400
[00:35:40.232] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3687122976743174
[00:35:40.232] Browser WARN    Inference API failed, using simulation 400
[00:35:40.232] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.11132351718582001
[00:35:40.587] Server  ERROR   Backend returned 400
[00:35:40.609] Server  ERROR   Backend returned 400
[00:35:40.623] Server  ERROR   Backend returned 400
[00:35:40.728] Browser WARN    Inference API failed, using simulation 400
[00:35:40.728] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.33619906901282914
[00:35:40.728] Browser WARN    Inference API failed, using simulation 400
[00:35:40.728] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.86024862072188
[00:35:40.728] Browser WARN    Inference API failed, using simulation 400
[00:35:40.728] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.43696207101687035
[00:35:41.108] Server  ERROR   Backend returned 400
[00:35:41.133] Server  ERROR   Backend returned 400
[00:35:41.149] Server  ERROR   Backend returned 400
[00:35:41.254] Browser WARN    Inference API failed, using simulation 400
[00:35:41.254] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7738572697377939
[00:35:41.254] Browser WARN    Inference API failed, using simulation 400
[00:35:41.254] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.17025732244144465
[00:35:41.254] Browser WARN    Inference API failed, using simulation 400
[00:35:41.254] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8083515653946997
[00:35:41.579] Server  ERROR   Backend returned 400
[00:35:41.604] Server  ERROR   Backend returned 400
[00:35:41.619] Server  ERROR   Backend returned 400
[00:35:41.723] Browser WARN    Inference API failed, using simulation 400
[00:35:41.723] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.32074227864471005
[00:35:41.723] Browser WARN    Inference API failed, using simulation 400
[00:35:41.723] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8030677130034701
[00:35:41.723] Browser WARN    Inference API failed, using simulation 400
[00:35:41.723] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9195830773659052
[00:35:42.090] Server  ERROR   Backend returned 400
[00:35:42.104] Server  ERROR   Backend returned 400
[00:35:42.118] Server  ERROR   Backend returned 400
[00:35:42.222] Browser WARN    Inference API failed, using simulation 400
[00:35:42.222] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.430664944895939
[00:35:42.222] Browser WARN    Inference API failed, using simulation 400
[00:35:42.222] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.05131032591684315
[00:35:42.222] Browser WARN    Inference API failed, using simulation 400
[00:35:42.222] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7914253157112964
[00:35:42.582] Server  ERROR   Backend returned 400
[00:35:42.603] Server  ERROR   Backend returned 400
[00:35:42.625] Server  ERROR   Backend returned 400
[00:35:42.730] Browser WARN    Inference API failed, using simulation 400
[00:35:42.730] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4672964765840985
[00:35:42.730] Browser WARN    Inference API failed, using simulation 400
[00:35:42.730] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8222397992764163
[00:35:42.730] Browser WARN    Inference API failed, using simulation 400
[00:35:42.730] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.32724633290746713
[00:35:43.081] Server  ERROR   Backend returned 400
[00:35:43.098] Server  ERROR   Backend returned 400
[00:35:43.110] Server  ERROR   Backend returned 400
[00:35:43.215] Browser WARN    Inference API failed, using simulation 400
[00:35:43.216] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3400308649689535
[00:35:43.216] Browser WARN    Inference API failed, using simulation 400
[00:35:43.216] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2915219687961085
[00:35:43.216] Browser WARN    Inference API failed, using simulation 400
[00:35:43.216] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.08449646465173699
[00:35:43.581] Server  ERROR   Backend returned 400
[00:35:43.599] Server  ERROR   Backend returned 400
[00:35:43.612] Server  ERROR   Backend returned 400
[00:35:43.717] Browser WARN    Inference API failed, using simulation 400
[00:35:43.717] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.44298537942500626
[00:35:43.717] Browser WARN    Inference API failed, using simulation 400
[00:35:43.717] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.37539245277746974
[00:35:43.717] Browser WARN    Inference API failed, using simulation 400
[00:35:43.717] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.33733757242390067
[00:35:44.085] Server  ERROR   Backend returned 400
[00:35:44.102] Server  ERROR   Backend returned 400
[00:35:44.115] Server  ERROR   Backend returned 400
[00:35:44.219] Browser WARN    Inference API failed, using simulation 400
[00:35:44.219] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7539982683927444
[00:35:44.219] Browser WARN    Inference API failed, using simulation 400
[00:35:44.219] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.18504978107202003
[00:35:44.219] Browser WARN    Inference API failed, using simulation 400
[00:35:44.219] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.048586286785938626
[00:35:44.577] Server  ERROR   Backend returned 400
[00:35:44.603] Server  ERROR   Backend returned 400
[00:35:44.616] Server  ERROR   Backend returned 400
[00:35:44.727] Browser WARN    Inference API failed, using simulation 400
[00:35:44.727] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9430040469617795
[00:35:44.727] Browser WARN    Inference API failed, using simulation 400
[00:35:44.727] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.04632212592035373
[00:35:44.727] Browser WARN    Inference API failed, using simulation 400
[00:35:44.727] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.35051544751561264
[00:35:45.076] Server  ERROR   Backend returned 400
[00:35:45.102] Server  ERROR   Backend returned 400
[00:35:45.118] Server  ERROR   Backend returned 400
[00:35:45.222] Browser WARN    Inference API failed, using simulation 400
[00:35:45.222] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8478778170019147
[00:35:45.222] Browser WARN    Inference API failed, using simulation 400
[00:35:45.222] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3818449086968509
[00:35:45.222] Browser WARN    Inference API failed, using simulation 400
[00:35:45.222] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3809065877453864
[00:35:45.579] Server  ERROR   Backend returned 400
[00:35:45.602] Server  ERROR   Backend returned 400
[00:35:45.615] Server  ERROR   Backend returned 400
[00:35:45.724] Browser WARN    Inference API failed, using simulation 400
[00:35:45.724] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.445149698012409
[00:35:45.724] Browser WARN    Inference API failed, using simulation 400
[00:35:45.724] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9215750560732479
[00:35:45.724] Browser WARN    Inference API failed, using simulation 400
[00:35:45.724] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3672209468595715
[00:35:46.086] Server  ERROR   Backend returned 400
[00:35:46.106] Server  ERROR   Backend returned 400
[00:35:46.124] Server  ERROR   Backend returned 400
[00:35:46.228] Browser WARN    Inference API failed, using simulation 400
[00:35:46.228] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8252682261029722
[00:35:46.228] Browser WARN    Inference API failed, using simulation 400
[00:35:46.228] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.24472387731591
[00:35:46.228] Browser WARN    Inference API failed, using simulation 400
[00:35:46.228] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7842273734692928
[00:35:46.579] Server  ERROR   Backend returned 400
[00:35:46.600] Server  ERROR   Backend returned 400
[00:35:46.615] Server  ERROR   Backend returned 400
[00:35:46.719] Browser WARN    Inference API failed, using simulation 400
[00:35:46.719] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8554791953909883
[00:35:46.719] Browser WARN    Inference API failed, using simulation 400
[00:35:46.719] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.12398419853733028
[00:35:46.719] Browser WARN    Inference API failed, using simulation 400
[00:35:46.719] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.17061033802065562
[00:35:47.077] Server  ERROR   Backend returned 400
[00:35:47.094] Server  ERROR   Backend returned 400
[00:35:47.106] Server  ERROR   Backend returned 400
[00:35:47.210] Browser WARN    Inference API failed, using simulation 400
[00:35:47.210] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9193143111592186
[00:35:47.210] Browser WARN    Inference API failed, using simulation 400
[00:35:47.210] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.29532453250814944
[00:35:47.210] Browser WARN    Inference API failed, using simulation 400
[00:35:47.210] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9859530293270955
[00:35:47.579] Server  ERROR   Backend returned 400
[00:35:47.610] Server  ERROR   Backend returned 400
[00:35:47.626] Server  ERROR   Backend returned 400
[00:35:47.733] Browser WARN    Inference API failed, using simulation 400
[00:35:47.733] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.37076553326630723
[00:35:47.733] Browser WARN    Inference API failed, using simulation 400
[00:35:47.733] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4761108161768585
[00:35:47.733] Browser WARN    Inference API failed, using simulation 400
[00:35:47.733] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4095607580101553
[00:35:48.082] Server  ERROR   Backend returned 400
[00:35:48.100] Server  ERROR   Backend returned 400
[00:35:48.117] Server  ERROR   Backend returned 400
[00:35:48.230] Browser WARN    Inference API failed, using simulation 400
[00:35:48.230] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.11335662555513693
[00:35:48.230] Browser WARN    Inference API failed, using simulation 400
[00:35:48.230] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.19208873176531466
[00:35:48.230] Browser WARN    Inference API failed, using simulation 400
[00:35:48.230] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7777354194100142
[00:35:48.588] Server  ERROR   Backend returned 400
[00:35:48.606] Server  ERROR   Backend returned 400
[00:35:48.618] Server  ERROR   Backend returned 400
[00:35:48.729] Browser WARN    Inference API failed, using simulation 400
[00:35:48.729] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9495738710874374
[00:35:48.729] Browser WARN    Inference API failed, using simulation 400
[00:35:48.729] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.011852607841700225
[00:35:48.729] Browser WARN    Inference API failed, using simulation 400
[00:35:48.729] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.23303345138687848
[00:35:49.080] Server  ERROR   Backend returned 400
[00:35:49.095] Server  ERROR   Backend returned 400
[00:35:49.108] Server  ERROR   Backend returned 400
[00:35:49.225] Browser WARN    Inference API failed, using simulation 400
[00:35:49.225] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8449869180206602
[00:35:49.225] Browser WARN    Inference API failed, using simulation 400
[00:35:49.225] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.15199432613548863
[00:35:49.225] Browser WARN    Inference API failed, using simulation 400
[00:35:49.225] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.20651935535999627
[00:35:49.581] Server  ERROR   Backend returned 400
[00:35:49.604] Server  ERROR   Backend returned 400
[00:35:49.618] Server  ERROR   Backend returned 400
[00:35:49.726] Browser WARN    Inference API failed, using simulation 400
[00:35:49.726] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.15200467304742654
[00:35:49.726] Browser WARN    Inference API failed, using simulation 400
[00:35:49.726] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.008186653423474577
[00:35:49.726] Browser WARN    Inference API failed, using simulation 400
[00:35:49.726] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.009982722879522954
[00:35:50.083] Server  ERROR   Backend returned 400
[00:35:50.096] Server  ERROR   Backend returned 400
[00:35:50.110] Server  ERROR   Backend returned 400
[00:35:50.223] Browser WARN    Inference API failed, using simulation 400
[00:35:50.223] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.41819955452047547
[00:35:50.223] Browser WARN    Inference API failed, using simulation 400
[00:35:50.223] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.05293758823062583
[00:35:50.223] Browser WARN    Inference API failed, using simulation 400
[00:35:50.223] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8433148461189588
[00:35:50.578] Server  ERROR   Backend returned 400
[00:35:50.600] Server  ERROR   Backend returned 400
[00:35:50.607] Server  ERROR   Backend returned 400
[00:35:50.721] Browser WARN    Inference API failed, using simulation 400
[00:35:50.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.17185364190894842
[00:35:50.721] Browser WARN    Inference API failed, using simulation 400
[00:35:50.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1539010855782943
[00:35:50.721] Browser WARN    Inference API failed, using simulation 400
[00:35:50.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.24527157936879174
[00:35:51.079] Server  ERROR   Backend returned 400
[00:35:51.099] Server  ERROR   Backend returned 400
[00:35:51.111] Server  ERROR   Backend returned 400
[00:35:51.220] Browser WARN    Inference API failed, using simulation 400
[00:35:51.220] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3211315400523791
[00:35:51.220] Browser WARN    Inference API failed, using simulation 400
[00:35:51.220] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4326401069075884
[00:35:51.220] Browser WARN    Inference API failed, using simulation 400
[00:35:51.220] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.07973126991474166
[00:35:51.579] Server  ERROR   Backend returned 400
[00:35:51.602] Server  ERROR   Backend returned 400
[00:35:51.618] Server  ERROR   Backend returned 400
[00:35:51.722] Browser WARN    Inference API failed, using simulation 400
[00:35:51.722] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.999106953913677
[00:35:51.722] Browser WARN    Inference API failed, using simulation 400
[00:35:51.722] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.16081799972045796
[00:35:51.722] Browser WARN    Inference API failed, using simulation 400
[00:35:51.722] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8757972132583143
[00:35:52.079] Server  ERROR   Backend returned 400
[00:35:52.093] Server  ERROR   Backend returned 400
[00:35:52.103] Server  ERROR   Backend returned 400
[00:35:52.208] Browser WARN    Inference API failed, using simulation 400
[00:35:52.208] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4492991383993954
[00:35:52.208] Browser WARN    Inference API failed, using simulation 400
[00:35:52.208] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.16141083659317307
[00:35:52.208] Browser WARN    Inference API failed, using simulation 400
[00:35:52.208] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7035540189235675
[00:35:52.579] Server  ERROR   Backend returned 400
[00:35:52.594] Server  ERROR   Backend returned 400
[00:35:52.607] Server  ERROR   Backend returned 400
[00:35:52.710] Browser WARN    Inference API failed, using simulation 400
[00:35:52.710] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.27901260678960715
[00:35:52.710] Browser WARN    Inference API failed, using simulation 400
[00:35:52.710] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2555790709679237
[00:35:52.710] Browser WARN    Inference API failed, using simulation 400
[00:35:52.710] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3487371836981138
[00:35:53.080] Server  ERROR   Backend returned 400
[00:35:53.095] Server  ERROR   Backend returned 400
[00:35:53.109] Server  ERROR   Backend returned 400
[00:35:53.214] Browser WARN    Inference API failed, using simulation 400
[00:35:53.214] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.24996984061770794
[00:35:53.214] Browser WARN    Inference API failed, using simulation 400
[00:35:53.214] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8227254124478584
[00:35:53.214] Browser WARN    Inference API failed, using simulation 400
[00:35:53.214] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8156999854466567
[00:35:53.579] Server  ERROR   Backend returned 400
[00:35:53.608] Server  ERROR   Backend returned 400
[00:35:53.622] Server  ERROR   Backend returned 400
[00:35:53.727] Browser WARN    Inference API failed, using simulation 400
[00:35:53.727] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9377774072505244
[00:35:53.727] Browser WARN    Inference API failed, using simulation 400
[00:35:53.727] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.08530421861654436
[00:35:53.727] Browser WARN    Inference API failed, using simulation 400
[00:35:53.727] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4825075066613774
[00:35:54.078] Server  ERROR   Backend returned 400
[00:35:54.093] Server  ERROR   Backend returned 400
[00:35:54.103] Server  ERROR   Backend returned 400
[00:35:54.210] Browser WARN    Inference API failed, using simulation 400
[00:35:54.210] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.13389540895709418
[00:35:54.210] Browser WARN    Inference API failed, using simulation 400
[00:35:54.210] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.11167874328507571
[00:35:54.210] Browser WARN    Inference API failed, using simulation 400
[00:35:54.210] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3113118826575688
[00:35:54.582] Server  ERROR   Backend returned 400
[00:35:54.602] Server  ERROR   Backend returned 400
[00:35:54.628] Server  ERROR   Backend returned 400
[00:35:54.734] Browser WARN    Inference API failed, using simulation 400
[00:35:54.734] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4446486682551715
[00:35:54.734] Browser WARN    Inference API failed, using simulation 400
[00:35:54.734] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8321535746974136
[00:35:54.734] Browser WARN    Inference API failed, using simulation 400
[00:35:54.734] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4004571313639522
[00:35:55.077] Server  ERROR   Backend returned 400
[00:35:55.094] Server  ERROR   Backend returned 400
[00:35:55.104] Server  ERROR   Backend returned 400
[00:35:55.210] Browser WARN    Inference API failed, using simulation 400
[00:35:55.210] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3114593737183897
[00:35:55.210] Browser WARN    Inference API failed, using simulation 400
[00:35:55.210] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.12031323238389696
[00:35:55.210] Browser WARN    Inference API failed, using simulation 400
[00:35:55.210] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1883257620697913
[00:35:55.582] Server  ERROR   Backend returned 400
[00:35:55.602] Server  ERROR   Backend returned 400
[00:35:55.616] Server  ERROR   Backend returned 400
[00:35:55.719] Browser WARN    Inference API failed, using simulation 400
[00:35:55.719] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4994458647277804
[00:35:55.719] Browser WARN    Inference API failed, using simulation 400
[00:35:55.719] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8529524509087079
[00:35:55.719] Browser WARN    Inference API failed, using simulation 400
[00:35:55.719] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.20254828976273886
[00:35:56.090] Server  ERROR   Backend returned 400
[00:35:56.117] Server  ERROR   Backend returned 400
[00:35:56.130] Server  ERROR   Backend returned 400
[00:35:56.233] Browser WARN    Inference API failed, using simulation 400
[00:35:56.233] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7930547455808282
[00:35:56.233] Browser WARN    Inference API failed, using simulation 400
[00:35:56.233] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.98369335695666
[00:35:56.233] Browser WARN    Inference API failed, using simulation 400
[00:35:56.233] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.33530081012931123
[00:35:56.578] Server  ERROR   Backend returned 400
[00:35:56.604] Server  ERROR   Backend returned 400
[00:35:56.608] Server  ERROR   Backend returned 400
[00:35:56.718] Browser WARN    Inference API failed, using simulation 400
[00:35:56.718] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7610731442243128
[00:35:56.718] Browser WARN    Inference API failed, using simulation 400
[00:35:56.718] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.044959123500074805
[00:35:56.718] Browser WARN    Inference API failed, using simulation 400
[00:35:56.718] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.07632919914681252
[00:35:57.082] Server  ERROR   Backend returned 400
[00:35:57.103] Server  ERROR   Backend returned 400
[00:35:57.115] Server  ERROR   Backend returned 400
[00:35:57.221] Browser WARN    Inference API failed, using simulation 400
[00:35:57.221] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.11587862809789273
[00:35:57.221] Browser WARN    Inference API failed, using simulation 400
[00:35:57.221] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.41226584348419326
[00:35:57.221] Browser WARN    Inference API failed, using simulation 400
[00:35:57.221] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.21969021443500508
[00:35:57.586] Server  ERROR   Backend returned 400
[00:35:57.605] Server  ERROR   Backend returned 400
[00:35:57.618] Server  ERROR   Backend returned 400
[00:35:57.722] Browser WARN    Inference API failed, using simulation 400
[00:35:57.722] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7468986905358469
[00:35:57.722] Browser WARN    Inference API failed, using simulation 400
[00:35:57.722] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8537518024974144
[00:35:57.722] Browser WARN    Inference API failed, using simulation 400
[00:35:57.722] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.860610760756696
[00:35:58.081] Server  ERROR   Backend returned 400
[00:35:58.100] Server  ERROR   Backend returned 400
[00:35:58.121] Server  ERROR   Backend returned 400
[00:35:58.224] Browser WARN    Inference API failed, using simulation 400
[00:35:58.224] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.08665242499321196
[00:35:58.224] Browser WARN    Inference API failed, using simulation 400
[00:35:58.224] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.889753520405316
[00:35:58.224] Browser WARN    Inference API failed, using simulation 400
[00:35:58.224] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7237473837876531
[00:35:58.578] Server  ERROR   Backend returned 400
[00:35:58.604] Server  ERROR   Backend returned 400
[00:35:58.618] Server  ERROR   Backend returned 400
[00:35:58.722] Browser WARN    Inference API failed, using simulation 400
[00:35:58.722] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.43479853359213744
[00:35:58.722] Browser WARN    Inference API failed, using simulation 400
[00:35:58.722] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.17236485910182858
[00:35:58.722] Browser WARN    Inference API failed, using simulation 400
[00:35:58.722] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7384630759466371
[00:35:59.080] Server  ERROR   Backend returned 400
[00:35:59.105] Server  ERROR   Backend returned 400
[00:35:59.116] Server  ERROR   Backend returned 400
[00:35:59.221] Browser WARN    Inference API failed, using simulation 400
[00:35:59.221] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4709393770464572
[00:35:59.221] Browser WARN    Inference API failed, using simulation 400
[00:35:59.221] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.10717066587427404
[00:35:59.221] Browser WARN    Inference API failed, using simulation 400
[00:35:59.221] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9632426382627055
[00:35:59.580] Server  ERROR   Backend returned 400
[00:35:59.604] Server  ERROR   Backend returned 400
[00:35:59.619] Server  ERROR   Backend returned 400
[00:35:59.722] Browser WARN    Inference API failed, using simulation 400
[00:35:59.722] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1889579970170424
[00:35:59.722] Browser WARN    Inference API failed, using simulation 400
[00:35:59.722] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9382975956831836
[00:35:59.722] Browser WARN    Inference API failed, using simulation 400
[00:35:59.722] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3618280786463351
[00:36:00.080] Server  ERROR   Backend returned 400
[00:36:00.100] Server  ERROR   Backend returned 400
[00:36:00.116] Server  ERROR   Backend returned 400
[00:36:00.221] Browser WARN    Inference API failed, using simulation 400
[00:36:00.221] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.20835472762672747
[00:36:00.221] Browser WARN    Inference API failed, using simulation 400
[00:36:00.221] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1253599619957984
[00:36:00.221] Browser WARN    Inference API failed, using simulation 400
[00:36:00.221] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.17901823522844995
[00:36:00.583] Server  ERROR   Backend returned 400
[00:36:00.602] Server  ERROR   Backend returned 400
[00:36:00.616] Server  ERROR   Backend returned 400
[00:36:00.719] Browser WARN    Inference API failed, using simulation 400
[00:36:00.719] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.14606512586582088
[00:36:00.719] Browser WARN    Inference API failed, using simulation 400
[00:36:00.719] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.24025557374567796
[00:36:00.719] Browser WARN    Inference API failed, using simulation 400
[00:36:00.719] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2202998604155189
[00:36:01.088] Server  ERROR   Backend returned 400
[00:36:01.103] Server  ERROR   Backend returned 400
[00:36:01.115] Server  ERROR   Backend returned 400
[00:36:01.219] Browser WARN    Inference API failed, using simulation 400
[00:36:01.219] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4483695165059064
[00:36:01.219] Browser WARN    Inference API failed, using simulation 400
[00:36:01.219] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.047336259111606105
[00:36:01.219] Browser WARN    Inference API failed, using simulation 400
[00:36:01.219] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8916531360834747
[00:36:01.580] Server  ERROR   Backend returned 400
[00:36:01.596] Server  ERROR   Backend returned 400
[00:36:01.610] Server  ERROR   Backend returned 400
[00:36:01.713] Browser WARN    Inference API failed, using simulation 400
[00:36:01.713] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1169352640076925
[00:36:01.713] Browser WARN    Inference API failed, using simulation 400
[00:36:01.713] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.21522371119323763
[00:36:01.713] Browser WARN    Inference API failed, using simulation 400
[00:36:01.713] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.982210168353635
[00:36:02.086] Server  ERROR   Backend returned 400
[00:36:02.105] Server  ERROR   Backend returned 400
[00:36:02.116] Server  ERROR   Backend returned 400
[00:36:02.230] Browser WARN    Inference API failed, using simulation 400
[00:36:02.230] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8808601272834784
[00:36:02.230] Browser WARN    Inference API failed, using simulation 400
[00:36:02.230] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.35490429071655893
[00:36:02.230] Browser WARN    Inference API failed, using simulation 400
[00:36:02.230] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.750373938187724
[00:36:02.576] Server  ERROR   Backend returned 400
[00:36:02.600] Server  ERROR   Backend returned 400
[00:36:02.617] Server  ERROR   Backend returned 400
[00:36:02.721] Browser WARN    Inference API failed, using simulation 400
[00:36:02.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.35919708496583386
[00:36:02.721] Browser WARN    Inference API failed, using simulation 400
[00:36:02.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.09296268369890515
[00:36:02.721] Browser WARN    Inference API failed, using simulation 400
[00:36:02.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.43075145561355577
[00:36:03.081] Server  ERROR   Backend returned 400
[00:36:03.104] Server  ERROR   Backend returned 400
[00:36:03.120] Server  ERROR   Backend returned 400
[00:36:03.225] Browser WARN    Inference API failed, using simulation 400
[00:36:03.225] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7249198392712246
[00:36:03.225] Browser WARN    Inference API failed, using simulation 400
[00:36:03.225] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.31069832071161385
[00:36:03.225] Browser WARN    Inference API failed, using simulation 400
[00:36:03.225] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9488987701290358
[00:36:03.579] Server  ERROR   Backend returned 400
[00:36:03.596] Server  ERROR   Backend returned 400
[00:36:03.610] Server  ERROR   Backend returned 400
[00:36:03.718] Browser WARN    Inference API failed, using simulation 400
[00:36:03.718] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.31449668997366925
[00:36:03.718] Browser WARN    Inference API failed, using simulation 400
[00:36:03.718] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3648729281899052
[00:36:03.718] Browser WARN    Inference API failed, using simulation 400
[00:36:03.718] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8015143353515519
[00:36:04.077] Server  ERROR   Backend returned 400
[00:36:04.095] Server  ERROR   Backend returned 400
[00:36:04.112] Server  ERROR   Backend returned 400
[00:36:04.215] Browser WARN    Inference API failed, using simulation 400
[00:36:04.215] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.08080378584341502
[00:36:04.215] Browser WARN    Inference API failed, using simulation 400
[00:36:04.215] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.0908410468695961
[00:36:04.215] Browser WARN    Inference API failed, using simulation 400
[00:36:04.215] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3461056297577973
[00:36:04.629] Server  ERROR   Backend returned 400
[00:36:04.701] Server  ERROR   Backend returned 400
[00:36:04.712] Server  ERROR   Backend returned 400
[00:36:04.817] Browser WARN    Inference API failed, using simulation 400
[00:36:04.817] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2170860654734909
[00:36:04.817] Browser WARN    Inference API failed, using simulation 400
[00:36:04.817] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7232914585780792
[00:36:04.817] Browser WARN    Inference API failed, using simulation 400
[00:36:04.817] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.24509350495674048
[00:36:05.076] Server  ERROR   Backend returned 400
[00:36:05.111] Server  ERROR   Backend returned 400
[00:36:05.113] Server  ERROR   Backend returned 400
[00:36:05.223] Browser WARN    Inference API failed, using simulation 400
[00:36:05.223] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9747652309191197
[00:36:05.223] Browser WARN    Inference API failed, using simulation 400
[00:36:05.223] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.09721303163497497
[00:36:05.223] Browser WARN    Inference API failed, using simulation 400
[00:36:05.223] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4650585113928974
[00:36:05.585] Server  ERROR   Backend returned 400
[00:36:05.628] Server  ERROR   Backend returned 400
[00:36:05.631] Server  ERROR   Backend returned 400
[00:36:05.740] Browser WARN    Inference API failed, using simulation 400
[00:36:05.740] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.33654915058578977
[00:36:05.740] Browser WARN    Inference API failed, using simulation 400
[00:36:05.740] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3407920180748983
[00:36:05.740] Browser WARN    Inference API failed, using simulation 400
[00:36:05.740] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9914866513846365
[00:36:06.081] Server  ERROR   Backend returned 400
[00:36:06.100] Server  ERROR   Backend returned 400
[00:36:06.116] Server  ERROR   Backend returned 400
[00:36:06.220] Browser WARN    Inference API failed, using simulation 400
[00:36:06.220] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.40139000194775515
[00:36:06.220] Browser WARN    Inference API failed, using simulation 400
[00:36:06.220] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4458794218531316
[00:36:06.220] Browser WARN    Inference API failed, using simulation 400
[00:36:06.220] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.03223235576077005
[00:36:06.582] Server  ERROR   Backend returned 400
[00:36:06.602] Server  ERROR   Backend returned 400
[00:36:06.620] Server  ERROR   Backend returned 400
[00:36:06.725] Browser WARN    Inference API failed, using simulation 400
[00:36:06.725] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.12033373238803308
[00:36:06.725] Browser WARN    Inference API failed, using simulation 400
[00:36:06.725] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3010161808868257
[00:36:06.725] Browser WARN    Inference API failed, using simulation 400
[00:36:06.725] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1140213833201022
[00:36:07.086] Server  ERROR   Backend returned 400
[00:36:07.115] Server  ERROR   Backend returned 400
[00:36:07.139] Server  ERROR   Backend returned 400
[00:36:07.243] Browser WARN    Inference API failed, using simulation 400
[00:36:07.243] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9820265150041929
[00:36:07.243] Browser WARN    Inference API failed, using simulation 400
[00:36:07.243] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3426687404209156
[00:36:07.243] Browser WARN    Inference API failed, using simulation 400
[00:36:07.243] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.02948165679266479
[00:36:07.581] Server  ERROR   Backend returned 400
[00:36:07.600] Server  ERROR   Backend returned 400
[00:36:07.615] Server  ERROR   Backend returned 400
[00:36:07.718] Browser WARN    Inference API failed, using simulation 400
[00:36:07.718] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3326691586270201
[00:36:07.718] Browser WARN    Inference API failed, using simulation 400
[00:36:07.718] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8150118678772118
[00:36:07.718] Browser WARN    Inference API failed, using simulation 400
[00:36:07.718] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.05335533649902224
[00:36:08.086] Server  ERROR   Backend returned 400
[00:36:08.107] Server  ERROR   Backend returned 400
[00:36:08.121] Server  ERROR   Backend returned 400
[00:36:08.224] Browser WARN    Inference API failed, using simulation 400
[00:36:08.224] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.33968213030757216
[00:36:08.224] Browser WARN    Inference API failed, using simulation 400
[00:36:08.224] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9836481553146128
[00:36:08.224] Browser WARN    Inference API failed, using simulation 400
[00:36:08.224] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8642821486731997
[00:36:08.578] Server  ERROR   Backend returned 400
[00:36:08.615] Server  ERROR   Backend returned 400
[00:36:08.617] Server  ERROR   Backend returned 400
[00:36:08.728] Browser WARN    Inference API failed, using simulation 400
[00:36:08.728] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9373664633743893
[00:36:08.728] Browser WARN    Inference API failed, using simulation 400
[00:36:08.728] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.12064920905608006
[00:36:08.728] Browser WARN    Inference API failed, using simulation 400
[00:36:08.728] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.10093280783996644
[00:36:09.085] Server  ERROR   Backend returned 400
[00:36:09.111] Server  ERROR   Backend returned 400
[00:36:09.125] Server  ERROR   Backend returned 400
[00:36:09.229] Browser WARN    Inference API failed, using simulation 400
[00:36:09.229] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.36200911604384944
[00:36:09.229] Browser WARN    Inference API failed, using simulation 400
[00:36:09.229] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9738791190838165
[00:36:09.229] Browser WARN    Inference API failed, using simulation 400
[00:36:09.229] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.16051076516316914
[00:36:09.574] Server  ERROR   Backend returned 400
[00:36:09.601] Server  ERROR   Backend returned 400
[00:36:09.612] Server  ERROR   Backend returned 400
[00:36:09.717] Browser WARN    Inference API failed, using simulation 400
[00:36:09.717] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8637256987786188
[00:36:09.717] Browser WARN    Inference API failed, using simulation 400
[00:36:09.717] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.20878526764118827
[00:36:09.717] Browser WARN    Inference API failed, using simulation 400
[00:36:09.717] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.49495752620439876
[00:36:10.081] Server  ERROR   Backend returned 400
[00:36:10.099] Server  ERROR   Backend returned 400
[00:36:10.112] Server  ERROR   Backend returned 400
[00:36:10.217] Browser WARN    Inference API failed, using simulation 400
[00:36:10.217] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.06113113128934139
[00:36:10.217] Browser WARN    Inference API failed, using simulation 400
[00:36:10.217] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7659800708630744
[00:36:10.217] Browser WARN    Inference API failed, using simulation 400
[00:36:10.217] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.43991488442652926
[00:36:10.578] Server  ERROR   Backend returned 400
[00:36:10.614] Server  ERROR   Backend returned 400
[00:36:10.616] Server  ERROR   Backend returned 400
[00:36:10.726] Browser WARN    Inference API failed, using simulation 400
[00:36:10.726] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3762423840123908
[00:36:10.726] Browser WARN    Inference API failed, using simulation 400
[00:36:10.726] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7200755051956018
[00:36:10.726] Browser WARN    Inference API failed, using simulation 400
[00:36:10.726] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9402484449269958
[00:36:11.080] Server  ERROR   Backend returned 400
[00:36:11.109] Server  ERROR   Backend returned 400
[00:36:11.112] Server  ERROR   Backend returned 400
[00:36:11.221] Browser WARN    Inference API failed, using simulation 400
[00:36:11.221] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.26980500911312694
[00:36:11.221] Browser WARN    Inference API failed, using simulation 400
[00:36:11.221] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.013183273509095872
[00:36:11.221] Browser WARN    Inference API failed, using simulation 400
[00:36:11.221] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3634607517785582
[00:36:11.584] Server  ERROR   Backend returned 400
[00:36:11.611] Server  ERROR   Backend returned 400
[00:36:11.622] Server  ERROR   Backend returned 400
[00:36:11.726] Browser WARN    Inference API failed, using simulation 400
[00:36:11.726] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8714232071957928
[00:36:11.726] Browser WARN    Inference API failed, using simulation 400
[00:36:11.726] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.209401113482712
[00:36:11.726] Browser WARN    Inference API failed, using simulation 400
[00:36:11.726] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.36730953282083867
[00:36:12.084] Server  ERROR   Backend returned 400
[00:36:12.110] Server  ERROR   Backend returned 400
[00:36:12.115] Server  ERROR   Backend returned 400
[00:36:12.223] Browser WARN    Inference API failed, using simulation 400
[00:36:12.223] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.010689120185280732
[00:36:12.223] Browser WARN    Inference API failed, using simulation 400
[00:36:12.223] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.46333519262418005
[00:36:12.223] Browser WARN    Inference API failed, using simulation 400
[00:36:12.223] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.22250030437821777
[00:36:12.589] Server  ERROR   Backend returned 400
[00:36:12.605] Server  ERROR   Backend returned 400
[00:36:12.625] Server  ERROR   Backend returned 400
[00:36:12.731] Browser WARN    Inference API failed, using simulation 400
[00:36:12.731] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.0499194410591583
[00:36:12.731] Browser WARN    Inference API failed, using simulation 400
[00:36:12.731] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.27903916295335024
[00:36:12.731] Browser WARN    Inference API failed, using simulation 400
[00:36:12.731] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.0022969164090029803
[00:36:13.081] Server  ERROR   Backend returned 400
[00:36:13.103] Server  ERROR   Backend returned 400
[00:36:13.116] Server  ERROR   Backend returned 400
[00:36:13.230] Browser WARN    Inference API failed, using simulation 400
[00:36:13.230] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3845110900739842
[00:36:13.230] Browser WARN    Inference API failed, using simulation 400
[00:36:13.230] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9262599618865828
[00:36:13.230] Browser WARN    Inference API failed, using simulation 400
[00:36:13.230] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1693943363161775
[00:36:13.580] Server  ERROR   Backend returned 400
[00:36:13.607] Server  ERROR   Backend returned 400
[00:36:13.609] Server  ERROR   Backend returned 400
[00:36:13.728] Browser WARN    Inference API failed, using simulation 400
[00:36:13.728] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4064808322105228
[00:36:13.728] Browser WARN    Inference API failed, using simulation 400
[00:36:13.728] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9299249802134286
[00:36:13.728] Browser WARN    Inference API failed, using simulation 400
[00:36:13.728] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.27306636192181294
[00:36:14.079] Server  ERROR   Backend returned 400
[00:36:14.110] Server  ERROR   Backend returned 400
[00:36:14.113] Server  ERROR   Backend returned 400
[00:36:14.221] Browser WARN    Inference API failed, using simulation 400
[00:36:14.221] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1401142016908436
[00:36:14.221] Browser WARN    Inference API failed, using simulation 400
[00:36:14.221] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.18314010198367153
[00:36:14.221] Browser WARN    Inference API failed, using simulation 400
[00:36:14.221] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4955237096234863
[00:36:14.592] Server  ERROR   Backend returned 400
[00:36:14.611] Server  ERROR   Backend returned 400
[00:36:14.626] Server  ERROR   Backend returned 400
[00:36:14.739] Browser WARN    Inference API failed, using simulation 400
[00:36:14.739] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7719495268695628
[00:36:14.739] Browser WARN    Inference API failed, using simulation 400
[00:36:14.739] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1818111424497113
[00:36:14.739] Browser WARN    Inference API failed, using simulation 400
[00:36:14.739] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.03440849787010586
[00:36:15.082] Server  ERROR   Backend returned 400
[00:36:15.101] Server  ERROR   Backend returned 400
[00:36:15.119] Server  ERROR   Backend returned 400
[00:36:15.223] Browser WARN    Inference API failed, using simulation 400
[00:36:15.223] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2160876076502054
[00:36:15.223] Browser WARN    Inference API failed, using simulation 400
[00:36:15.223] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.10333596300485282
[00:36:15.223] Browser WARN    Inference API failed, using simulation 400
[00:36:15.223] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3538531643070469
[00:36:15.598] Server  ERROR   Backend returned 400
[00:36:15.623] Server  ERROR   Backend returned 400
[00:36:15.627] Server  ERROR   Backend returned 400
[00:36:15.736] Browser WARN    Inference API failed, using simulation 400
[00:36:15.736] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.41658750069478284
[00:36:15.736] Browser WARN    Inference API failed, using simulation 400
[00:36:15.736] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9121722869019233
[00:36:15.736] Browser WARN    Inference API failed, using simulation 400
[00:36:15.736] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8966886346276497
[00:36:16.085] Server  ERROR   Backend returned 400
[00:36:16.121] Server  ERROR   Backend returned 400
[00:36:16.123] Server  ERROR   Backend returned 400
[00:36:16.234] Browser WARN    Inference API failed, using simulation 400
[00:36:16.234] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2497796980423685
[00:36:16.234] Browser WARN    Inference API failed, using simulation 400
[00:36:16.234] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4810486948207957
[00:36:16.234] Browser WARN    Inference API failed, using simulation 400
[00:36:16.234] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.45353911451332884
[00:36:16.587] Server  ERROR   Backend returned 400
[00:36:16.610] Server  ERROR   Backend returned 400
[00:36:16.614] Server  ERROR   Backend returned 400
[00:36:16.738] Browser WARN    Inference API failed, using simulation 400
[00:36:16.738] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.82635984106551
[00:36:16.738] Browser WARN    Inference API failed, using simulation 400
[00:36:16.738] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.0629496047722416
[00:36:16.738] Browser WARN    Inference API failed, using simulation 400
[00:36:16.738] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.24823280375275342
[00:36:17.092] Server  ERROR   Backend returned 400
[00:36:17.109] Server  ERROR   Backend returned 400
[00:36:17.126] Server  ERROR   Backend returned 400
[00:36:17.232] Browser WARN    Inference API failed, using simulation 400
[00:36:17.232] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.12060309495818405
[00:36:17.232] Browser WARN    Inference API failed, using simulation 400
[00:36:17.232] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.38420750966853495
[00:36:17.232] Browser WARN    Inference API failed, using simulation 400
[00:36:17.232] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.27676229789237755
[00:36:17.583] Server  ERROR   Backend returned 400
[00:36:17.606] Server  ERROR   Backend returned 400
[00:36:17.620] Server  ERROR   Backend returned 400
[00:36:17.723] Browser WARN    Inference API failed, using simulation 400
[00:36:17.723] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8503851922107408
[00:36:17.723] Browser WARN    Inference API failed, using simulation 400
[00:36:17.723] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.19890332785380105
[00:36:17.723] Browser WARN    Inference API failed, using simulation 400
[00:36:17.723] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9838372882012344
[00:36:18.083] Server  ERROR   Backend returned 400
[00:36:18.103] Server  ERROR   Backend returned 400
[00:36:18.127] Server  ERROR   Backend returned 400
[00:36:18.232] Browser WARN    Inference API failed, using simulation 400
[00:36:18.232] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.20390639326870236
[00:36:18.232] Browser WARN    Inference API failed, using simulation 400
[00:36:18.232] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.20564708406195148
[00:36:18.232] Browser WARN    Inference API failed, using simulation 400
[00:36:18.232] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.16169742990394748
[00:36:18.582] Server  ERROR   Backend returned 400
[00:36:18.601] Server  ERROR   Backend returned 400
[00:36:18.617] Server  ERROR   Backend returned 400
[00:36:18.721] Browser WARN    Inference API failed, using simulation 400
[00:36:18.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.25753499815346886
[00:36:18.721] Browser WARN    Inference API failed, using simulation 400
[00:36:18.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.15840641778739328
[00:36:18.721] Browser WARN    Inference API failed, using simulation 400
[00:36:18.721] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7313624589096358
[00:36:19.077] Server  ERROR   Backend returned 400
[00:36:19.095] Server  ERROR   Backend returned 400
[00:36:19.115] Server  ERROR   Backend returned 400
[00:36:19.220] Browser WARN    Inference API failed, using simulation 400
[00:36:19.220] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8247089582733945
[00:36:19.220] Browser WARN    Inference API failed, using simulation 400
[00:36:19.220] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.812641836134977
[00:36:19.220] Browser WARN    Inference API failed, using simulation 400
[00:36:19.220] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.10751066064930914
[00:36:19.576] Server  ERROR   Backend returned 400
[00:36:19.600] Server  ERROR   Backend returned 400
[00:36:19.602] Server  ERROR   Backend returned 400
[00:36:19.710] Browser WARN    Inference API failed, using simulation 400
[00:36:19.710] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8999841871129575
[00:36:19.710] Browser WARN    Inference API failed, using simulation 400
[00:36:19.710] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4299284601925187
[00:36:19.710] Browser WARN    Inference API failed, using simulation 400
[00:36:19.710] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.20996863573419422
[00:36:20.086] Server  ERROR   Backend returned 400
[00:36:20.103] Server  ERROR   Backend returned 400
[00:36:20.117] Server  ERROR   Backend returned 400
[00:36:20.220] Browser WARN    Inference API failed, using simulation 400
[00:36:20.220] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.23342183149517626
[00:36:20.220] Browser WARN    Inference API failed, using simulation 400
[00:36:20.220] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9818735681598743
[00:36:20.220] Browser WARN    Inference API failed, using simulation 400
[00:36:20.220] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9714621234987276
[00:36:20.580] Server  ERROR   Backend returned 400
[00:36:20.604] Server  ERROR   Backend returned 400
[00:36:20.616] Server  ERROR   Backend returned 400
[00:36:20.719] Browser WARN    Inference API failed, using simulation 400
[00:36:20.719] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.19740644219780085
[00:36:20.719] Browser WARN    Inference API failed, using simulation 400
[00:36:20.719] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.27808248329539037
[00:36:20.719] Browser WARN    Inference API failed, using simulation 400
[00:36:20.719] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.04348573664660044
[00:36:21.087] Server  ERROR   Backend returned 400
[00:36:21.115] Server  ERROR   Backend returned 400
[00:36:21.117] Server  ERROR   Backend returned 400
[00:36:21.237] Browser WARN    Inference API failed, using simulation 400
[00:36:21.237] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7227783775014932
[00:36:21.237] Browser WARN    Inference API failed, using simulation 400
[00:36:21.237] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9997484299474899
[00:36:21.237] Browser WARN    Inference API failed, using simulation 400
[00:36:21.237] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.36856810292711395
[00:36:21.576] Server  ERROR   Backend returned 400
[00:36:21.604] Server  ERROR   Backend returned 400
[00:36:21.607] Server  ERROR   Backend returned 400
[00:36:21.717] Browser WARN    Inference API failed, using simulation 400
[00:36:21.717] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9179252227439318
[00:36:21.717] Browser WARN    Inference API failed, using simulation 400
[00:36:21.717] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2605880680694484
[00:36:21.717] Browser WARN    Inference API failed, using simulation 400
[00:36:21.717] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3973592754654972
[00:36:22.079] Server  ERROR   Backend returned 400
[00:36:22.104] Server  ERROR   Backend returned 400
[00:36:22.107] Server  ERROR   Backend returned 400
[00:36:22.221] Browser WARN    Inference API failed, using simulation 400
[00:36:22.221] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4501822992724106
[00:36:22.221] Browser WARN    Inference API failed, using simulation 400
[00:36:22.221] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.44511338736442996
[00:36:22.221] Browser WARN    Inference API failed, using simulation 400
[00:36:22.221] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9604111697548903
[00:36:22.587] Server  ERROR   Backend returned 400
[00:36:22.603] Server  ERROR   Backend returned 400
[00:36:22.615] Server  ERROR   Backend returned 400
[00:36:22.719] Browser WARN    Inference API failed, using simulation 400
[00:36:22.719] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.19034015988485714
[00:36:22.719] Browser WARN    Inference API failed, using simulation 400
[00:36:22.719] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3045832080280399
[00:36:22.719] Browser WARN    Inference API failed, using simulation 400
[00:36:22.719] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9924281944608402
[00:36:23.080] Server  ERROR   Backend returned 400
[00:36:23.102] Server  ERROR   Backend returned 400
[00:36:23.115] Server  ERROR   Backend returned 400
[00:36:23.219] Browser WARN    Inference API failed, using simulation 400
[00:36:23.219] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.12687739013644178
[00:36:23.219] Browser WARN    Inference API failed, using simulation 400
[00:36:23.219] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.31107077257342625
[00:36:23.219] Browser WARN    Inference API failed, using simulation 400
[00:36:23.219] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.10307837733037023
[00:36:23.588] Server  ERROR   Backend returned 400
[00:36:23.604] Server  ERROR   Backend returned 400
[00:36:23.618] Server  ERROR   Backend returned 400
[00:36:23.721] Browser WARN    Inference API failed, using simulation 400
[00:36:23.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3323285750708642
[00:36:23.721] Browser WARN    Inference API failed, using simulation 400
[00:36:23.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.352068950205342
[00:36:23.721] Browser WARN    Inference API failed, using simulation 400
[00:36:23.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.22666896075964216
[00:36:24.090] Server  ERROR   Backend returned 400
[00:36:24.110] Server  ERROR   Backend returned 400
[00:36:24.144] Server  ERROR   Backend returned 400
[00:36:24.259] Browser WARN    Inference API failed, using simulation 400
[00:36:24.259] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4767266176004376
[00:36:24.259] Browser WARN    Inference API failed, using simulation 400
[00:36:24.259] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.02837477132756322
[00:36:24.259] Browser WARN    Inference API failed, using simulation 400
[00:36:24.259] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1412717482336145
[00:36:24.581] Server  ERROR   Backend returned 400
[00:36:24.597] Server  ERROR   Backend returned 400
[00:36:24.615] Server  ERROR   Backend returned 400
[00:36:24.721] Browser WARN    Inference API failed, using simulation 400
[00:36:24.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4109807450670722
[00:36:24.721] Browser WARN    Inference API failed, using simulation 400
[00:36:24.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2563881668921381
[00:36:24.721] Browser WARN    Inference API failed, using simulation 400
[00:36:24.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3173966438396095
[00:36:25.074] Server  ERROR   Backend returned 400
[00:36:25.091] Server  ERROR   Backend returned 400
[00:36:25.108] Server  ERROR   Backend returned 400
[00:36:25.211] Browser WARN    Inference API failed, using simulation 400
[00:36:25.211] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.009351928580413338
[00:36:25.211] Browser WARN    Inference API failed, using simulation 400
[00:36:25.211] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.11178930061128195
[00:36:25.211] Browser WARN    Inference API failed, using simulation 400
[00:36:25.211] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.018035191638734793
[00:36:25.583] Server  ERROR   Backend returned 400
[00:36:25.597] Server  ERROR   Backend returned 400
[00:36:25.620] Server  ERROR   Backend returned 400
[00:36:25.724] Browser WARN    Inference API failed, using simulation 400
[00:36:25.724] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.729150220601865
[00:36:25.724] Browser WARN    Inference API failed, using simulation 400
[00:36:25.724] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.0882400930171584
[00:36:25.724] Browser WARN    Inference API failed, using simulation 400
[00:36:25.724] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4971336071734416
[00:36:26.075] Server  ERROR   Backend returned 400
[00:36:26.103] Server  ERROR   Backend returned 400
[00:36:26.115] Server  ERROR   Backend returned 400
[00:36:26.219] Browser WARN    Inference API failed, using simulation 400
[00:36:26.219] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4245827141993548
[00:36:26.219] Browser WARN    Inference API failed, using simulation 400
[00:36:26.219] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8881617002878154
[00:36:26.219] Browser WARN    Inference API failed, using simulation 400
[00:36:26.219] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.05403915430972889
[00:36:26.603] Server  ERROR   Backend returned 400
[00:36:26.629] Server  ERROR   Backend returned 400
[00:36:26.641] Server  ERROR   Backend returned 400
[00:36:26.745] Browser WARN    Inference API failed, using simulation 400
[00:36:26.745] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9758582985534031
[00:36:26.745] Browser WARN    Inference API failed, using simulation 400
[00:36:26.745] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9768953559321094
[00:36:26.745] Browser WARN    Inference API failed, using simulation 400
[00:36:26.745] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.83395183156573
[00:36:27.141] Server  ERROR   Backend returned 400
[00:36:27.255] Browser WARN    Inference API failed, using simulation 400
[00:36:27.255] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.998617048157737
[00:36:27.259] Server  ERROR   Backend returned 400
[00:36:27.328] Server  ERROR   Backend returned 400
[00:36:27.432] Browser WARN    Inference API failed, using simulation 400
[00:36:27.432] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.345083451656712
[00:36:27.432] Browser WARN    Inference API failed, using simulation 400
[00:36:27.432] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.38968839665201255
[00:36:27.582] Server  ERROR   Backend returned 400
[00:36:27.616] Server  ERROR   Backend returned 400
[00:36:27.626] Server  ERROR   Backend returned 400
[00:36:27.730] Browser WARN    Inference API failed, using simulation 400
[00:36:27.730] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.48496696143007245
[00:36:27.730] Browser WARN    Inference API failed, using simulation 400
[00:36:27.730] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.006995893940858144
[00:36:27.730] Browser WARN    Inference API failed, using simulation 400
[00:36:27.730] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9821683957044554
[00:36:28.086] Server  ERROR   Backend returned 400
[00:36:28.106] Server  ERROR   Backend returned 400
[00:36:28.120] Server  ERROR   Backend returned 400
[00:36:28.224] Browser WARN    Inference API failed, using simulation 400
[00:36:28.224] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3024569034879504
[00:36:28.224] Browser WARN    Inference API failed, using simulation 400
[00:36:28.224] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4678228579672959
[00:36:28.224] Browser WARN    Inference API failed, using simulation 400
[00:36:28.224] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.803840430505501
[00:36:28.581] Server  ERROR   Backend returned 400
[00:36:28.607] Server  ERROR   Backend returned 400
[00:36:28.620] Server  ERROR   Backend returned 400
[00:36:28.724] Browser WARN    Inference API failed, using simulation 400
[00:36:28.724] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8199751001947907
[00:36:28.724] Browser WARN    Inference API failed, using simulation 400
[00:36:28.724] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8463655218130921
[00:36:28.724] Browser WARN    Inference API failed, using simulation 400
[00:36:28.724] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.36621922138338187
[00:36:29.075] Server  ERROR   Backend returned 400
[00:36:29.100] Server  ERROR   Backend returned 400
[00:36:29.116] Server  ERROR   Backend returned 400
[00:36:29.220] Browser WARN    Inference API failed, using simulation 400
[00:36:29.220] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.45339658355459445
[00:36:29.220] Browser WARN    Inference API failed, using simulation 400
[00:36:29.220] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8224849507098986
[00:36:29.220] Browser WARN    Inference API failed, using simulation 400
[00:36:29.220] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.17838053955326238
[00:36:29.582] Server  ERROR   Backend returned 400
[00:36:29.609] Server  ERROR   Backend returned 400
[00:36:29.623] Server  ERROR   Backend returned 400
[00:36:29.727] Browser WARN    Inference API failed, using simulation 400
[00:36:29.727] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.773290304193105
[00:36:29.727] Browser WARN    Inference API failed, using simulation 400
[00:36:29.727] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.08864237722692869
[00:36:29.727] Browser WARN    Inference API failed, using simulation 400
[00:36:29.727] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9761477324180463
[00:36:30.077] Server  ERROR   Backend returned 400
[00:36:30.100] Server  ERROR   Backend returned 400
[00:36:30.111] Server  ERROR   Backend returned 400
[00:36:30.217] Browser WARN    Inference API failed, using simulation 400
[00:36:30.217] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4473869767308306
[00:36:30.217] Browser WARN    Inference API failed, using simulation 400
[00:36:30.217] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.20248910849497487
[00:36:30.217] Browser WARN    Inference API failed, using simulation 400
[00:36:30.217] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.974783982061145
[00:36:30.589] Server  ERROR   Backend returned 400
[00:36:30.607] Server  ERROR   Backend returned 400
[00:36:30.622] Server  ERROR   Backend returned 400
[00:36:30.726] Browser WARN    Inference API failed, using simulation 400
[00:36:30.726] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.08939069852333503
[00:36:30.726] Browser WARN    Inference API failed, using simulation 400
[00:36:30.726] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.49096461460811125
[00:36:30.726] Browser WARN    Inference API failed, using simulation 400
[00:36:30.726] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2731238327729093
[00:36:31.078] Server  ERROR   Backend returned 400
[00:36:31.094] Server  ERROR   Backend returned 400
[00:36:31.114] Server  ERROR   Backend returned 400
[00:36:31.218] Browser WARN    Inference API failed, using simulation 400
[00:36:31.218] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9351621659986371
[00:36:31.218] Browser WARN    Inference API failed, using simulation 400
[00:36:31.218] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9039306742019252
[00:36:31.218] Browser WARN    Inference API failed, using simulation 400
[00:36:31.218] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.0857217251409475
[00:36:31.579] Server  ERROR   Backend returned 400
[00:36:31.614] Server  ERROR   Backend returned 400
[00:36:31.620] Server  ERROR   Backend returned 400
[00:36:31.727] Browser WARN    Inference API failed, using simulation 400
[00:36:31.727] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.28190668316470513
[00:36:31.727] Browser WARN    Inference API failed, using simulation 400
[00:36:31.727] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8779655959359154
[00:36:31.727] Browser WARN    Inference API failed, using simulation 400
[00:36:31.727] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7960990887570715
[00:36:32.088] Server  ERROR   Backend returned 400
[00:36:32.119] Server  ERROR   Backend returned 400
[00:36:32.121] Server  ERROR   Backend returned 400
[00:36:32.230] Browser WARN    Inference API failed, using simulation 400
[00:36:32.230] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9248422229587616
[00:36:32.230] Browser WARN    Inference API failed, using simulation 400
[00:36:32.230] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.38524183918746097
[00:36:32.230] Browser WARN    Inference API failed, using simulation 400
[00:36:32.230] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.887285067990736
[00:36:32.581] Server  ERROR   Backend returned 400
[00:36:32.630] Server  ERROR   Backend returned 400
[00:36:32.635] Server  ERROR   Backend returned 400
[00:36:32.757] Browser WARN    Inference API failed, using simulation 400
[00:36:32.757] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.28909674344994
[00:36:32.757] Browser WARN    Inference API failed, using simulation 400
[00:36:32.757] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1344323355821368
[00:36:32.757] Browser WARN    Inference API failed, using simulation 400
[00:36:32.757] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.100683656356434
[00:36:33.078] Server  ERROR   Backend returned 400
[00:36:33.095] Server  ERROR   Backend returned 400
[00:36:33.115] Server  ERROR   Backend returned 400
[00:36:33.220] Browser WARN    Inference API failed, using simulation 400
[00:36:33.220] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.366450008376991
[00:36:33.220] Browser WARN    Inference API failed, using simulation 400
[00:36:33.220] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.21955473462867597
[00:36:33.220] Browser WARN    Inference API failed, using simulation 400
[00:36:33.220] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.32710846634486673
[00:36:33.577] Server  ERROR   Backend returned 400
[00:36:33.611] Server  ERROR   Backend returned 400
[00:36:33.617] Server  ERROR   Backend returned 400
[00:36:33.723] Browser WARN    Inference API failed, using simulation 400
[00:36:33.723] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.768527870342114
[00:36:33.723] Browser WARN    Inference API failed, using simulation 400
[00:36:33.723] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.05535943810776439
[00:36:33.723] Browser WARN    Inference API failed, using simulation 400
[00:36:33.723] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.06845855366742365
[00:36:34.077] Server  ERROR   Backend returned 400
[00:36:34.113] Server  ERROR   Backend returned 400
[00:36:34.115] Server  ERROR   Backend returned 400
[00:36:34.223] Browser WARN    Inference API failed, using simulation 400
[00:36:34.223] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4144566435867591
[00:36:34.223] Browser WARN    Inference API failed, using simulation 400
[00:36:34.223] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9485227050772638
[00:36:34.223] Browser WARN    Inference API failed, using simulation 400
[00:36:34.223] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8822969595719138
[00:36:34.588] Server  ERROR   Backend returned 400
[00:36:34.605] Server  ERROR   Backend returned 400
[00:36:34.619] Server  ERROR   Backend returned 400
[00:36:34.724] Browser WARN    Inference API failed, using simulation 400
[00:36:34.724] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.14678541787776206
[00:36:34.724] Browser WARN    Inference API failed, using simulation 400
[00:36:34.724] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.17626600864540798
[00:36:34.724] Browser WARN    Inference API failed, using simulation 400
[00:36:34.724] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.21880208684932
[00:36:35.078] Server  ERROR   Backend returned 400
[00:36:35.101] Server  ERROR   Backend returned 400
[00:36:35.113] Server  ERROR   Backend returned 400
[00:36:35.218] Browser WARN    Inference API failed, using simulation 400
[00:36:35.218] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1353845863791106
[00:36:35.218] Browser WARN    Inference API failed, using simulation 400
[00:36:35.218] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9788612092183409
[00:36:35.218] Browser WARN    Inference API failed, using simulation 400
[00:36:35.218] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.05671607040395105
[00:36:35.580] Server  ERROR   Backend returned 400
[00:36:35.612] Server  ERROR   Backend returned 400
[00:36:35.623] Server  ERROR   Backend returned 400
[00:36:35.727] Browser WARN    Inference API failed, using simulation 400
[00:36:35.727] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.38374271956581674
[00:36:35.727] Browser WARN    Inference API failed, using simulation 400
[00:36:35.727] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2316093295110846
[00:36:35.727] Browser WARN    Inference API failed, using simulation 400
[00:36:35.727] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.12331192124631207
[00:36:36.079] Server  ERROR   Backend returned 400
[00:36:36.100] Server  ERROR   Backend returned 400
[00:36:36.111] Server  ERROR   Backend returned 400
[00:36:36.216] Browser WARN    Inference API failed, using simulation 400
[00:36:36.216] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.018203854488182125
[00:36:36.216] Browser WARN    Inference API failed, using simulation 400
[00:36:36.216] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4684932465072918
[00:36:36.216] Browser WARN    Inference API failed, using simulation 400
[00:36:36.216] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.13206354760193195
[00:36:36.580] Server  ERROR   Backend returned 400
[00:36:36.604] Server  ERROR   Backend returned 400
[00:36:36.617] Server  ERROR   Backend returned 400
[00:36:36.723] Browser WARN    Inference API failed, using simulation 400
[00:36:36.723] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.339327017098783
[00:36:36.723] Browser WARN    Inference API failed, using simulation 400
[00:36:36.723] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.02718346048343162
[00:36:36.723] Browser WARN    Inference API failed, using simulation 400
[00:36:36.723] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4281560072194258
[00:36:37.078] Server  ERROR   Backend returned 400
[00:36:37.100] Server  ERROR   Backend returned 400
[00:36:37.119] Server  ERROR   Backend returned 400
[00:36:37.226] Browser WARN    Inference API failed, using simulation 400
[00:36:37.226] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.05056605028292199
[00:36:37.226] Browser WARN    Inference API failed, using simulation 400
[00:36:37.226] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7850279458316675
[00:36:37.226] Browser WARN    Inference API failed, using simulation 400
[00:36:37.226] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.15147335727063366
[00:36:37.581] Server  ERROR   Backend returned 400
[00:36:37.596] Server  ERROR   Backend returned 400
[00:36:37.625] Server  ERROR   Backend returned 400
[00:36:37.728] Browser WARN    Inference API failed, using simulation 400
[00:36:37.728] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.078652080355191
[00:36:37.728] Browser WARN    Inference API failed, using simulation 400
[00:36:37.728] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3710308161551015
[00:36:37.728] Browser WARN    Inference API failed, using simulation 400
[00:36:37.728] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.47904771157787734
[00:36:38.079] Server  ERROR   Backend returned 400
[00:36:38.098] Server  ERROR   Backend returned 400
[00:36:38.111] Server  ERROR   Backend returned 400
[00:36:38.217] Browser WARN    Inference API failed, using simulation 400
[00:36:38.217] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.45529500210852103
[00:36:38.217] Browser WARN    Inference API failed, using simulation 400
[00:36:38.217] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8165657981096682
[00:36:38.217] Browser WARN    Inference API failed, using simulation 400
[00:36:38.217] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.16661670655411226
[00:36:38.581] Server  ERROR   Backend returned 400
[00:36:38.617] Server  ERROR   Backend returned 400
[00:36:38.620] Server  ERROR   Backend returned 400
[00:36:38.728] Browser WARN    Inference API failed, using simulation 400
[00:36:38.728] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8584515258052943
[00:36:38.728] Browser WARN    Inference API failed, using simulation 400
[00:36:38.728] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7645515789506422
[00:36:38.728] Browser WARN    Inference API failed, using simulation 400
[00:36:38.728] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.05772611125681287
[00:36:39.094] Server  ERROR   Backend returned 400
[00:36:39.117] Server  ERROR   Backend returned 400
[00:36:39.129] Server  ERROR   Backend returned 400
[00:36:39.232] Browser WARN    Inference API failed, using simulation 400
[00:36:39.232] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3861176521016881
[00:36:39.232] Browser WARN    Inference API failed, using simulation 400
[00:36:39.232] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4682329214349811
[00:36:39.232] Browser WARN    Inference API failed, using simulation 400
[00:36:39.232] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.461352979709501
[00:36:39.580] Server  ERROR   Backend returned 400
[00:36:39.610] Server  ERROR   Backend returned 400
[00:36:39.623] Server  ERROR   Backend returned 400
[00:36:39.738] Browser WARN    Inference API failed, using simulation 400
[00:36:39.738] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9947704259850298
[00:36:39.738] Browser WARN    Inference API failed, using simulation 400
[00:36:39.738] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8864659544511769
[00:36:39.738] Browser WARN    Inference API failed, using simulation 400
[00:36:39.738] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8464418325263041
[00:36:40.086] Server  ERROR   Backend returned 400
[00:36:40.115] Server  ERROR   Backend returned 400
[00:36:40.118] Server  ERROR   Backend returned 400
[00:36:40.226] Browser WARN    Inference API failed, using simulation 400
[00:36:40.226] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.38123517828006354
[00:36:40.226] Browser WARN    Inference API failed, using simulation 400
[00:36:40.226] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.34953906655308076
[00:36:40.226] Browser WARN    Inference API failed, using simulation 400
[00:36:40.226] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.19199066996041975
[00:36:40.579] Server  ERROR   Backend returned 400
[00:36:40.604] Server  ERROR   Backend returned 400
[00:36:40.627] Server  ERROR   Backend returned 400
[00:36:40.731] Browser WARN    Inference API failed, using simulation 400
[00:36:40.731] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3532662138858011
[00:36:40.731] Browser WARN    Inference API failed, using simulation 400
[00:36:40.731] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3725319694508073
[00:36:40.731] Browser WARN    Inference API failed, using simulation 400
[00:36:40.731] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4150371151020142
[00:36:41.080] Server  ERROR   Backend returned 400
[00:36:41.102] Server  ERROR   Backend returned 400
[00:36:41.114] Server  ERROR   Backend returned 400
[00:36:41.218] Browser WARN    Inference API failed, using simulation 400
[00:36:41.218] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7222954270085387
[00:36:41.218] Browser WARN    Inference API failed, using simulation 400
[00:36:41.218] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3613357255777357
[00:36:41.218] Browser WARN    Inference API failed, using simulation 400
[00:36:41.218] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7749855632442539
[00:36:41.583] Server  ERROR   Backend returned 400
[00:36:41.610] Server  ERROR   Backend returned 400
[00:36:41.613] Server  ERROR   Backend returned 400
[00:36:41.723] Browser WARN    Inference API failed, using simulation 400
[00:36:41.723] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4228121270036119
[00:36:41.723] Browser WARN    Inference API failed, using simulation 400
[00:36:41.723] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.012562570617488333
[00:36:41.723] Browser WARN    Inference API failed, using simulation 400
[00:36:41.723] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.08899945088759481
[00:36:42.079] Server  ERROR   Backend returned 400
[00:36:42.106] Server  ERROR   Backend returned 400
[00:36:42.109] Server  ERROR   Backend returned 400
[00:36:42.217] Browser WARN    Inference API failed, using simulation 400
[00:36:42.217] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7121793539201473
[00:36:42.217] Browser WARN    Inference API failed, using simulation 400
[00:36:42.217] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.10319007834326338
[00:36:42.217] Browser WARN    Inference API failed, using simulation 400
[00:36:42.217] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3645838514698053
[00:36:42.585] Server  ERROR   Backend returned 400
[00:36:42.610] Server  ERROR   Backend returned 400
[00:36:42.629] Server  ERROR   Backend returned 400
[00:36:42.732] Browser WARN    Inference API failed, using simulation 400
[00:36:42.732] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.09837725269355146
[00:36:42.732] Browser WARN    Inference API failed, using simulation 400
[00:36:42.732] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9382159192553674
[00:36:42.732] Browser WARN    Inference API failed, using simulation 400
[00:36:42.732] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4451865805413359
[00:36:43.082] Server  ERROR   Backend returned 400
[00:36:43.105] Server  ERROR   Backend returned 400
[00:36:43.117] Server  ERROR   Backend returned 400
[00:36:43.224] Browser WARN    Inference API failed, using simulation 400
[00:36:43.224] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.855926674486441
[00:36:43.224] Browser WARN    Inference API failed, using simulation 400
[00:36:43.224] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7700726201539023
[00:36:43.224] Browser WARN    Inference API failed, using simulation 400
[00:36:43.224] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7194088542802597
[00:36:43.579] Server  ERROR   Backend returned 400
[00:36:43.600] Server  ERROR   Backend returned 400
[00:36:43.613] Server  ERROR   Backend returned 400
[00:36:43.732] Browser WARN    Inference API failed, using simulation 400
[00:36:43.732] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.848914323457541
[00:36:43.732] Browser WARN    Inference API failed, using simulation 400
[00:36:43.732] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2952673888670921
[00:36:43.732] Browser WARN    Inference API failed, using simulation 400
[00:36:43.733] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4549235230915273
[00:36:44.083] Server  ERROR   Backend returned 400
[00:36:44.100] Server  ERROR   Backend returned 400
[00:36:44.116] Server  ERROR   Backend returned 400
[00:36:44.220] Browser WARN    Inference API failed, using simulation 400
[00:36:44.220] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7805573444702978
[00:36:44.220] Browser WARN    Inference API failed, using simulation 400
[00:36:44.220] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.04733490458780987
[00:36:44.220] Browser WARN    Inference API failed, using simulation 400
[00:36:44.220] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4995294121870688
[00:36:44.596] Server  ERROR   Backend returned 400
[00:36:44.612] Server  ERROR   Backend returned 400
[00:36:44.634] Server  ERROR   Backend returned 400
[00:36:44.739] Browser WARN    Inference API failed, using simulation 400
[00:36:44.739] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1969603325877486
[00:36:44.739] Browser WARN    Inference API failed, using simulation 400
[00:36:44.739] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.35757095890842516
[00:36:44.739] Browser WARN    Inference API failed, using simulation 400
[00:36:44.739] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.16987605967461994
[00:36:45.087] Server  ERROR   Backend returned 400
[00:36:45.111] Server  ERROR   Backend returned 400
[00:36:45.130] Server  ERROR   Backend returned 400
[00:36:45.234] Browser WARN    Inference API failed, using simulation 400
[00:36:45.234] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7536219000388182
[00:36:45.234] Browser WARN    Inference API failed, using simulation 400
[00:36:45.234] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2454064527841981
[00:36:45.234] Browser WARN    Inference API failed, using simulation 400
[00:36:45.234] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8330963796798802
[00:36:45.594] Server  ERROR   Backend returned 400
[00:36:45.615] Server  ERROR   Backend returned 400
[00:36:45.627] Server  ERROR   Backend returned 400
[00:36:45.731] Browser WARN    Inference API failed, using simulation 400
[00:36:45.731] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.12672654616531104
[00:36:45.731] Browser WARN    Inference API failed, using simulation 400
[00:36:45.731] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.09172299548386392
[00:36:45.732] Browser WARN    Inference API failed, using simulation 400
[00:36:45.732] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9632579682024339
[00:36:46.089] Server  ERROR   Backend returned 400
[00:36:46.115] Server  ERROR   Backend returned 400
[00:36:46.133] Server  ERROR   Backend returned 400
[00:36:46.238] Browser WARN    Inference API failed, using simulation 400
[00:36:46.238] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4922785161218392
[00:36:46.238] Browser WARN    Inference API failed, using simulation 400
[00:36:46.238] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.22681813013011276
[00:36:46.238] Browser WARN    Inference API failed, using simulation 400
[00:36:46.238] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.317537883564843
[00:36:46.587] Server  ERROR   Backend returned 400
[00:36:46.605] Server  ERROR   Backend returned 400
[00:36:46.619] Server  ERROR   Backend returned 400
[00:36:46.723] Browser WARN    Inference API failed, using simulation 400
[00:36:46.723] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4550868394946139
[00:36:46.723] Browser WARN    Inference API failed, using simulation 400
[00:36:46.723] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2803070248406952
[00:36:46.723] Browser WARN    Inference API failed, using simulation 400
[00:36:46.723] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.894329135966919
[00:36:47.082] Server  ERROR   Backend returned 400
[00:36:47.101] Server  ERROR   Backend returned 400
[00:36:47.115] Server  ERROR   Backend returned 400
[00:36:47.218] Browser WARN    Inference API failed, using simulation 400
[00:36:47.218] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9146074834892591
[00:36:47.218] Browser WARN    Inference API failed, using simulation 400
[00:36:47.218] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7296101947736615
[00:36:47.218] Browser WARN    Inference API failed, using simulation 400
[00:36:47.218] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7870304336624513
[00:36:47.579] Server  ERROR   Backend returned 400
[00:36:47.596] Server  ERROR   Backend returned 400
[00:36:47.614] Server  ERROR   Backend returned 400
[00:36:47.724] Browser WARN    Inference API failed, using simulation 400
[00:36:47.724] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8964289533361716
[00:36:47.724] Browser WARN    Inference API failed, using simulation 400
[00:36:47.724] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.730810691209077
[00:36:47.724] Browser WARN    Inference API failed, using simulation 400
[00:36:47.724] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.14075515674108724
[00:36:48.091] Server  ERROR   Backend returned 400
[00:36:48.116] Server  ERROR   Backend returned 400
[00:36:48.137] Server  ERROR   Backend returned 400
[00:36:48.240] Browser WARN    Inference API failed, using simulation 400
[00:36:48.240] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.0525519518002997
[00:36:48.240] Browser WARN    Inference API failed, using simulation 400
[00:36:48.240] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7951688161082819
[00:36:48.240] Browser WARN    Inference API failed, using simulation 400
[00:36:48.240] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.09454107751326846
[00:36:48.588] Server  ERROR   Backend returned 400
[00:36:48.614] Server  ERROR   Backend returned 400
[00:36:48.631] Server  ERROR   Backend returned 400
[00:36:48.735] Browser WARN    Inference API failed, using simulation 400
[00:36:48.735] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.05985456239976744
[00:36:48.735] Browser WARN    Inference API failed, using simulation 400
[00:36:48.735] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8785953928919084
[00:36:48.735] Browser WARN    Inference API failed, using simulation 400
[00:36:48.735] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4974667582311665
[00:36:49.077] Server  ERROR   Backend returned 400
[00:36:49.102] Server  ERROR   Backend returned 400
[00:36:49.110] Server  ERROR   Backend returned 400
[00:36:49.218] Browser WARN    Inference API failed, using simulation 400
[00:36:49.218] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9710256912996019
[00:36:49.218] Browser WARN    Inference API failed, using simulation 400
[00:36:49.218] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.47936562782652875
[00:36:49.218] Browser WARN    Inference API failed, using simulation 400
[00:36:49.218] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3247345135580722
[00:36:49.578] Server  ERROR   Backend returned 400
[00:36:49.610] Server  ERROR   Backend returned 400
[00:36:49.613] Server  ERROR   Backend returned 400
[00:36:49.722] Browser WARN    Inference API failed, using simulation 400
[00:36:49.722] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7741584781432892
[00:36:49.722] Browser WARN    Inference API failed, using simulation 400
[00:36:49.722] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.24771242609566851
[00:36:49.722] Browser WARN    Inference API failed, using simulation 400
[00:36:49.722] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.35121554353952694
[00:36:50.078] Server  ERROR   Backend returned 400
[00:36:50.109] Server  ERROR   Backend returned 400
[00:36:50.111] Server  ERROR   Backend returned 400
[00:36:50.219] Browser WARN    Inference API failed, using simulation 400
[00:36:50.219] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7886188786487495
[00:36:50.219] Browser WARN    Inference API failed, using simulation 400
[00:36:50.219] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.27924040501135705
[00:36:50.219] Browser WARN    Inference API failed, using simulation 400
[00:36:50.219] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9158965116046041
[00:36:50.586] Server  ERROR   Backend returned 400
[00:36:50.604] Server  ERROR   Backend returned 400
[00:36:50.617] Server  ERROR   Backend returned 400
[00:36:50.723] Browser WARN    Inference API failed, using simulation 400
[00:36:50.723] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3314966072189115
[00:36:50.723] Browser WARN    Inference API failed, using simulation 400
[00:36:50.723] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8479831301925019
[00:36:50.723] Browser WARN    Inference API failed, using simulation 400
[00:36:50.723] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.28704000272612756
[00:36:51.084] Server  ERROR   Backend returned 400
[00:36:51.114] Server  ERROR   Backend returned 400
[00:36:51.116] Server  ERROR   Backend returned 400
[00:36:51.226] Browser WARN    Inference API failed, using simulation 400
[00:36:51.226] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.07883397802003916
[00:36:51.226] Browser WARN    Inference API failed, using simulation 400
[00:36:51.226] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8866648314732791
[00:36:51.226] Browser WARN    Inference API failed, using simulation 400
[00:36:51.226] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.070630651626794
[00:36:51.583] Server  ERROR   Backend returned 400
[00:36:51.609] Server  ERROR   Backend returned 400
[00:36:51.613] Server  ERROR   Backend returned 400
[00:36:51.722] Browser WARN    Inference API failed, using simulation 400
[00:36:51.722] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.18251938380974736
[00:36:51.722] Browser WARN    Inference API failed, using simulation 400
[00:36:51.722] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.21608062585265797
[00:36:51.722] Browser WARN    Inference API failed, using simulation 400
[00:36:51.722] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8936405191684542
[00:36:52.081] Server  ERROR   Backend returned 400
[00:36:52.100] Server  ERROR   Backend returned 400
[00:36:52.114] Server  ERROR   Backend returned 400
[00:36:52.218] Browser WARN    Inference API failed, using simulation 400
[00:36:52.218] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.09684975502955306
[00:36:52.218] Browser WARN    Inference API failed, using simulation 400
[00:36:52.218] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9866563053765983
[00:36:52.218] Browser WARN    Inference API failed, using simulation 400
[00:36:52.218] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7760356193530642
[00:36:52.584] Server  ERROR   Backend returned 400
[00:36:52.602] Server  ERROR   Backend returned 400
[00:36:52.617] Server  ERROR   Backend returned 400
[00:36:52.721] Browser WARN    Inference API failed, using simulation 400
[00:36:52.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.16902924255735502
[00:36:52.721] Browser WARN    Inference API failed, using simulation 400
[00:36:52.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4433819892271205
[00:36:52.721] Browser WARN    Inference API failed, using simulation 400
[00:36:52.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4824132790882209
[00:36:53.082] Server  ERROR   Backend returned 400
[00:36:53.116] Server  ERROR   Backend returned 400
[00:36:53.119] Server  ERROR   Backend returned 400
[00:36:53.225] Browser WARN    Inference API failed, using simulation 400
[00:36:53.225] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.06667637131321502
[00:36:53.225] Browser WARN    Inference API failed, using simulation 400
[00:36:53.225] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4393474234653855
[00:36:53.225] Browser WARN    Inference API failed, using simulation 400
[00:36:53.225] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.022703768724096463
[00:36:53.584] Server  ERROR   Backend returned 400
[00:36:53.602] Server  ERROR   Backend returned 400
[00:36:53.617] Server  ERROR   Backend returned 400
[00:36:53.724] Browser WARN    Inference API failed, using simulation 400
[00:36:53.724] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3753657758967142
[00:36:53.724] Browser WARN    Inference API failed, using simulation 400
[00:36:53.724] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3563793685546006
[00:36:53.724] Browser WARN    Inference API failed, using simulation 400
[00:36:53.724] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.11011007772032022
[00:36:54.095] Server  ERROR   Backend returned 400
[00:36:54.124] Server  ERROR   Backend returned 400
[00:36:54.145] Server  ERROR   Backend returned 400
[00:36:54.265] Browser WARN    Inference API failed, using simulation 400
[00:36:54.265] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9602675507836969
[00:36:54.265] Browser WARN    Inference API failed, using simulation 400
[00:36:54.265] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9942081826652986
[00:36:54.265] Browser WARN    Inference API failed, using simulation 400
[00:36:54.265] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7917616184846565
[00:36:54.585] Server  ERROR   Backend returned 400
[00:36:54.611] Server  ERROR   Backend returned 400
[00:36:54.613] Server  ERROR   Backend returned 400
[00:36:54.724] Browser WARN    Inference API failed, using simulation 400
[00:36:54.724] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8109143360977006
[00:36:54.724] Browser WARN    Inference API failed, using simulation 400
[00:36:54.724] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.010097327893804409
[00:36:54.724] Browser WARN    Inference API failed, using simulation 400
[00:36:54.724] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.19538238960106524
[00:36:55.088] Server  ERROR   Backend returned 400
[00:36:55.107] Server  ERROR   Backend returned 400
[00:36:55.118] Server  ERROR   Backend returned 400
[00:36:55.224] Browser WARN    Inference API failed, using simulation 400
[00:36:55.224] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.20227960385893667
[00:36:55.224] Browser WARN    Inference API failed, using simulation 400
[00:36:55.224] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.29859688608532076
[00:36:55.224] Browser WARN    Inference API failed, using simulation 400
[00:36:55.224] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.41247948830412534
[00:36:55.592] Server  ERROR   Backend returned 400
[00:36:55.617] Server  ERROR   Backend returned 400
[00:36:55.622] Server  ERROR   Backend returned 400
[00:36:55.730] Browser WARN    Inference API failed, using simulation 400
[00:36:55.730] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.11085117995926647
[00:36:55.730] Browser WARN    Inference API failed, using simulation 400
[00:36:55.730] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9627035001067861
[00:36:55.730] Browser WARN    Inference API failed, using simulation 400
[00:36:55.730] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.12978700976810326
[00:36:56.079] Server  ERROR   Backend returned 400
[00:36:56.104] Server  ERROR   Backend returned 400
[00:36:56.109] Server  ERROR   Backend returned 400
[00:36:56.223] Browser WARN    Inference API failed, using simulation 400
[00:36:56.223] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.46427944660988923
[00:36:56.223] Browser WARN    Inference API failed, using simulation 400
[00:36:56.223] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9201901778230663
[00:36:56.223] Browser WARN    Inference API failed, using simulation 400
[00:36:56.223] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3726424644308839
[00:36:56.579] Server  ERROR   Backend returned 400
[00:36:56.600] Server  ERROR   Backend returned 400
[00:36:56.612] Server  ERROR   Backend returned 400
[00:36:56.717] Browser WARN    Inference API failed, using simulation 400
[00:36:56.717] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3787892328850263
[00:36:56.717] Browser WARN    Inference API failed, using simulation 400
[00:36:56.717] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3162482416665707
[00:36:56.717] Browser WARN    Inference API failed, using simulation 400
[00:36:56.717] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.17185717792479033
[00:36:57.079] Server  ERROR   Backend returned 400
[00:36:57.095] Server  ERROR   Backend returned 400
[00:36:57.107] Server  ERROR   Backend returned 400
[00:36:57.211] Browser WARN    Inference API failed, using simulation 400
[00:36:57.211] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.00531980353679723
[00:36:57.211] Browser WARN    Inference API failed, using simulation 400
[00:36:57.211] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9094362917806909
[00:36:57.211] Browser WARN    Inference API failed, using simulation 400
[00:36:57.211] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2172641971529758
[00:36:57.582] Server  ERROR   Backend returned 400
[00:36:57.610] Server  ERROR   Backend returned 400
[00:36:57.619] Server  ERROR   Backend returned 400
[00:36:57.726] Browser WARN    Inference API failed, using simulation 400
[00:36:57.726] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.03130834284465145
[00:36:57.726] Browser WARN    Inference API failed, using simulation 400
[00:36:57.726] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3294664922607807
[00:36:57.726] Browser WARN    Inference API failed, using simulation 400
[00:36:57.726] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4614000125403736
[00:36:58.076] Server  ERROR   Backend returned 400
[00:36:58.103] Server  ERROR   Backend returned 400
[00:36:58.106] Server  ERROR   Backend returned 400
[00:36:58.223] Browser WARN    Inference API failed, using simulation 400
[00:36:58.223] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.24076966082206236
[00:36:58.223] Browser WARN    Inference API failed, using simulation 400
[00:36:58.223] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.04174664410062118
[00:36:58.223] Browser WARN    Inference API failed, using simulation 400
[00:36:58.223] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9798296244838012
[00:36:58.584] Server  ERROR   Backend returned 400
[00:36:58.603] Server  ERROR   Backend returned 400
[00:36:58.617] Server  ERROR   Backend returned 400
[00:36:58.723] Browser WARN    Inference API failed, using simulation 400
[00:36:58.723] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.30810906933538534
[00:36:58.723] Browser WARN    Inference API failed, using simulation 400
[00:36:58.723] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.03311873421647682
[00:36:58.723] Browser WARN    Inference API failed, using simulation 400
[00:36:58.723] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7461741132659487
[00:36:59.087] Server  ERROR   Backend returned 400
[00:36:59.105] Server  ERROR   Backend returned 400
[00:36:59.115] Server  ERROR   Backend returned 400
[00:36:59.218] Browser WARN    Inference API failed, using simulation 400
[00:36:59.219] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.03250315571294943
[00:36:59.219] Browser WARN    Inference API failed, using simulation 400
[00:36:59.219] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.14411494878241643
[00:36:59.219] Browser WARN    Inference API failed, using simulation 400
[00:36:59.219] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.0025375516102865703
[00:36:59.576] Server  ERROR   Backend returned 400
[00:36:59.603] Server  ERROR   Backend returned 400
[00:36:59.627] Server  ERROR   Backend returned 400
[00:36:59.731] Browser WARN    Inference API failed, using simulation 400
[00:36:59.731] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.44249795097823597
[00:36:59.731] Browser WARN    Inference API failed, using simulation 400
[00:36:59.731] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.48778133447215843
[00:36:59.731] Browser WARN    Inference API failed, using simulation 400
[00:36:59.731] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8252140182916019
[00:37:00.079] Server  ERROR   Backend returned 400
[00:37:00.096] Server  ERROR   Backend returned 400
[00:37:00.110] Server  ERROR   Backend returned 400
[00:37:00.214] Browser WARN    Inference API failed, using simulation 400
[00:37:00.214] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3801043061784319
[00:37:00.214] Browser WARN    Inference API failed, using simulation 400
[00:37:00.214] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.21120422204142109
[00:37:00.214] Browser WARN    Inference API failed, using simulation 400
[00:37:00.214] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9098870621538242
[00:37:00.591] Server  ERROR   Backend returned 400
[00:37:00.616] Server  ERROR   Backend returned 400
[00:37:00.636] Server  ERROR   Backend returned 400
[00:37:00.740] Browser WARN    Inference API failed, using simulation 400
[00:37:00.740] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.14569455238013412
[00:37:00.740] Browser WARN    Inference API failed, using simulation 400
[00:37:00.740] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4873409059663658
[00:37:00.740] Browser WARN    Inference API failed, using simulation 400
[00:37:00.740] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8771956571290223
[00:37:01.078] Server  ERROR   Backend returned 400
[00:37:01.110] Server  ERROR   Backend returned 400
[00:37:01.114] Server  ERROR   Backend returned 400
[00:37:01.221] Browser WARN    Inference API failed, using simulation 400
[00:37:01.221] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.26500321149578426
[00:37:01.221] Browser WARN    Inference API failed, using simulation 400
[00:37:01.221] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.03156536170961166
[00:37:01.221] Browser WARN    Inference API failed, using simulation 400
[00:37:01.221] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7639479022103574
[00:37:01.591] Server  ERROR   Backend returned 400
[00:37:01.616] Server  ERROR   Backend returned 400
[00:37:01.618] Server  ERROR   Backend returned 400
[00:37:01.726] Browser WARN    Inference API failed, using simulation 400
[00:37:01.726] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2444435814868554
[00:37:01.726] Browser WARN    Inference API failed, using simulation 400
[00:37:01.726] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3459118598334981
[00:37:01.726] Browser WARN    Inference API failed, using simulation 400
[00:37:01.726] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.45105358982963795
[00:37:02.082] Server  ERROR   Backend returned 400
[00:37:02.099] Server  ERROR   Backend returned 400
[00:37:02.113] Server  ERROR   Backend returned 400
[00:37:02.218] Browser WARN    Inference API failed, using simulation 400
[00:37:02.218] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9699877929510086
[00:37:02.218] Browser WARN    Inference API failed, using simulation 400
[00:37:02.218] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7650379159405591
[00:37:02.218] Browser WARN    Inference API failed, using simulation 400
[00:37:02.218] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.31355498144024435
[00:37:02.613] Server  ERROR   Backend returned 400
[00:37:02.643] Server  ERROR   Backend returned 400
[00:37:02.657] Server  ERROR   Backend returned 400
[00:37:02.761] Browser WARN    Inference API failed, using simulation 400
[00:37:02.761] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.41318018530536577
[00:37:02.761] Browser WARN    Inference API failed, using simulation 400
[00:37:02.761] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9849892495105554
[00:37:02.761] Browser WARN    Inference API failed, using simulation 400
[00:37:02.761] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.38941083817668726
[00:37:03.086] Server  ERROR   Backend returned 400
[00:37:03.098] Server  ERROR   Backend returned 400
[00:37:03.112] Server  ERROR   Backend returned 400
[00:37:03.217] Browser WARN    Inference API failed, using simulation 400
[00:37:03.217] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.49423259425634897
[00:37:03.217] Browser WARN    Inference API failed, using simulation 400
[00:37:03.217] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.49900598305699767
[00:37:03.217] Browser WARN    Inference API failed, using simulation 400
[00:37:03.217] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.042226939465468294
[00:37:03.587] Server  ERROR   Backend returned 400
[00:37:03.608] Server  ERROR   Backend returned 400
[00:37:03.620] Server  ERROR   Backend returned 400
[00:37:03.725] Browser WARN    Inference API failed, using simulation 400
[00:37:03.725] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2629908787680256
[00:37:03.725] Browser WARN    Inference API failed, using simulation 400
[00:37:03.725] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.15262434970563332
[00:37:03.725] Browser WARN    Inference API failed, using simulation 400
[00:37:03.725] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.883043599354397
[00:37:04.146] Server  ERROR   Backend returned 400
[00:37:04.217] Server  ERROR   Backend returned 400
[00:37:04.224] Server  ERROR   Backend returned 400
[00:37:04.331] Browser WARN    Inference API failed, using simulation 400
[00:37:04.331] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7717505200059211
[00:37:04.331] Browser WARN    Inference API failed, using simulation 400
[00:37:04.331] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.795874119108163
[00:37:04.331] Browser WARN    Inference API failed, using simulation 400
[00:37:04.331] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4722193272600762
[00:37:04.585] Server  ERROR   Backend returned 400
[00:37:04.600] Server  ERROR   Backend returned 400
[00:37:04.614] Server  ERROR   Backend returned 400
[00:37:04.719] Browser WARN    Inference API failed, using simulation 400
[00:37:04.719] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8008123210948306
[00:37:04.719] Browser WARN    Inference API failed, using simulation 400
[00:37:04.719] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3169327879091197
[00:37:04.719] Browser WARN    Inference API failed, using simulation 400
[00:37:04.719] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.946234779867093
[00:37:05.089] Server  ERROR   Backend returned 400
[00:37:05.109] Server  ERROR   Backend returned 400
[00:37:05.122] Server  ERROR   Backend returned 400
[00:37:05.226] Browser WARN    Inference API failed, using simulation 400
[00:37:05.226] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.708277158768581
[00:37:05.226] Browser WARN    Inference API failed, using simulation 400
[00:37:05.226] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9426437369049021
[00:37:05.226] Browser WARN    Inference API failed, using simulation 400
[00:37:05.226] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.26278582144462775
[00:37:05.582] Server  ERROR   Backend returned 400
[00:37:05.599] Server  ERROR   Backend returned 400
[00:37:05.613] Server  ERROR   Backend returned 400
[00:37:05.718] Browser WARN    Inference API failed, using simulation 400
[00:37:05.718] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4139716894537078
[00:37:05.718] Browser WARN    Inference API failed, using simulation 400
[00:37:05.718] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1959369058890736
[00:37:05.718] Browser WARN    Inference API failed, using simulation 400
[00:37:05.718] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8326036028194312
[00:37:06.085] Server  ERROR   Backend returned 400
[00:37:06.110] Server  ERROR   Backend returned 400
[00:37:06.127] Server  ERROR   Backend returned 400
[00:37:06.233] Browser WARN    Inference API failed, using simulation 400
[00:37:06.233] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4780296337761893
[00:37:06.233] Browser WARN    Inference API failed, using simulation 400
[00:37:06.233] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2843667549067776
[00:37:06.233] Browser WARN    Inference API failed, using simulation 400
[00:37:06.233] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.18166619351089963
[00:37:06.584] Server  ERROR   Backend returned 400
[00:37:06.598] Server  ERROR   Backend returned 400
[00:37:06.611] Server  ERROR   Backend returned 400
[00:37:06.716] Browser WARN    Inference API failed, using simulation 400
[00:37:06.716] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2655222192884593
[00:37:06.716] Browser WARN    Inference API failed, using simulation 400
[00:37:06.716] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.27607613711544615
[00:37:06.716] Browser WARN    Inference API failed, using simulation 400
[00:37:06.716] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.005409624373196031
[00:37:07.078] Server  ERROR   Backend returned 400
[00:37:07.103] Server  ERROR   Backend returned 400
[00:37:07.115] Server  ERROR   Backend returned 400
[00:37:07.218] Browser WARN    Inference API failed, using simulation 400
[00:37:07.218] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8933980729032682
[00:37:07.219] Browser WARN    Inference API failed, using simulation 400
[00:37:07.219] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2263462806445124
[00:37:07.219] Browser WARN    Inference API failed, using simulation 400
[00:37:07.219] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3748212125154666
[00:37:07.579] Server  ERROR   Backend returned 400
[00:37:07.601] Server  ERROR   Backend returned 400
[00:37:07.615] Server  ERROR   Backend returned 400
[00:37:07.731] Browser WARN    Inference API failed, using simulation 400
[00:37:07.731] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.24016000820827643
[00:37:07.731] Browser WARN    Inference API failed, using simulation 400
[00:37:07.731] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9342925472297687
[00:37:07.731] Browser WARN    Inference API failed, using simulation 400
[00:37:07.731] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.40490373542290353
[00:37:08.081] Server  ERROR   Backend returned 400
[00:37:08.098] Server  ERROR   Backend returned 400
[00:37:08.121] Server  ERROR   Backend returned 400
[00:37:08.225] Browser WARN    Inference API failed, using simulation 400
[00:37:08.225] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.21391913705686527
[00:37:08.225] Browser WARN    Inference API failed, using simulation 400
[00:37:08.225] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.10724992139470069
[00:37:08.225] Browser WARN    Inference API failed, using simulation 400
[00:37:08.225] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2514027188720552
[00:37:08.580] Server  ERROR   Backend returned 400
[00:37:08.613] Server  ERROR   Backend returned 400
[00:37:08.619] Server  ERROR   Backend returned 400
[00:37:08.727] Browser WARN    Inference API failed, using simulation 400
[00:37:08.727] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.24138416174184735
[00:37:08.727] Browser WARN    Inference API failed, using simulation 400
[00:37:08.727] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.34702305979255726
[00:37:08.727] Browser WARN    Inference API failed, using simulation 400
[00:37:08.727] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7948517174062151
[00:37:09.080] Server  ERROR   Backend returned 400
[00:37:09.095] Server  ERROR   Backend returned 400
[00:37:09.111] Server  ERROR   Backend returned 400
[00:37:09.220] Browser WARN    Inference API failed, using simulation 400
[00:37:09.220] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3131230301810502
[00:37:09.220] Browser WARN    Inference API failed, using simulation 400
[00:37:09.220] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7542922803249839
[00:37:09.220] Browser WARN    Inference API failed, using simulation 400
[00:37:09.221] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.27886082880011137
[00:37:09.580] Server  ERROR   Backend returned 400
[00:37:09.596] Server  ERROR   Backend returned 400
[00:37:09.622] Server  ERROR   Backend returned 400
[00:37:09.729] Browser WARN    Inference API failed, using simulation 400
[00:37:09.729] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.49556438792674035
[00:37:09.729] Browser WARN    Inference API failed, using simulation 400
[00:37:09.729] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.34819664858995725
[00:37:09.729] Browser WARN    Inference API failed, using simulation 400
[00:37:09.729] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.34099069780278934
[00:37:10.093] Server  ERROR   Backend returned 400
[00:37:10.115] Server  ERROR   Backend returned 400
[00:37:10.126] Server  ERROR   Backend returned 400
[00:37:10.231] Browser WARN    Inference API failed, using simulation 400
[00:37:10.231] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8471716657317317
[00:37:10.231] Browser WARN    Inference API failed, using simulation 400
[00:37:10.231] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1283879292931694
[00:37:10.231] Browser WARN    Inference API failed, using simulation 400
[00:37:10.231] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.018376415703346805
[00:37:10.579] Server  ERROR   Backend returned 400
[00:37:10.603] Server  ERROR   Backend returned 400
[00:37:10.617] Server  ERROR   Backend returned 400
[00:37:10.721] Browser WARN    Inference API failed, using simulation 400
[00:37:10.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2488023857793492
[00:37:10.721] Browser WARN    Inference API failed, using simulation 400
[00:37:10.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3244185775758233
[00:37:10.721] Browser WARN    Inference API failed, using simulation 400
[00:37:10.721] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9828709177854165
[00:37:11.079] Server  ERROR   Backend returned 400
[00:37:11.110] Server  ERROR   Backend returned 400
[00:37:11.114] Server  ERROR   Backend returned 400
[00:37:11.229] Browser WARN    Inference API failed, using simulation 400
[00:37:11.229] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.700300093276692
[00:37:11.229] Browser WARN    Inference API failed, using simulation 400
[00:37:11.229] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.18418592344162887
[00:37:11.229] Browser WARN    Inference API failed, using simulation 400
[00:37:11.229] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7712625917845939
[00:37:11.579] Server  ERROR   Backend returned 400
[00:37:11.605] Server  ERROR   Backend returned 400
[00:37:11.618] Server  ERROR   Backend returned 400
[00:37:11.722] Browser WARN    Inference API failed, using simulation 400
[00:37:11.722] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.08169057349741965
[00:37:11.722] Browser WARN    Inference API failed, using simulation 400
[00:37:11.722] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.958769922252226
[00:37:11.722] Browser WARN    Inference API failed, using simulation 400
[00:37:11.722] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.24419712931117749
[00:37:12.093] Server  ERROR   Backend returned 400
[00:37:12.117] Server  ERROR   Backend returned 400
[00:37:12.119] Server  ERROR   Backend returned 400
[00:37:12.227] Browser WARN    Inference API failed, using simulation 400
[00:37:12.227] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.25657319242205173
[00:37:12.227] Browser WARN    Inference API failed, using simulation 400
[00:37:12.227] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3899394844179569
[00:37:12.227] Browser WARN    Inference API failed, using simulation 400
[00:37:12.227] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.27992899906885393
[00:37:12.577] Server  ERROR   Backend returned 400
[00:37:12.599] Server  ERROR   Backend returned 400
[00:37:12.610] Server  ERROR   Backend returned 400
[00:37:12.715] Browser WARN    Inference API failed, using simulation 400
[00:37:12.715] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2908822570092897
[00:37:12.715] Browser WARN    Inference API failed, using simulation 400
[00:37:12.715] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.33188132027328515
[00:37:12.715] Browser WARN    Inference API failed, using simulation 400
[00:37:12.715] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.37936858038789095
[00:37:13.081] Server  ERROR   Backend returned 400
[00:37:13.096] Server  ERROR   Backend returned 400
[00:37:13.113] Server  ERROR   Backend returned 400
[00:37:13.218] Browser WARN    Inference API failed, using simulation 400
[00:37:13.218] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.14933733104474955
[00:37:13.218] Browser WARN    Inference API failed, using simulation 400
[00:37:13.218] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.08353184452529144
[00:37:13.218] Browser WARN    Inference API failed, using simulation 400
[00:37:13.218] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4193734087400363
[00:37:13.581] Server  ERROR   Backend returned 400
[00:37:13.596] Server  ERROR   Backend returned 400
[00:37:13.614] Server  ERROR   Backend returned 400
[00:37:13.718] Browser WARN    Inference API failed, using simulation 400
[00:37:13.718] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8045894795819588
[00:37:13.718] Browser WARN    Inference API failed, using simulation 400
[00:37:13.718] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8993364164703139
[00:37:13.718] Browser WARN    Inference API failed, using simulation 400
[00:37:13.718] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.12465259465778677
[00:37:14.095] Server  ERROR   Backend returned 400
[00:37:14.116] Server  ERROR   Backend returned 400
[00:37:14.129] Server  ERROR   Backend returned 400
[00:37:14.233] Browser WARN    Inference API failed, using simulation 400
[00:37:14.233] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2210243762748949
[00:37:14.233] Browser WARN    Inference API failed, using simulation 400
[00:37:14.233] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.05083297179714197
[00:37:14.233] Browser WARN    Inference API failed, using simulation 400
[00:37:14.233] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4799129969122933
[00:37:14.579] Server  ERROR   Backend returned 400
[00:37:14.600] Server  ERROR   Backend returned 400
[00:37:14.611] Server  ERROR   Backend returned 400
[00:37:14.716] Browser WARN    Inference API failed, using simulation 400
[00:37:14.716] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.20637674687231894
[00:37:14.716] Browser WARN    Inference API failed, using simulation 400
[00:37:14.716] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8234306557303334
[00:37:14.716] Browser WARN    Inference API failed, using simulation 400
[00:37:14.716] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.04218874177730558
[00:37:15.103] Server  ERROR   Backend returned 400
[00:37:15.131] Server  ERROR   Backend returned 400
[00:37:15.152] Server  ERROR   Backend returned 400
[00:37:15.258] Browser WARN    Inference API failed, using simulation 400
[00:37:15.258] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2336379598150526
[00:37:15.258] Browser WARN    Inference API failed, using simulation 400
[00:37:15.258] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7453758907181331
[00:37:15.258] Browser WARN    Inference API failed, using simulation 400
[00:37:15.258] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1350456789914048
[00:37:15.587] Server  ERROR   Backend returned 400
[00:37:15.601] Server  ERROR   Backend returned 400
[00:37:15.617] Server  ERROR   Backend returned 400
[00:37:15.721] Browser WARN    Inference API failed, using simulation 400
[00:37:15.721] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.944985625620983
[00:37:15.721] Browser WARN    Inference API failed, using simulation 400
[00:37:15.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4664777940798335
[00:37:15.721] Browser WARN    Inference API failed, using simulation 400
[00:37:15.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3696549436056431
[00:37:16.081] Server  ERROR   Backend returned 400
[00:37:16.108] Server  ERROR   Backend returned 400
[00:37:16.119] Server  ERROR   Backend returned 400
[00:37:16.230] Browser WARN    Inference API failed, using simulation 400
[00:37:16.230] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.30851103119292395
[00:37:16.230] Browser WARN    Inference API failed, using simulation 400
[00:37:16.230] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.05967926403900542
[00:37:16.230] Browser WARN    Inference API failed, using simulation 400
[00:37:16.230] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7249814751701309
[00:37:16.583] Server  ERROR   Backend returned 400
[00:37:16.604] Server  ERROR   Backend returned 400
[00:37:16.617] Server  ERROR   Backend returned 400
[00:37:16.728] Browser WARN    Inference API failed, using simulation 400
[00:37:16.728] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.23334791172901787
[00:37:16.728] Browser WARN    Inference API failed, using simulation 400
[00:37:16.728] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.025235274078649172
[00:37:16.728] Browser WARN    Inference API failed, using simulation 400
[00:37:16.728] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.24049768060027166
[00:37:17.078] Server  ERROR   Backend returned 400
[00:37:17.104] Server  ERROR   Backend returned 400
[00:37:17.115] Server  ERROR   Backend returned 400
[00:37:17.218] Browser WARN    Inference API failed, using simulation 400
[00:37:17.218] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.995450872865637
[00:37:17.218] Browser WARN    Inference API failed, using simulation 400
[00:37:17.218] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9727707854228342
[00:37:17.218] Browser WARN    Inference API failed, using simulation 400
[00:37:17.218] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.15612269678479956
[00:37:17.587] Server  ERROR   Backend returned 400
[00:37:17.610] Server  ERROR   Backend returned 400
[00:37:17.615] Server  ERROR   Backend returned 400
[00:37:17.727] Browser WARN    Inference API failed, using simulation 400
[00:37:17.727] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.04747393767119834
[00:37:17.727] Browser WARN    Inference API failed, using simulation 400
[00:37:17.727] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.17117933684105086
[00:37:17.727] Browser WARN    Inference API failed, using simulation 400
[00:37:17.727] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.38224811450778096
[00:37:18.079] Server  ERROR   Backend returned 400
[00:37:18.107] Server  ERROR   Backend returned 400
[00:37:18.109] Server  ERROR   Backend returned 400
[00:37:18.222] Browser WARN    Inference API failed, using simulation 400
[00:37:18.222] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.283040624046815
[00:37:18.223] Browser WARN    Inference API failed, using simulation 400
[00:37:18.223] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7787939760775539
[00:37:18.223] Browser WARN    Inference API failed, using simulation 400
[00:37:18.223] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.25545112382360163
[00:37:18.581] Server  ERROR   Backend returned 400
[00:37:18.614] Server  ERROR   Backend returned 400
[00:37:18.618] Server  ERROR   Backend returned 400
[00:37:18.735] Browser WARN    Inference API failed, using simulation 400
[00:37:18.735] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.37232689957301635
[00:37:18.735] Browser WARN    Inference API failed, using simulation 400
[00:37:18.735] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2968183707420267
[00:37:18.735] Browser WARN    Inference API failed, using simulation 400
[00:37:18.735] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.24992213460115797
[00:37:19.087] Server  ERROR   Backend returned 400
[00:37:19.097] Server  ERROR   Backend returned 400
[00:37:19.112] Server  ERROR   Backend returned 400
[00:37:19.228] Browser WARN    Inference API failed, using simulation 400
[00:37:19.228] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.11345215776674883
[00:37:19.228] Browser WARN    Inference API failed, using simulation 400
[00:37:19.228] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4814279931215943
[00:37:19.228] Browser WARN    Inference API failed, using simulation 400
[00:37:19.228] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.19186446959097203
[00:37:19.581] Server  ERROR   Backend returned 400
[00:37:19.596] Server  ERROR   Backend returned 400
[00:37:19.617] Server  ERROR   Backend returned 400
[00:37:19.726] Browser WARN    Inference API failed, using simulation 400
[00:37:19.726] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.787430717429983
[00:37:19.726] Browser WARN    Inference API failed, using simulation 400
[00:37:19.726] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.22649928669606656
[00:37:19.726] Browser WARN    Inference API failed, using simulation 400
[00:37:19.726] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.10462245849485868
[00:37:20.078] Server  ERROR   Backend returned 400
[00:37:20.107] Server  ERROR   Backend returned 400
[00:37:20.110] Server  ERROR   Backend returned 400
[00:37:20.219] Browser WARN    Inference API failed, using simulation 400
[00:37:20.219] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7754938554857715
[00:37:20.219] Browser WARN    Inference API failed, using simulation 400
[00:37:20.219] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.481687373263375
[00:37:20.220] Browser WARN    Inference API failed, using simulation 400
[00:37:20.220] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4358504189967903
[00:37:20.582] Server  ERROR   Backend returned 400
[00:37:20.602] Server  ERROR   Backend returned 400
[00:37:20.616] Server  ERROR   Backend returned 400
[00:37:20.720] Browser WARN    Inference API failed, using simulation 400
[00:37:20.721] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9356150039614947
[00:37:20.721] Browser WARN    Inference API failed, using simulation 400
[00:37:20.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1697207980891146
[00:37:20.721] Browser WARN    Inference API failed, using simulation 400
[00:37:20.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.09695171506633066
[00:37:21.095] Server  ERROR   Backend returned 400
[00:37:21.136] Server  ERROR   Backend returned 400
[00:37:21.152] Server  ERROR   Backend returned 400
[00:37:21.259] Browser WARN    Inference API failed, using simulation 400
[00:37:21.259] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.06553328016326332
[00:37:21.259] Browser WARN    Inference API failed, using simulation 400
[00:37:21.259] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2716035990010083
[00:37:21.259] Browser WARN    Inference API failed, using simulation 400
[00:37:21.259] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.027001141965738662
[00:37:21.584] Server  ERROR   Backend returned 400
[00:37:21.602] Server  ERROR   Backend returned 400
[00:37:21.616] Server  ERROR   Backend returned 400
[00:37:21.720] Browser WARN    Inference API failed, using simulation 400
[00:37:21.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.07207391881321745
[00:37:21.720] Browser WARN    Inference API failed, using simulation 400
[00:37:21.720] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.0900125630624517
[00:37:21.720] Browser WARN    Inference API failed, using simulation 400
[00:37:21.720] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9034703651594833
[00:37:22.086] Server  ERROR   Backend returned 400
[00:37:22.103] Server  ERROR   Backend returned 400
[00:37:22.116] Server  ERROR   Backend returned 400
[00:37:22.219] Browser WARN    Inference API failed, using simulation 400
[00:37:22.220] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2386893559379118
[00:37:22.220] Browser WARN    Inference API failed, using simulation 400
[00:37:22.220] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.32215543680832054
[00:37:22.220] Browser WARN    Inference API failed, using simulation 400
[00:37:22.220] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3387792110112925
[00:37:22.588] Server  ERROR   Backend returned 400
[00:37:22.606] Server  ERROR   Backend returned 400
[00:37:22.617] Server  ERROR   Backend returned 400
[00:37:22.721] Browser WARN    Inference API failed, using simulation 400
[00:37:22.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4948957301975167
[00:37:22.721] Browser WARN    Inference API failed, using simulation 400
[00:37:22.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.17029375477462888
[00:37:22.721] Browser WARN    Inference API failed, using simulation 400
[00:37:22.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3596283646977173
[00:37:23.080] Server  ERROR   Backend returned 400
[00:37:23.103] Server  ERROR   Backend returned 400
[00:37:23.116] Server  ERROR   Backend returned 400
[00:37:23.221] Browser WARN    Inference API failed, using simulation 400
[00:37:23.221] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7700713102031064
[00:37:23.221] Browser WARN    Inference API failed, using simulation 400
[00:37:23.221] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7793434353057238
[00:37:23.221] Browser WARN    Inference API failed, using simulation 400
[00:37:23.221] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.31279261109515794
[00:37:23.583] Server  ERROR   Backend returned 400
[00:37:23.600] Server  ERROR   Backend returned 400
[00:37:23.614] Server  ERROR   Backend returned 400
[00:37:23.718] Browser WARN    Inference API failed, using simulation 400
[00:37:23.718] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.14472017981347718
[00:37:23.718] Browser WARN    Inference API failed, using simulation 400
[00:37:23.718] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8302906652678517
[00:37:23.718] Browser WARN    Inference API failed, using simulation 400
[00:37:23.718] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9969152522543758
[00:37:24.080] Server  ERROR   Backend returned 400
[00:37:24.111] Server  ERROR   Backend returned 400
[00:37:24.123] Server  ERROR   Backend returned 400
[00:37:24.227] Browser WARN    Inference API failed, using simulation 400
[00:37:24.227] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.49326025651661287
[00:37:24.227] Browser WARN    Inference API failed, using simulation 400
[00:37:24.227] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.876898135691988
[00:37:24.227] Browser WARN    Inference API failed, using simulation 400
[00:37:24.227] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.12689096917587428
[00:37:24.581] Server  ERROR   Backend returned 400
[00:37:24.600] Server  ERROR   Backend returned 400
[00:37:24.615] Server  ERROR   Backend returned 400
[00:37:24.720] Browser WARN    Inference API failed, using simulation 400
[00:37:24.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.028794578881643718
[00:37:24.720] Browser WARN    Inference API failed, using simulation 400
[00:37:24.720] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.36154456670863505
[00:37:24.720] Browser WARN    Inference API failed, using simulation 400
[00:37:24.720] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.08638579690961451
[00:37:25.079] Server  ERROR   Backend returned 400
[00:37:25.114] Server  ERROR   Backend returned 400
[00:37:25.120] Server  ERROR   Backend returned 400
[00:37:25.225] Browser WARN    Inference API failed, using simulation 400
[00:37:25.225] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8679782794831365
[00:37:25.225] Browser WARN    Inference API failed, using simulation 400
[00:37:25.225] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7114025795771924
[00:37:25.225] Browser WARN    Inference API failed, using simulation 400
[00:37:25.225] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.983921966926748
[00:37:25.588] Server  ERROR   Backend returned 400
[00:37:25.613] Server  ERROR   Backend returned 400
[00:37:25.616] Server  ERROR   Backend returned 400
[00:37:25.724] Browser WARN    Inference API failed, using simulation 400
[00:37:25.724] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9674295264562822
[00:37:25.724] Browser WARN    Inference API failed, using simulation 400
[00:37:25.724] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.38987552909839085
[00:37:25.724] Browser WARN    Inference API failed, using simulation 400
[00:37:25.724] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.05430117754428637
[00:37:26.078] Server  ERROR   Backend returned 400
[00:37:26.111] Server  ERROR   Backend returned 400
[00:37:26.114] Server  ERROR   Backend returned 400
[00:37:26.223] Browser WARN    Inference API failed, using simulation 400
[00:37:26.223] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.18363993772155618
[00:37:26.223] Browser WARN    Inference API failed, using simulation 400
[00:37:26.223] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1048671372587876
[00:37:26.223] Browser WARN    Inference API failed, using simulation 400
[00:37:26.223] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.47721903842972085
[00:37:26.588] Server  ERROR   Backend returned 400
[00:37:26.618] Server  ERROR   Backend returned 400
[00:37:26.630] Server  ERROR   Backend returned 400
[00:37:26.736] Browser WARN    Inference API failed, using simulation 400
[00:37:26.736] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8991589088117324
[00:37:26.736] Browser WARN    Inference API failed, using simulation 400
[00:37:26.736] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4957947312777416
[00:37:26.736] Browser WARN    Inference API failed, using simulation 400
[00:37:26.736] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.17219155244538392
[00:37:27.103] Server  ERROR   Backend returned 400
[00:37:27.144] Server  ERROR   Backend returned 400
[00:37:27.164] Server  ERROR   Backend returned 400
[00:37:27.269] Browser WARN    Inference API failed, using simulation 400
[00:37:27.269] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.19862897731209883
[00:37:27.269] Browser WARN    Inference API failed, using simulation 400
[00:37:27.269] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.16342577592563173
[00:37:27.269] Browser WARN    Inference API failed, using simulation 400
[00:37:27.269] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.022031363104901003
[00:37:27.578] Server  ERROR   Backend returned 400
[00:37:27.601] Server  ERROR   Backend returned 400
[00:37:27.603] Server  ERROR   Backend returned 400
[00:37:27.714] Browser WARN    Inference API failed, using simulation 400
[00:37:27.714] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2355260773752899
[00:37:27.714] Browser WARN    Inference API failed, using simulation 400
[00:37:27.714] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.06353727879114185
[00:37:27.714] Browser WARN    Inference API failed, using simulation 400
[00:37:27.714] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.12697724971182373
[00:37:28.101] Server  ERROR   Backend returned 400
[00:37:28.114] Server  ERROR   Backend returned 400
[00:37:28.130] Server  ERROR   Backend returned 400
[00:37:28.234] Browser WARN    Inference API failed, using simulation 400
[00:37:28.234] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.788810382664323
[00:37:28.234] Browser WARN    Inference API failed, using simulation 400
[00:37:28.234] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1647272554673001
[00:37:28.234] Browser WARN    Inference API failed, using simulation 400
[00:37:28.234] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8302160527056006
[00:37:28.727] Server  ERROR   Backend returned 400
[00:37:28.732] Server  ERROR   Backend returned 400
[00:37:28.741] Server  ERROR   Backend returned 400
[00:37:28.868] Browser WARN    Inference API failed, using simulation 400
[00:37:28.868] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.26322315830971
[00:37:28.868] Browser WARN    Inference API failed, using simulation 400
[00:37:28.868] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3625310024399844
[00:37:28.868] Browser WARN    Inference API failed, using simulation 400
[00:37:28.868] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.23203422854628142
[00:37:29.092] Server  ERROR   Backend returned 400
[00:37:29.109] Server  ERROR   Backend returned 400
[00:37:29.129] Server  ERROR   Backend returned 400
[00:37:29.234] Browser WARN    Inference API failed, using simulation 400
[00:37:29.234] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.05447597582160779
[00:37:29.234] Browser WARN    Inference API failed, using simulation 400
[00:37:29.234] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2520618738929233
[00:37:29.234] Browser WARN    Inference API failed, using simulation 400
[00:37:29.234] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.029100522127528783
[00:37:29.599] Server  ERROR   Backend returned 400
[00:37:29.611] Server  ERROR   Backend returned 400
[00:37:29.647] Server  ERROR   Backend returned 400
[00:37:29.751] Browser WARN    Inference API failed, using simulation 400
[00:37:29.751] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2468483005608425
[00:37:29.751] Browser WARN    Inference API failed, using simulation 400
[00:37:29.751] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.041025403639620484
[00:37:29.751] Browser WARN    Inference API failed, using simulation 400
[00:37:29.751] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.06848173443791872
[00:37:30.078] Server  ERROR   Backend returned 400
[00:37:30.109] Server  ERROR   Backend returned 400
[00:37:30.115] Server  ERROR   Backend returned 400
[00:37:30.223] Browser WARN    Inference API failed, using simulation 400
[00:37:30.223] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3601968620359171
[00:37:30.224] Browser WARN    Inference API failed, using simulation 400
[00:37:30.224] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.16145526907031577
[00:37:30.224] Browser WARN    Inference API failed, using simulation 400
[00:37:30.224] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.251927809634785
[00:37:30.581] Server  ERROR   Backend returned 400
[00:37:30.609] Server  ERROR   Backend returned 400
[00:37:30.631] Server  ERROR   Backend returned 400
[00:37:30.734] Browser WARN    Inference API failed, using simulation 400
[00:37:30.734] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7756455760143006
[00:37:30.734] Browser WARN    Inference API failed, using simulation 400
[00:37:30.734] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7573647903669964
[00:37:30.734] Browser WARN    Inference API failed, using simulation 400
[00:37:30.734] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.20456990901550237
[00:37:31.087] Server  ERROR   Backend returned 400
[00:37:31.107] Server  ERROR   Backend returned 400
[00:37:31.118] Server  ERROR   Backend returned 400
[00:37:31.222] Browser WARN    Inference API failed, using simulation 400
[00:37:31.222] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.48069476709527886
[00:37:31.222] Browser WARN    Inference API failed, using simulation 400
[00:37:31.222] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3525985001374452
[00:37:31.222] Browser WARN    Inference API failed, using simulation 400
[00:37:31.222] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.734844902141664
[00:37:31.586] Server  ERROR   Backend returned 400
[00:37:31.601] Server  ERROR   Backend returned 400
[00:37:31.618] Server  ERROR   Backend returned 400
[00:37:31.723] Browser WARN    Inference API failed, using simulation 400
[00:37:31.723] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7518348466860152
[00:37:31.723] Browser WARN    Inference API failed, using simulation 400
[00:37:31.723] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.33772327799555657
[00:37:31.723] Browser WARN    Inference API failed, using simulation 400
[00:37:31.723] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7510508833595562
[00:37:32.108] Server  ERROR   Backend returned 400
[00:37:32.130] Server  ERROR   Backend returned 400
[00:37:32.145] Server  ERROR   Backend returned 400
[00:37:32.250] Browser WARN    Inference API failed, using simulation 400
[00:37:32.250] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.23241723736311615
[00:37:32.250] Browser WARN    Inference API failed, using simulation 400
[00:37:32.250] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.004763757558799819
[00:37:32.250] Browser WARN    Inference API failed, using simulation 400
[00:37:32.250] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.21902566930989725
[00:37:32.584] Server  ERROR   Backend returned 400
[00:37:32.603] Server  ERROR   Backend returned 400
[00:37:32.627] Server  ERROR   Backend returned 400
[00:37:32.731] Browser WARN    Inference API failed, using simulation 400
[00:37:32.731] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8842265735141122
[00:37:32.731] Browser WARN    Inference API failed, using simulation 400
[00:37:32.731] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3171071653260704
[00:37:32.731] Browser WARN    Inference API failed, using simulation 400
[00:37:32.731] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3130934703284996
[00:37:33.090] Server  ERROR   Backend returned 400
[00:37:33.107] Server  ERROR   Backend returned 400
[00:37:33.121] Server  ERROR   Backend returned 400
[00:37:33.225] Browser WARN    Inference API failed, using simulation 400
[00:37:33.225] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.43148338716511797
[00:37:33.225] Browser WARN    Inference API failed, using simulation 400
[00:37:33.225] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.06996735250201264
[00:37:33.225] Browser WARN    Inference API failed, using simulation 400
[00:37:33.225] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7673772539460095
[00:37:33.584] Server  ERROR   Backend returned 400
[00:37:33.609] Server  ERROR   Backend returned 400
[00:37:33.621] Server  ERROR   Backend returned 400
[00:37:33.726] Browser WARN    Inference API failed, using simulation 400
[00:37:33.726] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3848082770965221
[00:37:33.726] Browser WARN    Inference API failed, using simulation 400
[00:37:33.726] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.26335642668337467
[00:37:33.726] Browser WARN    Inference API failed, using simulation 400
[00:37:33.726] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9462082852369216
[00:37:34.093] Server  ERROR   Backend returned 400
[00:37:34.158] Server  ERROR   Backend returned 400
[00:37:34.171] Server  ERROR   Backend returned 400
[00:37:34.277] Browser WARN    Inference API failed, using simulation 400
[00:37:34.278] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.06637112850198201
[00:37:34.278] Browser WARN    Inference API failed, using simulation 400
[00:37:34.278] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4371999194949846
[00:37:34.278] Browser WARN    Inference API failed, using simulation 400
[00:37:34.278] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.112966626672385
[00:37:34.579] Server  ERROR   Backend returned 400
[00:37:34.604] Server  ERROR   Backend returned 400
[00:37:34.617] Server  ERROR   Backend returned 400
[00:37:34.722] Browser WARN    Inference API failed, using simulation 400
[00:37:34.722] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1848238130252326
[00:37:34.722] Browser WARN    Inference API failed, using simulation 400
[00:37:34.722] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3822538063990661
[00:37:34.722] Browser WARN    Inference API failed, using simulation 400
[00:37:34.722] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3120890974498183
[00:37:35.085] Server  ERROR   Backend returned 400
[00:37:35.118] Server  ERROR   Backend returned 400
[00:37:35.132] Server  ERROR   Backend returned 400
[00:37:35.237] Browser WARN    Inference API failed, using simulation 400
[00:37:35.237] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2421745387835187
[00:37:35.237] Browser WARN    Inference API failed, using simulation 400
[00:37:35.237] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.02709610551061986
[00:37:35.237] Browser WARN    Inference API failed, using simulation 400
[00:37:35.237] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.09288184457097187
[00:37:35.580] Server  ERROR   Backend returned 400
[00:37:35.599] Server  ERROR   Backend returned 400
[00:37:35.611] Server  ERROR   Backend returned 400
[00:37:35.717] Browser WARN    Inference API failed, using simulation 400
[00:37:35.717] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9660075034880888
[00:37:35.717] Browser WARN    Inference API failed, using simulation 400
[00:37:35.717] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.89678163907442
[00:37:35.717] Browser WARN    Inference API failed, using simulation 400
[00:37:35.717] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.47564060423877974
[00:37:36.079] Server  ERROR   Backend returned 400
[00:37:36.110] Server  ERROR   Backend returned 400
[00:37:36.114] Server  ERROR   Backend returned 400
[00:37:36.222] Browser WARN    Inference API failed, using simulation 400
[00:37:36.222] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9310297174582689
[00:37:36.222] Browser WARN    Inference API failed, using simulation 400
[00:37:36.222] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2929501036953948
[00:37:36.222] Browser WARN    Inference API failed, using simulation 400
[00:37:36.222] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.23222123961330127
[00:37:36.576] Server  ERROR   Backend returned 400
[00:37:36.608] Server  ERROR   Backend returned 400
[00:37:36.612] Server  ERROR   Backend returned 400
[00:37:36.720] Browser WARN    Inference API failed, using simulation 400
[00:37:36.720] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9404689360054487
[00:37:36.720] Browser WARN    Inference API failed, using simulation 400
[00:37:36.720] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9252475684623678
[00:37:36.720] Browser WARN    Inference API failed, using simulation 400
[00:37:36.720] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9732255928995608
[00:37:37.082] Server  ERROR   Backend returned 400
[00:37:37.100] Server  ERROR   Backend returned 400
[00:37:37.113] Server  ERROR   Backend returned 400
[00:37:37.218] Browser WARN    Inference API failed, using simulation 400
[00:37:37.218] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8577098303438302
[00:37:37.218] Browser WARN    Inference API failed, using simulation 400
[00:37:37.218] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7289402242591831
[00:37:37.218] Browser WARN    Inference API failed, using simulation 400
[00:37:37.218] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3497864777768347
[00:37:37.583] Server  ERROR   Backend returned 400
[00:37:37.607] Server  ERROR   Backend returned 400
[00:37:37.609] Server  ERROR   Backend returned 400
[00:37:37.718] Browser WARN    Inference API failed, using simulation 400
[00:37:37.718] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9456511403812539
[00:37:37.718] Browser WARN    Inference API failed, using simulation 400
[00:37:37.718] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.318821793406019
[00:37:37.718] Browser WARN    Inference API failed, using simulation 400
[00:37:37.718] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9715548092374906
[00:37:38.077] Server  ERROR   Backend returned 400
[00:37:38.101] Server  ERROR   Backend returned 400
[00:37:38.103] Server  ERROR   Backend returned 400
[00:37:38.212] Browser WARN    Inference API failed, using simulation 400
[00:37:38.212] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4562004609841335
[00:37:38.212] Browser WARN    Inference API failed, using simulation 400
[00:37:38.212] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7330275101396203
[00:37:38.212] Browser WARN    Inference API failed, using simulation 400
[00:37:38.212] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.23061054384405694
[00:37:38.580] Server  ERROR   Backend returned 400
[00:37:38.602] Server  ERROR   Backend returned 400
[00:37:38.618] Server  ERROR   Backend returned 400
[00:37:38.721] Browser WARN    Inference API failed, using simulation 400
[00:37:38.721] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8204058773830183
[00:37:38.721] Browser WARN    Inference API failed, using simulation 400
[00:37:38.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.45592147700612246
[00:37:38.721] Browser WARN    Inference API failed, using simulation 400
[00:37:38.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.29608825595698374
[00:37:39.077] Server  ERROR   Backend returned 400
[00:37:39.107] Server  ERROR   Backend returned 400
[00:37:39.110] Server  ERROR   Backend returned 400
[00:37:39.218] Browser WARN    Inference API failed, using simulation 400
[00:37:39.218] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8832568626116186
[00:37:39.218] Browser WARN    Inference API failed, using simulation 400
[00:37:39.218] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3330737746954314
[00:37:39.218] Browser WARN    Inference API failed, using simulation 400
[00:37:39.218] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.30610193463421526
[00:37:39.579] Server  ERROR   Backend returned 400
[00:37:39.604] Server  ERROR   Backend returned 400
[00:37:39.616] Server  ERROR   Backend returned 400
[00:37:39.720] Browser WARN    Inference API failed, using simulation 400
[00:37:39.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.12204467814494163
[00:37:39.720] Browser WARN    Inference API failed, using simulation 400
[00:37:39.720] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.847208567714597
[00:37:39.720] Browser WARN    Inference API failed, using simulation 400
[00:37:39.720] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9572974633243441
[00:37:40.082] Server  ERROR   Backend returned 400
[00:37:40.101] Server  ERROR   Backend returned 400
[00:37:40.115] Server  ERROR   Backend returned 400
[00:37:40.219] Browser WARN    Inference API failed, using simulation 400
[00:37:40.219] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.21000258874799893
[00:37:40.219] Browser WARN    Inference API failed, using simulation 400
[00:37:40.219] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7666706688368173
[00:37:40.219] Browser WARN    Inference API failed, using simulation 400
[00:37:40.219] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.32974444616794146
[00:37:40.585] Server  ERROR   Backend returned 400
[00:37:40.614] Server  ERROR   Backend returned 400
[00:37:40.627] Server  ERROR   Backend returned 400
[00:37:40.731] Browser WARN    Inference API failed, using simulation 400
[00:37:40.731] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.968536986199976
[00:37:40.731] Browser WARN    Inference API failed, using simulation 400
[00:37:40.731] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.16016296841514754
[00:37:40.731] Browser WARN    Inference API failed, using simulation 400
[00:37:40.731] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9548769879401844
[00:37:41.083] Server  ERROR   Backend returned 400
[00:37:41.108] Server  ERROR   Backend returned 400
[00:37:41.134] Server  ERROR   Backend returned 400
[00:37:41.237] Browser WARN    Inference API failed, using simulation 400
[00:37:41.237] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4230426993121807
[00:37:41.237] Browser WARN    Inference API failed, using simulation 400
[00:37:41.237] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9168621017922367
[00:37:41.237] Browser WARN    Inference API failed, using simulation 400
[00:37:41.237] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7423411412411103
[00:37:41.578] Server  ERROR   Backend returned 400
[00:37:41.610] Server  ERROR   Backend returned 400
[00:37:41.633] Server  ERROR   Backend returned 400
[00:37:41.737] Browser WARN    Inference API failed, using simulation 400
[00:37:41.737] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.00882802326493537
[00:37:41.737] Browser WARN    Inference API failed, using simulation 400
[00:37:41.737] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.07510586906475686
[00:37:41.737] Browser WARN    Inference API failed, using simulation 400
[00:37:41.737] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7632308026872217
[00:37:42.086] Server  ERROR   Backend returned 400
[00:37:42.103] Server  ERROR   Backend returned 400
[00:37:42.115] Server  ERROR   Backend returned 400
[00:37:42.220] Browser WARN    Inference API failed, using simulation 400
[00:37:42.220] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8582206189692754
[00:37:42.220] Browser WARN    Inference API failed, using simulation 400
[00:37:42.220] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9467757357644911
[00:37:42.220] Browser WARN    Inference API failed, using simulation 400
[00:37:42.220] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3487326280966771
[00:37:42.580] Server  ERROR   Backend returned 400
[00:37:42.606] Server  ERROR   Backend returned 400
[00:37:42.622] Server  ERROR   Backend returned 400
[00:37:42.726] Browser WARN    Inference API failed, using simulation 400
[00:37:42.726] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7428149198414596
[00:37:42.726] Browser WARN    Inference API failed, using simulation 400
[00:37:42.726] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.839908089025999
[00:37:42.726] Browser WARN    Inference API failed, using simulation 400
[00:37:42.726] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8409122945747538
[00:37:43.078] Server  ERROR   Backend returned 400
[00:37:43.095] Server  ERROR   Backend returned 400
[00:37:43.113] Server  ERROR   Backend returned 400
[00:37:43.220] Browser WARN    Inference API failed, using simulation 400
[00:37:43.220] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3527506726747134
[00:37:43.220] Browser WARN    Inference API failed, using simulation 400
[00:37:43.220] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.12169290119153187
[00:37:43.220] Browser WARN    Inference API failed, using simulation 400
[00:37:43.220] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9437850241299809
[00:37:43.576] Server  ERROR   Backend returned 400
[00:37:43.600] Server  ERROR   Backend returned 400
[00:37:43.611] Server  ERROR   Backend returned 400
[00:37:43.733] Browser WARN    Inference API failed, using simulation 400
[00:37:43.733] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.37644782029129575
[00:37:43.733] Browser WARN    Inference API failed, using simulation 400
[00:37:43.733] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.29173726931759375
[00:37:43.733] Browser WARN    Inference API failed, using simulation 400
[00:37:43.733] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.38080781675654074
[00:37:44.650] Server  ERROR   Backend returned 400
[00:37:44.685] Server  ERROR   Backend returned 400
[00:37:44.719] Server  ERROR   Backend returned 400
[00:37:44.721] Server  ERROR   Backend returned 400
[00:37:44.726] Server  ERROR   Backend returned 400
[00:37:44.731] Server  ERROR   Backend returned 400
[00:37:44.847] Browser WARN    Inference API failed, using simulation 400
[00:37:44.848] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4354604570120013
[00:37:44.848] Browser WARN    Inference API failed, using simulation 400
[00:37:44.848] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8982843245641096
[00:37:44.848] Browser WARN    Inference API failed, using simulation 400
[00:37:44.848] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.32587289059656066
[00:37:44.848] Browser WARN    Inference API failed, using simulation 400
[00:37:44.848] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2976790697885475
[00:37:44.848] Browser WARN    Inference API failed, using simulation 400
[00:37:44.848] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.05192723809074945
[00:37:44.848] Browser WARN    Inference API failed, using simulation 400
[00:37:44.848] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.44253701208051677
[00:37:45.078] Server  ERROR   Backend returned 400
[00:37:45.102] Server  ERROR   Backend returned 400
[00:37:45.120] Server  ERROR   Backend returned 400
[00:37:45.226] Browser WARN    Inference API failed, using simulation 400
[00:37:45.226] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7497079723835526
[00:37:45.226] Browser WARN    Inference API failed, using simulation 400
[00:37:45.226] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7332036174235286
[00:37:45.226] Browser WARN    Inference API failed, using simulation 400
[00:37:45.226] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.013260611272241207
[00:37:45.587] Server  ERROR   Backend returned 400
[00:37:45.604] Server  ERROR   Backend returned 400
[00:37:45.617] Server  ERROR   Backend returned 400
[00:37:45.721] Browser WARN    Inference API failed, using simulation 400
[00:37:45.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1929585580949047
[00:37:45.722] Browser WARN    Inference API failed, using simulation 400
[00:37:45.722] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7791147988735655
[00:37:45.722] Browser WARN    Inference API failed, using simulation 400
[00:37:45.722] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.38294108380765934
[00:37:46.082] Server  ERROR   Backend returned 400
[00:37:46.097] Server  ERROR   Backend returned 400
[00:37:46.121] Server  ERROR   Backend returned 400
[00:37:46.224] Browser WARN    Inference API failed, using simulation 400
[00:37:46.224] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.44678331725146847
[00:37:46.224] Browser WARN    Inference API failed, using simulation 400
[00:37:46.224] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3454925886630037
[00:37:46.224] Browser WARN    Inference API failed, using simulation 400
[00:37:46.224] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9289024779250494
[00:37:46.577] Server  ERROR   Backend returned 400
[00:37:46.609] Server  ERROR   Backend returned 400
[00:37:46.612] Server  ERROR   Backend returned 400
[00:37:46.720] Browser WARN    Inference API failed, using simulation 400
[00:37:46.720] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7487820318128277
[00:37:46.720] Browser WARN    Inference API failed, using simulation 400
[00:37:46.720] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9139799920245926
[00:37:46.720] Browser WARN    Inference API failed, using simulation 400
[00:37:46.720] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7114155094859491
[00:37:47.078] Server  ERROR   Backend returned 400
[00:37:47.103] Server  ERROR   Backend returned 400
[00:37:47.115] Server  ERROR   Backend returned 400
[00:37:47.221] Browser WARN    Inference API failed, using simulation 400
[00:37:47.221] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3747952763923615
[00:37:47.221] Browser WARN    Inference API failed, using simulation 400
[00:37:47.221] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4556064173595547
[00:37:47.221] Browser WARN    Inference API failed, using simulation 400
[00:37:47.221] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1414373180085397
[00:37:47.580] Server  ERROR   Backend returned 400
[00:37:47.600] Server  ERROR   Backend returned 400
[00:37:47.615] Server  ERROR   Backend returned 400
[00:37:47.719] Browser WARN    Inference API failed, using simulation 400
[00:37:47.719] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3893172582319528
[00:37:47.719] Browser WARN    Inference API failed, using simulation 400
[00:37:47.719] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.010877368453568736
[00:37:47.719] Browser WARN    Inference API failed, using simulation 400
[00:37:47.719] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.24433652934487587
[00:37:48.084] Server  ERROR   Backend returned 400
[00:37:48.101] Server  ERROR   Backend returned 400
[00:37:48.110] Server  ERROR   Backend returned 400
[00:37:48.216] Browser WARN    Inference API failed, using simulation 400
[00:37:48.216] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.37091372191707256
[00:37:48.216] Browser WARN    Inference API failed, using simulation 400
[00:37:48.216] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7853380668927847
[00:37:48.216] Browser WARN    Inference API failed, using simulation 400
[00:37:48.216] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4313348421478138
[00:37:48.582] Server  ERROR   Backend returned 400
[00:37:48.602] Server  ERROR   Backend returned 400
[00:37:48.629] Server  ERROR   Backend returned 400
[00:37:48.733] Browser WARN    Inference API failed, using simulation 400
[00:37:48.733] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2929102401588834
[00:37:48.733] Browser WARN    Inference API failed, using simulation 400
[00:37:48.733] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1910602072902896
[00:37:48.733] Browser WARN    Inference API failed, using simulation 400
[00:37:48.733] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2386877827708896
[00:37:49.083] Server  ERROR   Backend returned 400
[00:37:49.101] Server  ERROR   Backend returned 400
[00:37:49.120] Server  ERROR   Backend returned 400
[00:37:49.236] Browser WARN    Inference API failed, using simulation 400
[00:37:49.236] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9938087793323376
[00:37:49.236] Browser WARN    Inference API failed, using simulation 400
[00:37:49.236] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3050698638939504
[00:37:49.236] Browser WARN    Inference API failed, using simulation 400
[00:37:49.236] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3182494241124564
[00:37:49.580] Server  ERROR   Backend returned 400
[00:37:49.608] Server  ERROR   Backend returned 400
[00:37:49.625] Server  ERROR   Backend returned 400
[00:37:49.730] Browser WARN    Inference API failed, using simulation 400
[00:37:49.730] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.842264436414937
[00:37:49.730] Browser WARN    Inference API failed, using simulation 400
[00:37:49.730] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9834679892529957
[00:37:49.730] Browser WARN    Inference API failed, using simulation 400
[00:37:49.730] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.058415645974897235
[00:37:50.097] Server  ERROR   Backend returned 400
[00:37:50.120] Server  ERROR   Backend returned 400
[00:37:50.122] Server  ERROR   Backend returned 400
[00:37:50.234] Browser WARN    Inference API failed, using simulation 400
[00:37:50.234] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.0341159499600297
[00:37:50.234] Browser WARN    Inference API failed, using simulation 400
[00:37:50.234] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.45090843956911825
[00:37:50.234] Browser WARN    Inference API failed, using simulation 400
[00:37:50.234] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.07594979374221256
[00:37:50.576] Server  ERROR   Backend returned 400
[00:37:50.609] Server  ERROR   Backend returned 400
[00:37:50.613] Server  ERROR   Backend returned 400
[00:37:50.730] Browser WARN    Inference API failed, using simulation 400
[00:37:50.731] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3523239376726157
[00:37:50.731] Browser WARN    Inference API failed, using simulation 400
[00:37:50.731] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.25890480239552854
[00:37:50.731] Browser WARN    Inference API failed, using simulation 400
[00:37:50.731] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9984489704970858
[00:37:51.085] Server  ERROR   Backend returned 400
[00:37:51.101] Server  ERROR   Backend returned 400
[00:37:51.117] Server  ERROR   Backend returned 400
[00:37:51.229] Browser WARN    Inference API failed, using simulation 400
[00:37:51.229] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2024292781934408
[00:37:51.229] Browser WARN    Inference API failed, using simulation 400
[00:37:51.229] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8379714552005156
[00:37:51.229] Browser WARN    Inference API failed, using simulation 400
[00:37:51.229] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9849196630458781
[00:37:51.582] Server  ERROR   Backend returned 400
[00:37:51.601] Server  ERROR   Backend returned 400
[00:37:51.615] Server  ERROR   Backend returned 400
[00:37:51.719] Browser WARN    Inference API failed, using simulation 400
[00:37:51.719] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.07755195215171445
[00:37:51.719] Browser WARN    Inference API failed, using simulation 400
[00:37:51.719] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2885153161803678
[00:37:51.719] Browser WARN    Inference API failed, using simulation 400
[00:37:51.719] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1499417593828158
[00:37:52.085] Server  ERROR   Backend returned 400
[00:37:52.103] Server  ERROR   Backend returned 400
[00:37:52.121] Server  ERROR   Backend returned 400
[00:37:52.226] Browser WARN    Inference API failed, using simulation 400
[00:37:52.226] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4410080797781362
[00:37:52.226] Browser WARN    Inference API failed, using simulation 400
[00:37:52.226] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.44395106598973383
[00:37:52.226] Browser WARN    Inference API failed, using simulation 400
[00:37:52.226] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2651002522372917
[00:37:52.589] Server  ERROR   Backend returned 400
[00:37:52.606] Server  ERROR   Backend returned 400
[00:37:52.620] Server  ERROR   Backend returned 400
[00:37:52.737] Browser WARN    Inference API failed, using simulation 400
[00:37:52.737] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9980757623425678
[00:37:52.737] Browser WARN    Inference API failed, using simulation 400
[00:37:52.737] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7153101293520319
[00:37:52.737] Browser WARN    Inference API failed, using simulation 400
[00:37:52.737] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.07465340704185369
[00:37:53.081] Server  ERROR   Backend returned 400
[00:37:53.120] Server  ERROR   Backend returned 400
[00:37:53.135] Server  ERROR   Backend returned 400
[00:37:53.241] Browser WARN    Inference API failed, using simulation 400
[00:37:53.241] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3905198057460849
[00:37:53.241] Browser WARN    Inference API failed, using simulation 400
[00:37:53.241] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9498148854914323
[00:37:53.241] Browser WARN    Inference API failed, using simulation 400
[00:37:53.241] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8333245572506887
[00:37:53.583] Server  ERROR   Backend returned 400
[00:37:53.610] Server  ERROR   Backend returned 400
[00:37:53.622] Server  ERROR   Backend returned 400
[00:37:53.727] Browser WARN    Inference API failed, using simulation 400
[00:37:53.727] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2522124718304205
[00:37:53.727] Browser WARN    Inference API failed, using simulation 400
[00:37:53.727] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.462773969619483
[00:37:53.727] Browser WARN    Inference API failed, using simulation 400
[00:37:53.727] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.13492813639244478
[00:37:54.078] Server  ERROR   Backend returned 400
[00:37:54.114] Server  ERROR   Backend returned 400
[00:37:54.116] Server  ERROR   Backend returned 400
[00:37:54.224] Browser WARN    Inference API failed, using simulation 400
[00:37:54.224] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9131536998606274
[00:37:54.224] Browser WARN    Inference API failed, using simulation 400
[00:37:54.224] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.05012043482604167
[00:37:54.224] Browser WARN    Inference API failed, using simulation 400
[00:37:54.224] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.261153435233697
[00:37:54.580] Server  ERROR   Backend returned 400
[00:37:54.604] Server  ERROR   Backend returned 400
[00:37:54.617] Server  ERROR   Backend returned 400
[00:37:54.721] Browser WARN    Inference API failed, using simulation 400
[00:37:54.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2272071800704369
[00:37:54.721] Browser WARN    Inference API failed, using simulation 400
[00:37:54.721] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8054145692331708
[00:37:54.721] Browser WARN    Inference API failed, using simulation 400
[00:37:54.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.427269811202637
[00:37:55.077] Server  ERROR   Backend returned 400
[00:37:55.094] Server  ERROR   Backend returned 400
[00:37:55.107] Server  ERROR   Backend returned 400
[00:37:55.210] Browser WARN    Inference API failed, using simulation 400
[00:37:55.210] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.05918226967526152
[00:37:55.210] Browser WARN    Inference API failed, using simulation 400
[00:37:55.210] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.16982197865823134
[00:37:55.210] Browser WARN    Inference API failed, using simulation 400
[00:37:55.210] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3123026359963534
[00:37:55.581] Server  ERROR   Backend returned 400
[00:37:55.609] Server  ERROR   Backend returned 400
[00:37:55.615] Server  ERROR   Backend returned 400
[00:37:55.723] Browser WARN    Inference API failed, using simulation 400
[00:37:55.723] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9375443395870082
[00:37:55.723] Browser WARN    Inference API failed, using simulation 400
[00:37:55.723] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7336846757150673
[00:37:55.723] Browser WARN    Inference API failed, using simulation 400
[00:37:55.723] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.31579691704935975
[00:37:56.079] Server  ERROR   Backend returned 400
[00:37:56.100] Server  ERROR   Backend returned 400
[00:37:56.118] Server  ERROR   Backend returned 400
[00:37:56.227] Browser WARN    Inference API failed, using simulation 400
[00:37:56.227] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.11565554609181572
[00:37:56.227] Browser WARN    Inference API failed, using simulation 400
[00:37:56.227] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.22665770209819242
[00:37:56.227] Browser WARN    Inference API failed, using simulation 400
[00:37:56.227] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1190308888712539
[00:37:56.584] Server  ERROR   Backend returned 400
[00:37:56.611] Server  ERROR   Backend returned 400
[00:37:56.621] Server  ERROR   Backend returned 400
[00:37:56.726] Browser WARN    Inference API failed, using simulation 400
[00:37:56.726] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.32097360085247106
[00:37:56.726] Browser WARN    Inference API failed, using simulation 400
[00:37:56.726] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1982381659657736
[00:37:56.726] Browser WARN    Inference API failed, using simulation 400
[00:37:56.726] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9094740490463639
[00:37:57.077] Server  ERROR   Backend returned 400
[00:37:57.103] Server  ERROR   Backend returned 400
[00:37:57.107] Server  ERROR   Backend returned 400
[00:37:57.215] Browser WARN    Inference API failed, using simulation 400
[00:37:57.215] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.34158164626829635
[00:37:57.215] Browser WARN    Inference API failed, using simulation 400
[00:37:57.215] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.17621397793316468
[00:37:57.215] Browser WARN    Inference API failed, using simulation 400
[00:37:57.215] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.15532342815223898
[00:37:57.583] Server  ERROR   Backend returned 400
[00:37:57.601] Server  ERROR   Backend returned 400
[00:37:57.616] Server  ERROR   Backend returned 400
[00:37:57.723] Browser WARN    Inference API failed, using simulation 400
[00:37:57.723] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9688814607000523
[00:37:57.723] Browser WARN    Inference API failed, using simulation 400
[00:37:57.723] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8986628076551044
[00:37:57.723] Browser WARN    Inference API failed, using simulation 400
[00:37:57.723] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.28418146127915783
[00:37:58.079] Server  ERROR   Backend returned 400
[00:37:58.107] Server  ERROR   Backend returned 400
[00:37:58.110] Server  ERROR   Backend returned 400
[00:37:58.220] Browser WARN    Inference API failed, using simulation 400
[00:37:58.220] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.059484978015838574
[00:37:58.220] Browser WARN    Inference API failed, using simulation 400
[00:37:58.220] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.40062603752569387
[00:37:58.220] Browser WARN    Inference API failed, using simulation 400
[00:37:58.220] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2672614473612235
[00:37:58.587] Server  ERROR   Backend returned 400
[00:37:58.602] Server  ERROR   Backend returned 400
[00:37:58.619] Server  ERROR   Backend returned 400
[00:37:58.737] Browser WARN    Inference API failed, using simulation 400
[00:37:58.737] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2395246132762039
[00:37:58.737] Browser WARN    Inference API failed, using simulation 400
[00:37:58.737] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.15885506453291282
[00:37:58.737] Browser WARN    Inference API failed, using simulation 400
[00:37:58.737] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3911909240530215
[00:37:59.095] Server  ERROR   Backend returned 400
[00:37:59.110] Server  ERROR   Backend returned 400
[00:37:59.122] Server  ERROR   Backend returned 400
[00:37:59.227] Browser WARN    Inference API failed, using simulation 400
[00:37:59.227] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.10783641330898708
[00:37:59.227] Browser WARN    Inference API failed, using simulation 400
[00:37:59.227] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7666161839390523
[00:37:59.227] Browser WARN    Inference API failed, using simulation 400
[00:37:59.227] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4696513992936533
[00:37:59.581] Server  ERROR   Backend returned 400
[00:37:59.600] Server  ERROR   Backend returned 400
[00:37:59.616] Server  ERROR   Backend returned 400
[00:37:59.720] Browser WARN    Inference API failed, using simulation 400
[00:37:59.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.14951647659667888
[00:37:59.720] Browser WARN    Inference API failed, using simulation 400
[00:37:59.720] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.23310357457225434
[00:37:59.720] Browser WARN    Inference API failed, using simulation 400
[00:37:59.720] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.46476409557028825
[00:38:00.083] Server  ERROR   Backend returned 400
[00:38:00.111] Server  ERROR   Backend returned 400
[00:38:00.113] Server  ERROR   Backend returned 400
[00:38:00.224] Browser WARN    Inference API failed, using simulation 400
[00:38:00.224] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3897587178296358
[00:38:00.224] Browser WARN    Inference API failed, using simulation 400
[00:38:00.224] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9161921165229359
[00:38:00.224] Browser WARN    Inference API failed, using simulation 400
[00:38:00.224] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3629719890396877
[00:38:00.581] Server  ERROR   Backend returned 400
[00:38:00.599] Server  ERROR   Backend returned 400
[00:38:00.614] Server  ERROR   Backend returned 400
[00:38:00.717] Browser WARN    Inference API failed, using simulation 400
[00:38:00.717] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.21690232995861775
[00:38:00.717] Browser WARN    Inference API failed, using simulation 400
[00:38:00.717] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9497914317595687
[00:38:00.717] Browser WARN    Inference API failed, using simulation 400
[00:38:00.717] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.20569089343777447
[00:38:01.096] Server  ERROR   Backend returned 400
[00:38:01.112] Server  ERROR   Backend returned 400
[00:38:01.133] Server  ERROR   Backend returned 400
[00:38:01.237] Browser WARN    Inference API failed, using simulation 400
[00:38:01.237] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7487946913897174
[00:38:01.237] Browser WARN    Inference API failed, using simulation 400
[00:38:01.237] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4170532146141702
[00:38:01.237] Browser WARN    Inference API failed, using simulation 400
[00:38:01.237] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.11232329502955518
[00:38:01.583] Server  ERROR   Backend returned 400
[00:38:01.605] Server  ERROR   Backend returned 400
[00:38:01.616] Server  ERROR   Backend returned 400
[00:38:01.722] Browser WARN    Inference API failed, using simulation 400
[00:38:01.722] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.19809095781961006
[00:38:01.722] Browser WARN    Inference API failed, using simulation 400
[00:38:01.722] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3934667245426445
[00:38:01.722] Browser WARN    Inference API failed, using simulation 400
[00:38:01.722] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.055170542540124656
[00:38:02.109] Server  ERROR   Backend returned 400
[00:38:02.163] Server  ERROR   Backend returned 400
[00:38:02.181] Server  ERROR   Backend returned 400
[00:38:02.286] Browser WARN    Inference API failed, using simulation 400
[00:38:02.286] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4659631431255701
[00:38:02.286] Browser WARN    Inference API failed, using simulation 400
[00:38:02.286] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.06314380057386065
[00:38:02.286] Browser WARN    Inference API failed, using simulation 400
[00:38:02.286] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3306913600365897
[00:38:02.580] Server  ERROR   Backend returned 400
[00:38:02.600] Server  ERROR   Backend returned 400
[00:38:02.612] Server  ERROR   Backend returned 400
[00:38:02.716] Browser WARN    Inference API failed, using simulation 400
[00:38:02.716] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.03918950465227983
[00:38:02.716] Browser WARN    Inference API failed, using simulation 400
[00:38:02.716] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.38244593764225737
[00:38:02.716] Browser WARN    Inference API failed, using simulation 400
[00:38:02.716] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2605518461151555
[00:38:03.086] Server  ERROR   Backend returned 400
[00:38:03.115] Server  ERROR   Backend returned 400
[00:38:03.117] Server  ERROR   Backend returned 400
[00:38:03.227] Browser WARN    Inference API failed, using simulation 400
[00:38:03.227] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2771825253272627
[00:38:03.227] Browser WARN    Inference API failed, using simulation 400
[00:38:03.227] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.47210229622322886
[00:38:03.227] Browser WARN    Inference API failed, using simulation 400
[00:38:03.227] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.38110682336100027
[00:38:03.586] Server  ERROR   Backend returned 400
[00:38:03.608] Server  ERROR   Backend returned 400
[00:38:03.621] Server  ERROR   Backend returned 400
[00:38:03.724] Browser WARN    Inference API failed, using simulation 400
[00:38:03.724] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3626355186795421
[00:38:03.724] Browser WARN    Inference API failed, using simulation 400
[00:38:03.724] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.41907903799063917
[00:38:03.724] Browser WARN    Inference API failed, using simulation 400
[00:38:03.724] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2278107383427873
[00:38:04.079] Server  ERROR   Backend returned 400
[00:38:04.105] Server  ERROR   Backend returned 400
[00:38:04.116] Server  ERROR   Backend returned 400
[00:38:04.222] Browser WARN    Inference API failed, using simulation 400
[00:38:04.222] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7667034432682706
[00:38:04.222] Browser WARN    Inference API failed, using simulation 400
[00:38:04.222] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3644302197120425
[00:38:04.222] Browser WARN    Inference API failed, using simulation 400
[00:38:04.222] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7459495923652631
[00:38:04.581] Server  ERROR   Backend returned 400
[00:38:04.609] Server  ERROR   Backend returned 400
[00:38:04.613] Server  ERROR   Backend returned 400
[00:38:04.722] Browser WARN    Inference API failed, using simulation 400
[00:38:04.722] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9969921419270795
[00:38:04.722] Browser WARN    Inference API failed, using simulation 400
[00:38:04.722] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7785676962813401
[00:38:04.722] Browser WARN    Inference API failed, using simulation 400
[00:38:04.722] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.32374239295535295
[00:38:05.121] Server  ERROR   Backend returned 400
[00:38:05.228] Server  ERROR   Backend returned 400
[00:38:05.230] Browser WARN    Inference API failed, using simulation 400
[00:38:05.230] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4559855028932979
[00:38:05.231] Server  ERROR   Backend returned 400
[00:38:05.343] Browser WARN    Inference API failed, using simulation 400
[00:38:05.343] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8347325932707648
[00:38:05.343] Browser WARN    Inference API failed, using simulation 400
[00:38:05.343] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1988096672920534
[00:38:05.579] Server  ERROR   Backend returned 400
[00:38:05.607] Server  ERROR   Backend returned 400
[00:38:05.620] Server  ERROR   Backend returned 400
[00:38:05.725] Browser WARN    Inference API failed, using simulation 400
[00:38:05.725] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.22234842463895815
[00:38:05.725] Browser WARN    Inference API failed, using simulation 400
[00:38:05.725] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8249092941366138
[00:38:05.725] Browser WARN    Inference API failed, using simulation 400
[00:38:05.725] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.006615702519136013
[00:38:06.079] Server  ERROR   Backend returned 400
[00:38:06.114] Server  ERROR   Backend returned 400
[00:38:06.117] Server  ERROR   Backend returned 400
[00:38:06.225] Browser WARN    Inference API failed, using simulation 400
[00:38:06.225] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.765823246767742
[00:38:06.225] Browser WARN    Inference API failed, using simulation 400
[00:38:06.225] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9500344057307182
[00:38:06.225] Browser WARN    Inference API failed, using simulation 400
[00:38:06.225] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.436059569788317
[00:38:06.591] Server  ERROR   Backend returned 400
[00:38:06.611] Server  ERROR   Backend returned 400
[00:38:06.624] Server  ERROR   Backend returned 400
[00:38:06.729] Browser WARN    Inference API failed, using simulation 400
[00:38:06.729] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.43089526892957364
[00:38:06.729] Browser WARN    Inference API failed, using simulation 400
[00:38:06.729] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.16337032205607693
[00:38:06.729] Browser WARN    Inference API failed, using simulation 400
[00:38:06.729] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2855174321047522
[00:38:07.079] Server  ERROR   Backend returned 400
[00:38:07.108] Server  ERROR   Backend returned 400
[00:38:07.121] Server  ERROR   Backend returned 400
[00:38:07.226] Browser WARN    Inference API failed, using simulation 400
[00:38:07.226] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.09654079315591024
[00:38:07.226] Browser WARN    Inference API failed, using simulation 400
[00:38:07.226] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7305453172622722
[00:38:07.226] Browser WARN    Inference API failed, using simulation 400
[00:38:07.226] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.40514256839614904
[00:38:07.580] Server  ERROR   Backend returned 400
[00:38:07.602] Server  ERROR   Backend returned 400
[00:38:07.633] Server  ERROR   Backend returned 400
[00:38:07.737] Browser WARN    Inference API failed, using simulation 400
[00:38:07.737] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8366595307335707
[00:38:07.737] Browser WARN    Inference API failed, using simulation 400
[00:38:07.737] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.06257540432128528
[00:38:07.737] Browser WARN    Inference API failed, using simulation 400
[00:38:07.737] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.33919580600972615
[00:38:08.081] Server  ERROR   Backend returned 400
[00:38:08.098] Server  ERROR   Backend returned 400
[00:38:08.122] Server  ERROR   Backend returned 400
[00:38:08.238] Browser WARN    Inference API failed, using simulation 400
[00:38:08.238] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.06177496698835738
[00:38:08.238] Browser WARN    Inference API failed, using simulation 400
[00:38:08.238] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.19539076933337463
[00:38:08.238] Browser WARN    Inference API failed, using simulation 400
[00:38:08.238] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.21586775845079692
[00:38:08.581] Server  ERROR   Backend returned 400
[00:38:08.602] Server  ERROR   Backend returned 400
[00:38:08.619] Server  ERROR   Backend returned 400
[00:38:08.723] Browser WARN    Inference API failed, using simulation 400
[00:38:08.723] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7108796782904152
[00:38:08.723] Browser WARN    Inference API failed, using simulation 400
[00:38:08.723] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.26489846473138207
[00:38:08.723] Browser WARN    Inference API failed, using simulation 400
[00:38:08.723] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.33354368308694127
[00:38:09.080] Server  ERROR   Backend returned 400
[00:38:09.107] Server  ERROR   Backend returned 400
[00:38:09.109] Server  ERROR   Backend returned 400
[00:38:09.230] Browser WARN    Inference API failed, using simulation 400
[00:38:09.230] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.14457130374039157
[00:38:09.230] Browser WARN    Inference API failed, using simulation 400
[00:38:09.230] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.41803154401093773
[00:38:09.230] Browser WARN    Inference API failed, using simulation 400
[00:38:09.230] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.09605978628302758
[00:38:09.594] Server  ERROR   Backend returned 400
[00:38:09.618] Server  ERROR   Backend returned 400
[00:38:09.632] Server  ERROR   Backend returned 400
[00:38:09.737] Browser WARN    Inference API failed, using simulation 400
[00:38:09.737] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3908212701859968
[00:38:09.737] Browser WARN    Inference API failed, using simulation 400
[00:38:09.737] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7509810176986103
[00:38:09.737] Browser WARN    Inference API failed, using simulation 400
[00:38:09.737] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7620635966330029
[00:38:10.076] Server  ERROR   Backend returned 400
[00:38:10.111] Server  ERROR   Backend returned 400
[00:38:10.115] Server  ERROR   Backend returned 400
[00:38:10.228] Browser WARN    Inference API failed, using simulation 400
[00:38:10.228] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8377353392101616
[00:38:10.228] Browser WARN    Inference API failed, using simulation 400
[00:38:10.228] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.015527995580277731
[00:38:10.228] Browser WARN    Inference API failed, using simulation 400
[00:38:10.228] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3821423490939227
[00:38:10.585] Server  ERROR   Backend returned 400
[00:38:10.615] Server  ERROR   Backend returned 400
[00:38:10.618] Server  ERROR   Backend returned 400
[00:38:10.729] Browser WARN    Inference API failed, using simulation 400
[00:38:10.729] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.794571516495497
[00:38:10.729] Browser WARN    Inference API failed, using simulation 400
[00:38:10.729] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7637666764738662
[00:38:10.729] Browser WARN    Inference API failed, using simulation 400
[00:38:10.729] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.042537556648169716
[00:38:11.086] Server  ERROR   Backend returned 400
[00:38:11.106] Server  ERROR   Backend returned 400
[00:38:11.139] Server  ERROR   Backend returned 400
[00:38:11.243] Browser WARN    Inference API failed, using simulation 400
[00:38:11.243] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.485707136967992
[00:38:11.243] Browser WARN    Inference API failed, using simulation 400
[00:38:11.243] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8374553199213984
[00:38:11.243] Browser WARN    Inference API failed, using simulation 400
[00:38:11.243] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9620321459792114
[00:38:11.583] Server  ERROR   Backend returned 400
[00:38:11.602] Server  ERROR   Backend returned 400
[00:38:11.619] Server  ERROR   Backend returned 400
[00:38:11.722] Browser WARN    Inference API failed, using simulation 400
[00:38:11.722] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.41121540836382436
[00:38:11.722] Browser WARN    Inference API failed, using simulation 400
[00:38:11.722] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.24308752962182895
[00:38:11.722] Browser WARN    Inference API failed, using simulation 400
[00:38:11.722] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8293903734327474
[00:38:12.086] Server  ERROR   Backend returned 400
[00:38:12.102] Server  ERROR   Backend returned 400
[00:38:12.115] Server  ERROR   Backend returned 400
[00:38:12.219] Browser WARN    Inference API failed, using simulation 400
[00:38:12.219] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9787272194315446
[00:38:12.219] Browser WARN    Inference API failed, using simulation 400
[00:38:12.219] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3478677629657568
[00:38:12.219] Browser WARN    Inference API failed, using simulation 400
[00:38:12.219] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.11868221007381463
[00:38:12.582] Server  ERROR   Backend returned 400
[00:38:12.602] Server  ERROR   Backend returned 400
[00:38:12.616] Server  ERROR   Backend returned 400
[00:38:12.720] Browser WARN    Inference API failed, using simulation 400
[00:38:12.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.42977160478613585
[00:38:12.720] Browser WARN    Inference API failed, using simulation 400
[00:38:12.720] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1771203805953424
[00:38:12.720] Browser WARN    Inference API failed, using simulation 400
[00:38:12.720] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.49443738526059766
[00:38:13.086] Server  ERROR   Backend returned 400
[00:38:13.103] Server  ERROR   Backend returned 400
[00:38:13.120] Server  ERROR   Backend returned 400
[00:38:13.228] Browser WARN    Inference API failed, using simulation 400
[00:38:13.228] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.13088057633279077
[00:38:13.228] Browser WARN    Inference API failed, using simulation 400
[00:38:13.228] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.12099989239610776
[00:38:13.228] Browser WARN    Inference API failed, using simulation 400
[00:38:13.228] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.35766154299495617
[00:38:13.591] Server  ERROR   Backend returned 400
[00:38:13.617] Server  ERROR   Backend returned 400
[00:38:13.634] Server  ERROR   Backend returned 400
[00:38:13.742] Browser WARN    Inference API failed, using simulation 400
[00:38:13.742] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.33264587982234256
[00:38:13.742] Browser WARN    Inference API failed, using simulation 400
[00:38:13.742] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.10293962974276472
[00:38:13.742] Browser WARN    Inference API failed, using simulation 400
[00:38:13.742] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.01676264597528271
[00:38:14.081] Server  ERROR   Backend returned 400
[00:38:14.107] Server  ERROR   Backend returned 400
[00:38:14.119] Server  ERROR   Backend returned 400
[00:38:14.224] Browser WARN    Inference API failed, using simulation 400
[00:38:14.224] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2486264231487626
[00:38:14.224] Browser WARN    Inference API failed, using simulation 400
[00:38:14.224] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.22817452695264145
[00:38:14.224] Browser WARN    Inference API failed, using simulation 400
[00:38:14.224] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.015125566633765797
[00:38:14.579] Server  ERROR   Backend returned 400
[00:38:14.613] Server  ERROR   Backend returned 400
[00:38:14.616] Server  ERROR   Backend returned 400
[00:38:14.722] Browser WARN    Inference API failed, using simulation 400
[00:38:14.722] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.12743464525843973
[00:38:14.722] Browser WARN    Inference API failed, using simulation 400
[00:38:14.722] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.27370024188508285
[00:38:14.722] Browser WARN    Inference API failed, using simulation 400
[00:38:14.722] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.17543452482221356
[00:38:15.080] Server  ERROR   Backend returned 400
[00:38:15.115] Server  ERROR   Backend returned 400
[00:38:15.118] Server  ERROR   Backend returned 400
[00:38:15.233] Browser WARN    Inference API failed, using simulation 400
[00:38:15.233] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8557121647678079
[00:38:15.233] Browser WARN    Inference API failed, using simulation 400
[00:38:15.233] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.050409501266499734
[00:38:15.233] Browser WARN    Inference API failed, using simulation 400
[00:38:15.233] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7502036660350911
[00:38:15.581] Server  ERROR   Backend returned 400
[00:38:15.614] Server  ERROR   Backend returned 400
[00:38:15.616] Server  ERROR   Backend returned 400
[00:38:15.725] Browser WARN    Inference API failed, using simulation 400
[00:38:15.725] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7074729884533307
[00:38:15.725] Browser WARN    Inference API failed, using simulation 400
[00:38:15.725] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.11352420105147148
[00:38:15.725] Browser WARN    Inference API failed, using simulation 400
[00:38:15.725] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.08431997260230323
[00:38:16.080] Server  ERROR   Backend returned 400
[00:38:16.130] Server  ERROR   Backend returned 400
[00:38:16.133] Server  ERROR   Backend returned 400
[00:38:16.243] Browser WARN    Inference API failed, using simulation 400
[00:38:16.243] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.33674356285896123
[00:38:16.243] Browser WARN    Inference API failed, using simulation 400
[00:38:16.243] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.17414597678815535
[00:38:16.243] Browser WARN    Inference API failed, using simulation 400
[00:38:16.243] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8688265518360457
[00:38:16.581] Server  ERROR   Backend returned 400
[00:38:16.602] Server  ERROR   Backend returned 400
[00:38:16.617] Server  ERROR   Backend returned 400
[00:38:16.720] Browser WARN    Inference API failed, using simulation 400
[00:38:16.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.48982935617718415
[00:38:16.720] Browser WARN    Inference API failed, using simulation 400
[00:38:16.720] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2602484397814675
[00:38:16.720] Browser WARN    Inference API failed, using simulation 400
[00:38:16.720] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2725650029782753
[00:38:17.080] Server  ERROR   Backend returned 400
[00:38:17.106] Server  ERROR   Backend returned 400
[00:38:17.116] Server  ERROR   Backend returned 400
[00:38:17.229] Browser WARN    Inference API failed, using simulation 400
[00:38:17.229] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3273728945124397
[00:38:17.229] Browser WARN    Inference API failed, using simulation 400
[00:38:17.229] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4601917391742494
[00:38:17.229] Browser WARN    Inference API failed, using simulation 400
[00:38:17.229] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7043839254907605
[00:38:17.585] Server  ERROR   Backend returned 400
[00:38:17.604] Server  ERROR   Backend returned 400
[00:38:17.615] Server  ERROR   Backend returned 400
[00:38:17.720] Browser WARN    Inference API failed, using simulation 400
[00:38:17.720] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8192836841214014
[00:38:17.720] Browser WARN    Inference API failed, using simulation 400
[00:38:17.720] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.358216503216226
[00:38:17.720] Browser WARN    Inference API failed, using simulation 400
[00:38:17.720] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4575308579151637
[00:38:18.085] Server  ERROR   Backend returned 400
[00:38:18.102] Server  ERROR   Backend returned 400
[00:38:18.125] Server  ERROR   Backend returned 400
[00:38:18.229] Browser WARN    Inference API failed, using simulation 400
[00:38:18.229] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7407534842970529
[00:38:18.229] Browser WARN    Inference API failed, using simulation 400
[00:38:18.229] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.08863241743680123
[00:38:18.229] Browser WARN    Inference API failed, using simulation 400
[00:38:18.229] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.49539748877966605
[00:38:18.579] Server  ERROR   Backend returned 400
[00:38:18.604] Server  ERROR   Backend returned 400
[00:38:18.621] Server  ERROR   Backend returned 400
[00:38:18.725] Browser WARN    Inference API failed, using simulation 400
[00:38:18.725] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4229802857920577
[00:38:18.725] Browser WARN    Inference API failed, using simulation 400
[00:38:18.725] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.19811447275670585
[00:38:18.725] Browser WARN    Inference API failed, using simulation 400
[00:38:18.725] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4085069202204829
[00:38:19.077] Server  ERROR   Backend returned 400
[00:38:19.108] Server  ERROR   Backend returned 400
[00:38:19.112] Server  ERROR   Backend returned 400
[00:38:19.225] Browser WARN    Inference API failed, using simulation 400
[00:38:19.225] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.24709101778962983
[00:38:19.225] Browser WARN    Inference API failed, using simulation 400
[00:38:19.225] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.36948581746507586
[00:38:19.225] Browser WARN    Inference API failed, using simulation 400
[00:38:19.225] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.19879906141635817
[00:38:19.578] Server  ERROR   Backend returned 400
[00:38:19.613] Server  ERROR   Backend returned 400
[00:38:19.615] Server  ERROR   Backend returned 400
[00:38:19.723] Browser WARN    Inference API failed, using simulation 400
[00:38:19.723] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.49959909114745893
[00:38:19.723] Browser WARN    Inference API failed, using simulation 400
[00:38:19.723] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7087088513856292
[00:38:19.723] Browser WARN    Inference API failed, using simulation 400
[00:38:19.723] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.039395742142925794
[00:38:20.078] Server  ERROR   Backend returned 400
[00:38:20.101] Server  ERROR   Backend returned 400
[00:38:20.104] Server  ERROR   Backend returned 400
[00:38:20.212] Browser WARN    Inference API failed, using simulation 400
[00:38:20.212] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.016619780958771146
[00:38:20.212] Browser WARN    Inference API failed, using simulation 400
[00:38:20.212] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9294223080101072
[00:38:20.212] Browser WARN    Inference API failed, using simulation 400
[00:38:20.212] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.16354780063560842
[00:38:20.588] Server  ERROR   Backend returned 400
[00:38:20.608] Server  ERROR   Backend returned 400
[00:38:20.622] Server  ERROR   Backend returned 400
[00:38:20.727] Browser WARN    Inference API failed, using simulation 400
[00:38:20.727] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8648945464940474
[00:38:20.727] Browser WARN    Inference API failed, using simulation 400
[00:38:20.727] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.07027456589629477
[00:38:20.727] Browser WARN    Inference API failed, using simulation 400
[00:38:20.727] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.999944769760857
[00:38:21.079] Server  ERROR   Backend returned 400
[00:38:21.107] Server  ERROR   Backend returned 400
[00:38:21.110] Server  ERROR   Backend returned 400
[00:38:21.222] Browser WARN    Inference API failed, using simulation 400
[00:38:21.222] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9147727271263904
[00:38:21.222] Browser WARN    Inference API failed, using simulation 400
[00:38:21.222] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8999748105766874
[00:38:21.222] Browser WARN    Inference API failed, using simulation 400
[00:38:21.222] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.12034695170572601
[00:38:21.582] Server  ERROR   Backend returned 400
[00:38:21.603] Server  ERROR   Backend returned 400
[00:38:21.630] Server  ERROR   Backend returned 400
[00:38:21.734] Browser WARN    Inference API failed, using simulation 400
[00:38:21.734] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.21841275457472248
[00:38:21.734] Browser WARN    Inference API failed, using simulation 400
[00:38:21.734] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.44735680783759635
[00:38:21.734] Browser WARN    Inference API failed, using simulation 400
[00:38:21.734] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3092539969535919
[00:38:22.081] Server  ERROR   Backend returned 400
[00:38:22.106] Server  ERROR   Backend returned 400
[00:38:22.108] Server  ERROR   Backend returned 400
[00:38:22.218] Browser WARN    Inference API failed, using simulation 400
[00:38:22.218] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7096475358218272
[00:38:22.218] Browser WARN    Inference API failed, using simulation 400
[00:38:22.218] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.004464816249053982
[00:38:22.218] Browser WARN    Inference API failed, using simulation 400
[00:38:22.218] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7094347728110513
[00:38:22.582] Server  ERROR   Backend returned 400
[00:38:22.602] Server  ERROR   Backend returned 400
[00:38:22.618] Server  ERROR   Backend returned 400
[00:38:22.722] Browser WARN    Inference API failed, using simulation 400
[00:38:22.722] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1223556777298258
[00:38:22.722] Browser WARN    Inference API failed, using simulation 400
[00:38:22.722] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7971737509575
[00:38:22.722] Browser WARN    Inference API failed, using simulation 400
[00:38:22.722] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3495440777660508
[00:38:23.078] Server  ERROR   Backend returned 400
[00:38:23.125] Server  ERROR   Backend returned 400
[00:38:23.130] Server  ERROR   Backend returned 400
[00:38:23.246] Browser WARN    Inference API failed, using simulation 400
[00:38:23.246] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.21719161825167332
[00:38:23.246] Browser WARN    Inference API failed, using simulation 400
[00:38:23.246] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.39619668754199344
[00:38:23.246] Browser WARN    Inference API failed, using simulation 400
[00:38:23.246] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.24040953743738164
[00:38:23.579] Server  ERROR   Backend returned 400
[00:38:23.606] Server  ERROR   Backend returned 400
[00:38:23.618] Server  ERROR   Backend returned 400
[00:38:23.722] Browser WARN    Inference API failed, using simulation 400
[00:38:23.722] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9858405287465279
[00:38:23.723] Browser WARN    Inference API failed, using simulation 400
[00:38:23.723] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.36008464438520493
[00:38:23.723] Browser WARN    Inference API failed, using simulation 400
[00:38:23.723] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.37853379537164883
[00:38:24.078] Server  ERROR   Backend returned 400
[00:38:24.105] Server  ERROR   Backend returned 400
[00:38:24.108] Server  ERROR   Backend returned 400
[00:38:24.220] Browser WARN    Inference API failed, using simulation 400
[00:38:24.220] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3180125738275336
[00:38:24.220] Browser WARN    Inference API failed, using simulation 400
[00:38:24.220] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7301019741192849
[00:38:24.220] Browser WARN    Inference API failed, using simulation 400
[00:38:24.220] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7822171671515132
[00:38:24.578] Server  ERROR   Backend returned 400
[00:38:24.609] Server  ERROR   Backend returned 400
[00:38:24.614] Server  ERROR   Backend returned 400
[00:38:24.722] Browser WARN    Inference API failed, using simulation 400
[00:38:24.722] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4980002000253313
[00:38:24.722] Browser WARN    Inference API failed, using simulation 400
[00:38:24.722] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.26539110294267765
[00:38:24.722] Browser WARN    Inference API failed, using simulation 400
[00:38:24.722] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9911214694676456
[00:38:25.080] Server  ERROR   Backend returned 400
[00:38:25.100] Server  ERROR   Backend returned 400
[00:38:25.115] Server  ERROR   Backend returned 400
[00:38:25.222] Browser WARN    Inference API failed, using simulation 400
[00:38:25.222] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9286252855458133
[00:38:25.222] Browser WARN    Inference API failed, using simulation 400
[00:38:25.222] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.48247374086208605
[00:38:25.222] Browser WARN    Inference API failed, using simulation 400
[00:38:25.222] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.14172170371221915
[00:38:25.579] Server  ERROR   Backend returned 400
[00:38:25.611] Server  ERROR   Backend returned 400
[00:38:25.614] Server  ERROR   Backend returned 400
[00:38:25.723] Browser WARN    Inference API failed, using simulation 400
[00:38:25.723] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.08589600193669855
[00:38:25.723] Browser WARN    Inference API failed, using simulation 400
[00:38:25.723] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.10206095972362389
[00:38:25.723] Browser WARN    Inference API failed, using simulation 400
[00:38:25.723] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.09965181881866941
[00:38:26.082] Server  ERROR   Backend returned 400
[00:38:26.115] Server  ERROR   Backend returned 400
[00:38:26.117] Server  ERROR   Backend returned 400
[00:38:26.226] Browser WARN    Inference API failed, using simulation 400
[00:38:26.227] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.900822096527058
[00:38:26.227] Browser WARN    Inference API failed, using simulation 400
[00:38:26.227] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.36755973946940085
[00:38:26.227] Browser WARN    Inference API failed, using simulation 400
[00:38:26.227] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.11391892604872106
[00:38:26.579] Server  ERROR   Backend returned 400
[00:38:26.604] Server  ERROR   Backend returned 400
[00:38:26.625] Server  ERROR   Backend returned 400
[00:38:26.730] Browser WARN    Inference API failed, using simulation 400
[00:38:26.730] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.46884388150325024
[00:38:26.730] Browser WARN    Inference API failed, using simulation 400
[00:38:26.730] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.14725365688773973
[00:38:26.730] Browser WARN    Inference API failed, using simulation 400
[00:38:26.730] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3274295480349282
[00:38:27.087] Server  ERROR   Backend returned 400
[00:38:27.117] Server  ERROR   Backend returned 400
[00:38:27.131] Server  ERROR   Backend returned 400
[00:38:27.239] Browser WARN    Inference API failed, using simulation 400
[00:38:27.239] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8084799217109987
[00:38:27.239] Browser WARN    Inference API failed, using simulation 400
[00:38:27.239] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8108118282656518
[00:38:27.239] Browser WARN    Inference API failed, using simulation 400
[00:38:27.239] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7034880802325771
[00:38:27.579] Server  ERROR   Backend returned 400
[00:38:27.593] Server  ERROR   Backend returned 400
[00:38:27.603] Server  ERROR   Backend returned 400
[00:38:27.709] Browser WARN    Inference API failed, using simulation 400
[00:38:27.709] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7238866755473736
[00:38:27.709] Browser WARN    Inference API failed, using simulation 400
[00:38:27.709] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7554110791443559
[00:38:27.709] Browser WARN    Inference API failed, using simulation 400
[00:38:27.709] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.05723050568849336
[00:38:28.079] Server  ERROR   Backend returned 400
[00:38:28.098] Server  ERROR   Backend returned 400
[00:38:28.112] Server  ERROR   Backend returned 400
[00:38:28.216] Browser WARN    Inference API failed, using simulation 400
[00:38:28.216] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1819390163385869
[00:38:28.216] Browser WARN    Inference API failed, using simulation 400
[00:38:28.216] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.14526670751163268
[00:38:28.216] Browser WARN    Inference API failed, using simulation 400
[00:38:28.217] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1136483666988175
[00:38:28.579] Server  ERROR   Backend returned 400
[00:38:28.594] Server  ERROR   Backend returned 400
[00:38:28.617] Server  ERROR   Backend returned 400
[00:38:28.721] Browser WARN    Inference API failed, using simulation 400
[00:38:28.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.27322007281041655
[00:38:28.721] Browser WARN    Inference API failed, using simulation 400
[00:38:28.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.30709017761809615
[00:38:28.721] Browser WARN    Inference API failed, using simulation 400
[00:38:28.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.38882726874966717
[00:38:29.086] Server  ERROR   Backend returned 400
[00:38:29.104] Server  ERROR   Backend returned 400
[00:38:29.121] Server  ERROR   Backend returned 400
[00:38:29.227] Browser WARN    Inference API failed, using simulation 400
[00:38:29.227] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1399772631408372
[00:38:29.227] Browser WARN    Inference API failed, using simulation 400
[00:38:29.227] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.39743455817472395
[00:38:29.227] Browser WARN    Inference API failed, using simulation 400
[00:38:29.227] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.40517185760286556
[00:38:29.580] Server  ERROR   Backend returned 400
[00:38:29.596] Server  ERROR   Backend returned 400
[00:38:29.610] Server  ERROR   Backend returned 400
[00:38:29.716] Browser WARN    Inference API failed, using simulation 400
[00:38:29.716] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.17380239851492135
[00:38:29.716] Browser WARN    Inference API failed, using simulation 400
[00:38:29.716] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.44902402942780817
[00:38:29.716] Browser WARN    Inference API failed, using simulation 400
[00:38:29.716] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3608837868306024
[00:38:30.078] Server  ERROR   Backend returned 400
[00:38:30.102] Server  ERROR   Backend returned 400
[00:38:30.115] Server  ERROR   Backend returned 400
[00:38:30.219] Browser WARN    Inference API failed, using simulation 400
[00:38:30.219] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8276201282883449
[00:38:30.219] Browser WARN    Inference API failed, using simulation 400
[00:38:30.219] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7730881946763422
[00:38:30.219] Browser WARN    Inference API failed, using simulation 400
[00:38:30.219] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4056922478343019
[00:38:30.580] Server  ERROR   Backend returned 400
[00:38:30.595] Server  ERROR   Backend returned 400
[00:38:30.608] Server  ERROR   Backend returned 400
[00:38:30.712] Browser WARN    Inference API failed, using simulation 400
[00:38:30.712] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4775834789989611
[00:38:30.712] Browser WARN    Inference API failed, using simulation 400
[00:38:30.712] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3304027342901888
[00:38:30.712] Browser WARN    Inference API failed, using simulation 400
[00:38:30.712] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.22799095779621098
[00:38:31.087] Server  ERROR   Backend returned 400
[00:38:31.105] Server  ERROR   Backend returned 400
[00:38:31.118] Server  ERROR   Backend returned 400
[00:38:31.222] Browser WARN    Inference API failed, using simulation 400
[00:38:31.222] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.45049703943824193
[00:38:31.222] Browser WARN    Inference API failed, using simulation 400
[00:38:31.222] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.07782652926392974
[00:38:31.222] Browser WARN    Inference API failed, using simulation 400
[00:38:31.222] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.14206545066936688
[00:38:31.582] Server  ERROR   Backend returned 400
[00:38:31.617] Server  ERROR   Backend returned 400
[00:38:31.629] Server  ERROR   Backend returned 400
[00:38:31.732] Browser WARN    Inference API failed, using simulation 400
[00:38:31.732] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.08385170277395626
[00:38:31.732] Browser WARN    Inference API failed, using simulation 400
[00:38:31.732] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3026998674297638
[00:38:31.732] Browser WARN    Inference API failed, using simulation 400
[00:38:31.732] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2950421953201626
[00:38:32.086] Server  ERROR   Backend returned 400
[00:38:32.110] Server  ERROR   Backend returned 400
[00:38:32.118] Server  ERROR   Backend returned 400
[00:38:32.224] Browser WARN    Inference API failed, using simulation 400
[00:38:32.224] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.32508254300580053
[00:38:32.224] Browser WARN    Inference API failed, using simulation 400
[00:38:32.224] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.28238083170251893
[00:38:32.224] Browser WARN    Inference API failed, using simulation 400
[00:38:32.224] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2883694809913835
[00:38:32.581] Server  ERROR   Backend returned 400
[00:38:32.604] Server  ERROR   Backend returned 400
[00:38:32.618] Server  ERROR   Backend returned 400
[00:38:32.723] Browser WARN    Inference API failed, using simulation 400
[00:38:32.723] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7124596495786716
[00:38:32.723] Browser WARN    Inference API failed, using simulation 400
[00:38:32.723] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.416616682044915
[00:38:32.723] Browser WARN    Inference API failed, using simulation 400
[00:38:32.723] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2651841609978695
[00:38:33.085] Server  ERROR   Backend returned 400
[00:38:33.101] Server  ERROR   Backend returned 400
[00:38:33.113] Server  ERROR   Backend returned 400
[00:38:33.217] Browser WARN    Inference API failed, using simulation 400
[00:38:33.217] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.028401378363416885
[00:38:33.217] Browser WARN    Inference API failed, using simulation 400
[00:38:33.217] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2558264639239217
[00:38:33.217] Browser WARN    Inference API failed, using simulation 400
[00:38:33.217] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7937432955776554
[00:38:33.579] Server  ERROR   Backend returned 400
[00:38:33.596] Server  ERROR   Backend returned 400
[00:38:33.610] Server  ERROR   Backend returned 400
[00:38:33.723] Browser WARN    Inference API failed, using simulation 400
[00:38:33.723] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7617018257594389
[00:38:33.723] Browser WARN    Inference API failed, using simulation 400
[00:38:33.723] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.28999075590315326
[00:38:33.723] Browser WARN    Inference API failed, using simulation 400
[00:38:33.723] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.34109500312200886
[00:38:34.091] Server  ERROR   Backend returned 400
[00:38:34.106] Server  ERROR   Backend returned 400
[00:38:34.120] Server  ERROR   Backend returned 400
[00:38:34.224] Browser WARN    Inference API failed, using simulation 400
[00:38:34.224] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8851972626530666
[00:38:34.224] Browser WARN    Inference API failed, using simulation 400
[00:38:34.224] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.31007745621881516
[00:38:34.224] Browser WARN    Inference API failed, using simulation 400
[00:38:34.224] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.26625081406478734
[00:38:34.580] Server  ERROR   Backend returned 400
[00:38:34.597] Server  ERROR   Backend returned 400
[00:38:34.619] Server  ERROR   Backend returned 400
[00:38:34.724] Browser WARN    Inference API failed, using simulation 400
[00:38:34.724] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7994786020965028
[00:38:34.724] Browser WARN    Inference API failed, using simulation 400
[00:38:34.724] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8419823715555155
[00:38:34.724] Browser WARN    Inference API failed, using simulation 400
[00:38:34.724] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7285707264329626
[00:38:35.079] Server  ERROR   Backend returned 400
[00:38:35.094] Server  ERROR   Backend returned 400
[00:38:35.106] Server  ERROR   Backend returned 400
[00:38:35.239] Browser WARN    Inference API failed, using simulation 400
[00:38:35.239] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.38026275345493693
[00:38:35.239] Browser WARN    Inference API failed, using simulation 400
[00:38:35.239] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4073426173210106
[00:38:35.239] Browser WARN    Inference API failed, using simulation 400
[00:38:35.239] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1064408353073028
[00:38:35.586] Server  ERROR   Backend returned 400
[00:38:35.607] Server  ERROR   Backend returned 400
[00:38:35.620] Server  ERROR   Backend returned 400
[00:38:35.723] Browser WARN    Inference API failed, using simulation 400
[00:38:35.723] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9884608538092547
[00:38:35.723] Browser WARN    Inference API failed, using simulation 400
[00:38:35.723] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2558976118806279
[00:38:35.723] Browser WARN    Inference API failed, using simulation 400
[00:38:35.723] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.33391952788966256
[00:38:36.077] Server  ERROR   Backend returned 400
[00:38:36.097] Server  ERROR   Backend returned 400
[00:38:36.109] Server  ERROR   Backend returned 400
[00:38:36.214] Browser WARN    Inference API failed, using simulation 400
[00:38:36.214] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.43452283463819713
[00:38:36.214] Browser WARN    Inference API failed, using simulation 400
[00:38:36.214] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.15346416725488415
[00:38:36.214] Browser WARN    Inference API failed, using simulation 400
[00:38:36.214] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8663245300435867
[00:38:36.583] Server  ERROR   Backend returned 400
[00:38:36.600] Server  ERROR   Backend returned 400
[00:38:36.616] Server  ERROR   Backend returned 400
[00:38:36.726] Browser WARN    Inference API failed, using simulation 400
[00:38:36.726] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.37507048239173413
[00:38:36.726] Browser WARN    Inference API failed, using simulation 400
[00:38:36.726] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.11511954599817525
[00:38:36.726] Browser WARN    Inference API failed, using simulation 400
[00:38:36.726] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.42032196149099615
[00:38:37.081] Server  ERROR   Backend returned 400
[00:38:37.097] Server  ERROR   Backend returned 400
[00:38:37.108] Server  ERROR   Backend returned 400
[00:38:37.225] Browser WARN    Inference API failed, using simulation 400
[00:38:37.225] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.15090750617596121
[00:38:37.225] Browser WARN    Inference API failed, using simulation 400
[00:38:37.225] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7232162409407621
[00:38:37.225] Browser WARN    Inference API failed, using simulation 400
[00:38:37.225] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.44110430864229494
[00:38:37.586] Server  ERROR   Backend returned 400
[00:38:37.605] Server  ERROR   Backend returned 400
[00:38:37.621] Server  ERROR   Backend returned 400
[00:38:37.725] Browser WARN    Inference API failed, using simulation 400
[00:38:37.725] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.41236374369025824
[00:38:37.725] Browser WARN    Inference API failed, using simulation 400
[00:38:37.725] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4675475097134273
[00:38:37.725] Browser WARN    Inference API failed, using simulation 400
[00:38:37.725] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.022752123023541515
[00:38:38.076] Server  ERROR   Backend returned 400
[00:38:38.096] Server  ERROR   Backend returned 400
[00:38:38.120] Server  ERROR   Backend returned 400
[00:38:38.224] Browser WARN    Inference API failed, using simulation 400
[00:38:38.224] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9258084403842908
[00:38:38.224] Browser WARN    Inference API failed, using simulation 400
[00:38:38.224] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.04332846272086138
[00:38:38.224] Browser WARN    Inference API failed, using simulation 400
[00:38:38.224] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4720398629176739
[00:38:38.581] Server  ERROR   Backend returned 400
[00:38:38.599] Server  ERROR   Backend returned 400
[00:38:38.614] Server  ERROR   Backend returned 400
[00:38:38.717] Browser WARN    Inference API failed, using simulation 400
[00:38:38.717] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9291445333162803
[00:38:38.717] Browser WARN    Inference API failed, using simulation 400
[00:38:38.717] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.32690598417470806
[00:38:38.717] Browser WARN    Inference API failed, using simulation 400
[00:38:38.717] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.31269393822390856
[00:38:39.080] Server  ERROR   Backend returned 400
[00:38:39.100] Server  ERROR   Backend returned 400
[00:38:39.112] Server  ERROR   Backend returned 400
[00:38:39.217] Browser WARN    Inference API failed, using simulation 400
[00:38:39.217] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2677876269030683
[00:38:39.217] Browser WARN    Inference API failed, using simulation 400
[00:38:39.217] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4319476439209895
[00:38:39.217] Browser WARN    Inference API failed, using simulation 400
[00:38:39.217] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9407956258649135
[00:38:39.575] Server  ERROR   Backend returned 400
[00:38:39.595] Server  ERROR   Backend returned 400
[00:38:39.623] Server  ERROR   Backend returned 400
[00:38:39.728] Browser WARN    Inference API failed, using simulation 400
[00:38:39.728] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9621037248699393
[00:38:39.728] Browser WARN    Inference API failed, using simulation 400
[00:38:39.728] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7265503955602945
[00:38:39.728] Browser WARN    Inference API failed, using simulation 400
[00:38:39.728] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.005105804904551259
[00:38:40.087] Server  ERROR   Backend returned 400
[00:38:40.110] Server  ERROR   Backend returned 400
[00:38:40.122] Server  ERROR   Backend returned 400
[00:38:40.226] Browser WARN    Inference API failed, using simulation 400
[00:38:40.226] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8034489583168438
[00:38:40.226] Browser WARN    Inference API failed, using simulation 400
[00:38:40.226] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.18612403556648638
[00:38:40.226] Browser WARN    Inference API failed, using simulation 400
[00:38:40.226] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2893513774920238
[00:38:40.591] Server  ERROR   Backend returned 400
[00:38:40.605] Server  ERROR   Backend returned 400
[00:38:40.614] Server  ERROR   Backend returned 400
[00:38:40.722] Browser WARN    Inference API failed, using simulation 400
[00:38:40.722] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.10260595288874375
[00:38:40.722] Browser WARN    Inference API failed, using simulation 400
[00:38:40.722] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.07010632784955884
[00:38:40.722] Browser WARN    Inference API failed, using simulation 400
[00:38:40.722] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.01494210535826579
[00:38:41.088] Server  ERROR   Backend returned 400
[00:38:41.103] Server  ERROR   Backend returned 400
[00:38:41.116] Server  ERROR   Backend returned 400
[00:38:41.221] Browser WARN    Inference API failed, using simulation 400
[00:38:41.221] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.16592308137433803
[00:38:41.221] Browser WARN    Inference API failed, using simulation 400
[00:38:41.221] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4473504618612823
[00:38:41.221] Browser WARN    Inference API failed, using simulation 400
[00:38:41.221] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9368716559623554
[00:38:41.583] Server  ERROR   Backend returned 400
[00:38:41.602] Server  ERROR   Backend returned 400
[00:38:41.617] Server  ERROR   Backend returned 400
[00:38:41.720] Browser WARN    Inference API failed, using simulation 400
[00:38:41.720] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.20977528609653923
[00:38:41.720] Browser WARN    Inference API failed, using simulation 400
[00:38:41.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1394159013517341
[00:38:41.720] Browser WARN    Inference API failed, using simulation 400
[00:38:41.720] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.08882235120912735
[00:38:42.078] Server  ERROR   Backend returned 400
[00:38:42.103] Server  ERROR   Backend returned 400
[00:38:42.114] Server  ERROR   Backend returned 400
[00:38:42.218] Browser WARN    Inference API failed, using simulation 400
[00:38:42.218] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9053140260991612
[00:38:42.218] Browser WARN    Inference API failed, using simulation 400
[00:38:42.218] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.06974973499259468
[00:38:42.218] Browser WARN    Inference API failed, using simulation 400
[00:38:42.218] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.04583284616231259
[00:38:42.578] Server  ERROR   Backend returned 400
[00:38:42.603] Server  ERROR   Backend returned 400
[00:38:42.615] Server  ERROR   Backend returned 400
[00:38:42.720] Browser WARN    Inference API failed, using simulation 400
[00:38:42.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.27596101599715367
[00:38:42.720] Browser WARN    Inference API failed, using simulation 400
[00:38:42.720] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.312961593294717
[00:38:42.720] Browser WARN    Inference API failed, using simulation 400
[00:38:42.720] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4090822848157011
[00:38:43.078] Server  ERROR   Backend returned 400
[00:38:43.093] Server  ERROR   Backend returned 400
[00:38:43.112] Server  ERROR   Backend returned 400
[00:38:43.227] Browser WARN    Inference API failed, using simulation 400
[00:38:43.227] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.21406198392912917
[00:38:43.228] Browser WARN    Inference API failed, using simulation 400
[00:38:43.228] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9643957691240246
[00:38:43.228] Browser WARN    Inference API failed, using simulation 400
[00:38:43.228] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.24409330990642747
[00:38:43.588] Server  ERROR   Backend returned 400
[00:38:43.600] Server  ERROR   Backend returned 400
[00:38:43.621] Server  ERROR   Backend returned 400
[00:38:43.724] Browser WARN    Inference API failed, using simulation 400
[00:38:43.724] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.10256126932915755
[00:38:43.724] Browser WARN    Inference API failed, using simulation 400
[00:38:43.724] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4619024444391307
[00:38:43.724] Browser WARN    Inference API failed, using simulation 400
[00:38:43.724] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.461183460848032
[00:38:44.084] Server  ERROR   Backend returned 400
[00:38:44.099] Server  ERROR   Backend returned 400
[00:38:44.111] Server  ERROR   Backend returned 400
[00:38:44.216] Browser WARN    Inference API failed, using simulation 400
[00:38:44.216] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3234514443004283
[00:38:44.216] Browser WARN    Inference API failed, using simulation 400
[00:38:44.216] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.975169489701061
[00:38:44.216] Browser WARN    Inference API failed, using simulation 400
[00:38:44.216] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.382858445048155
[00:38:44.582] Server  ERROR   Backend returned 400
[00:38:44.599] Server  ERROR   Backend returned 400
[00:38:44.615] Server  ERROR   Backend returned 400
[00:38:44.719] Browser WARN    Inference API failed, using simulation 400
[00:38:44.719] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.923346758543041
[00:38:44.719] Browser WARN    Inference API failed, using simulation 400
[00:38:44.719] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4557008174097252
[00:38:44.719] Browser WARN    Inference API failed, using simulation 400
[00:38:44.719] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7393435288839277
[00:38:45.081] Server  ERROR   Backend returned 400
[00:38:45.097] Server  ERROR   Backend returned 400
[00:38:45.120] Server  ERROR   Backend returned 400
[00:38:45.235] Browser WARN    Inference API failed, using simulation 400
[00:38:45.235] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.876175369650202
[00:38:45.235] Browser WARN    Inference API failed, using simulation 400
[00:38:45.235] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.19900361572302488
[00:38:45.235] Browser WARN    Inference API failed, using simulation 400
[00:38:45.235] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.0763051758865636
[00:38:45.576] Server  ERROR   Backend returned 400
[00:38:45.600] Server  ERROR   Backend returned 400
[00:38:45.614] Server  ERROR   Backend returned 400
[00:38:45.718] Browser WARN    Inference API failed, using simulation 400
[00:38:45.718] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7364004654069107
[00:38:45.718] Browser WARN    Inference API failed, using simulation 400
[00:38:45.718] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3739912345244337
[00:38:45.718] Browser WARN    Inference API failed, using simulation 400
[00:38:45.718] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.04305717747322785
[00:38:46.084] Server  ERROR   Backend returned 400
[00:38:46.128] Server  ERROR   Backend returned 400
[00:38:46.134] Server  ERROR   Backend returned 400
[00:38:46.240] Browser WARN    Inference API failed, using simulation 400
[00:38:46.240] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.14487298205605215
[00:38:46.240] Browser WARN    Inference API failed, using simulation 400
[00:38:46.240] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.03692132636164813
[00:38:46.240] Browser WARN    Inference API failed, using simulation 400
[00:38:46.240] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8414359379832896
[00:38:46.585] Server  ERROR   Backend returned 400
[00:38:46.600] Server  ERROR   Backend returned 400
[00:38:46.619] Server  ERROR   Backend returned 400
[00:38:46.723] Browser WARN    Inference API failed, using simulation 400
[00:38:46.723] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.11656697325458582
[00:38:46.723] Browser WARN    Inference API failed, using simulation 400
[00:38:46.723] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7295376377610081
[00:38:46.723] Browser WARN    Inference API failed, using simulation 400
[00:38:46.723] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.09443306872330931
[00:38:47.081] Server  ERROR   Backend returned 400
[00:38:47.109] Server  ERROR   Backend returned 400
[00:38:47.114] Server  ERROR   Backend returned 400
[00:38:47.227] Browser WARN    Inference API failed, using simulation 400
[00:38:47.227] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1619394701722166
[00:38:47.227] Browser WARN    Inference API failed, using simulation 400
[00:38:47.227] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.30852377875741377
[00:38:47.227] Browser WARN    Inference API failed, using simulation 400
[00:38:47.227] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.01656989419544902
[00:38:47.586] Server  ERROR   Backend returned 400
[00:38:47.601] Server  ERROR   Backend returned 400
[00:38:47.616] Server  ERROR   Backend returned 400
[00:38:47.722] Browser WARN    Inference API failed, using simulation 400
[00:38:47.722] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4531132570848913
[00:38:47.722] Browser WARN    Inference API failed, using simulation 400
[00:38:47.722] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3527335083072971
[00:38:47.722] Browser WARN    Inference API failed, using simulation 400
[00:38:47.722] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4862296409985177
[00:38:48.077] Server  ERROR   Backend returned 400
[00:38:48.127] Server  ERROR   Backend returned 400
[00:38:48.130] Server  ERROR   Backend returned 400
[00:38:48.242] Browser WARN    Inference API failed, using simulation 400
[00:38:48.242] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9814891813234268
[00:38:48.242] Browser WARN    Inference API failed, using simulation 400
[00:38:48.242] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7998512449166859
[00:38:48.242] Browser WARN    Inference API failed, using simulation 400
[00:38:48.242] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8086966930767313
[00:38:48.580] Server  ERROR   Backend returned 400
[00:38:48.605] Server  ERROR   Backend returned 400
[00:38:48.619] Server  ERROR   Backend returned 400
[00:38:48.723] Browser WARN    Inference API failed, using simulation 400
[00:38:48.723] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.39557903642732
[00:38:48.723] Browser WARN    Inference API failed, using simulation 400
[00:38:48.723] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1276338833763268
[00:38:48.723] Browser WARN    Inference API failed, using simulation 400
[00:38:48.723] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8364867228783499
[00:38:49.086] Server  ERROR   Backend returned 400
[00:38:49.104] Server  ERROR   Backend returned 400
[00:38:49.116] Server  ERROR   Backend returned 400
[00:38:49.221] Browser WARN    Inference API failed, using simulation 400
[00:38:49.221] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.28793073700667476
[00:38:49.221] Browser WARN    Inference API failed, using simulation 400
[00:38:49.221] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7063821064402488
[00:38:49.221] Browser WARN    Inference API failed, using simulation 400
[00:38:49.221] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9903511400524869
[00:38:49.578] Server  ERROR   Backend returned 400
[00:38:49.609] Server  ERROR   Backend returned 400
[00:38:49.612] Server  ERROR   Backend returned 400
[00:38:49.720] Browser WARN    Inference API failed, using simulation 400
[00:38:49.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3537334636911047
[00:38:49.721] Browser WARN    Inference API failed, using simulation 400
[00:38:49.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.41671306574786526
[00:38:49.721] Browser WARN    Inference API failed, using simulation 400
[00:38:49.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3963299313117785
[00:38:50.087] Server  ERROR   Backend returned 400
[00:38:50.108] Server  ERROR   Backend returned 400
[00:38:50.119] Server  ERROR   Backend returned 400
[00:38:50.223] Browser WARN    Inference API failed, using simulation 400
[00:38:50.223] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1270421122683147
[00:38:50.223] Browser WARN    Inference API failed, using simulation 400
[00:38:50.223] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.06961866975431807
[00:38:50.223] Browser WARN    Inference API failed, using simulation 400
[00:38:50.223] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3477473135942061
[00:38:50.577] Server  ERROR   Backend returned 400
[00:38:50.603] Server  ERROR   Backend returned 400
[00:38:50.616] Server  ERROR   Backend returned 400
[00:38:50.729] Browser WARN    Inference API failed, using simulation 400
[00:38:50.729] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.29468560865502547
[00:38:50.729] Browser WARN    Inference API failed, using simulation 400
[00:38:50.729] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9657827710351266
[00:38:50.729] Browser WARN    Inference API failed, using simulation 400
[00:38:50.729] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8043027464548012
[00:38:51.085] Server  ERROR   Backend returned 400
[00:38:51.101] Server  ERROR   Backend returned 400
[00:38:51.112] Server  ERROR   Backend returned 400
[00:38:51.217] Browser WARN    Inference API failed, using simulation 400
[00:38:51.217] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3506771717427317
[00:38:51.217] Browser WARN    Inference API failed, using simulation 400
[00:38:51.217] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.278835154900924
[00:38:51.217] Browser WARN    Inference API failed, using simulation 400
[00:38:51.217] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.016673877595844766
[00:38:51.579] Server  ERROR   Backend returned 400
[00:38:51.603] Server  ERROR   Backend returned 400
[00:38:51.617] Server  ERROR   Backend returned 400
[00:38:51.721] Browser WARN    Inference API failed, using simulation 400
[00:38:51.721] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9060487668220429
[00:38:51.721] Browser WARN    Inference API failed, using simulation 400
[00:38:51.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2150070728919693
[00:38:51.721] Browser WARN    Inference API failed, using simulation 400
[00:38:51.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4846711402876958
[00:38:52.088] Server  ERROR   Backend returned 400
[00:38:52.103] Server  ERROR   Backend returned 400
[00:38:52.116] Server  ERROR   Backend returned 400
[00:38:52.220] Browser WARN    Inference API failed, using simulation 400
[00:38:52.220] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8464316845511088
[00:38:52.220] Browser WARN    Inference API failed, using simulation 400
[00:38:52.220] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.44593976465286733
[00:38:52.220] Browser WARN    Inference API failed, using simulation 400
[00:38:52.220] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3836743932786477
[00:38:52.578] Server  ERROR   Backend returned 400
[00:38:52.603] Server  ERROR   Backend returned 400
[00:38:52.615] Server  ERROR   Backend returned 400
[00:38:52.719] Browser WARN    Inference API failed, using simulation 400
[00:38:52.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.15762117153359873
[00:38:52.720] Browser WARN    Inference API failed, using simulation 400
[00:38:52.720] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.41894377350373657
[00:38:52.720] Browser WARN    Inference API failed, using simulation 400
[00:38:52.720] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8968907966786028
[00:38:53.079] Server  ERROR   Backend returned 400
[00:38:53.099] Server  ERROR   Backend returned 400
[00:38:53.114] Server  ERROR   Backend returned 400
[00:38:53.217] Browser WARN    Inference API failed, using simulation 400
[00:38:53.217] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2820819463317508
[00:38:53.217] Browser WARN    Inference API failed, using simulation 400
[00:38:53.217] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9037563600059129
[00:38:53.217] Browser WARN    Inference API failed, using simulation 400
[00:38:53.217] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.014443844313518861
[00:38:53.579] Server  ERROR   Backend returned 400
[00:38:53.594] Server  ERROR   Backend returned 400
[00:38:53.612] Server  ERROR   Backend returned 400
[00:38:53.718] Browser WARN    Inference API failed, using simulation 400
[00:38:53.718] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.23876202883951175
[00:38:53.718] Browser WARN    Inference API failed, using simulation 400
[00:38:53.718] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3614253367673223
[00:38:53.718] Browser WARN    Inference API failed, using simulation 400
[00:38:53.718] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1247676113137473
[00:38:54.087] Server  ERROR   Backend returned 400
[00:38:54.105] Server  ERROR   Backend returned 400
[00:38:54.121] Server  ERROR   Backend returned 400
[00:38:54.225] Browser WARN    Inference API failed, using simulation 400
[00:38:54.225] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.10069312196957542
[00:38:54.225] Browser WARN    Inference API failed, using simulation 400
[00:38:54.225] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7644224929444113
[00:38:54.225] Browser WARN    Inference API failed, using simulation 400
[00:38:54.225] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.12531611884875604
[00:38:54.579] Server  ERROR   Backend returned 400
[00:38:54.593] Server  ERROR   Backend returned 400
[00:38:54.605] Server  ERROR   Backend returned 400
[00:38:54.710] Browser WARN    Inference API failed, using simulation 400
[00:38:54.710] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7477451957523956
[00:38:54.710] Browser WARN    Inference API failed, using simulation 400
[00:38:54.710] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2957820727057938
[00:38:54.710] Browser WARN    Inference API failed, using simulation 400
[00:38:54.710] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7896221437605593
[00:38:55.083] Server  ERROR   Backend returned 400
[00:38:55.101] Server  ERROR   Backend returned 400
[00:38:55.114] Server  ERROR   Backend returned 400
[00:38:55.218] Browser WARN    Inference API failed, using simulation 400
[00:38:55.218] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4075182049565688
[00:38:55.218] Browser WARN    Inference API failed, using simulation 400
[00:38:55.218] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8205195946107167
[00:38:55.218] Browser WARN    Inference API failed, using simulation 400
[00:38:55.218] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7529388956475765
[00:38:55.577] Server  ERROR   Backend returned 400
[00:38:55.601] Server  ERROR   Backend returned 400
[00:38:55.614] Server  ERROR   Backend returned 400
[00:38:55.717] Browser WARN    Inference API failed, using simulation 400
[00:38:55.717] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4936860560520439
[00:38:55.717] Browser WARN    Inference API failed, using simulation 400
[00:38:55.717] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7522398984209097
[00:38:55.717] Browser WARN    Inference API failed, using simulation 400
[00:38:55.717] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3665121798655621
[00:38:56.079] Server  ERROR   Backend returned 400
[00:38:56.112] Server  ERROR   Backend returned 400
[00:38:56.119] Server  ERROR   Backend returned 400
[00:38:56.225] Browser WARN    Inference API failed, using simulation 400
[00:38:56.225] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8448630445592737
[00:38:56.225] Browser WARN    Inference API failed, using simulation 400
[00:38:56.225] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.934407193414347
[00:38:56.225] Browser WARN    Inference API failed, using simulation 400
[00:38:56.225] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9742125744645556
[00:38:56.579] Server  ERROR   Backend returned 400
[00:38:56.620] Server  ERROR   Backend returned 400
[00:38:56.623] Server  ERROR   Backend returned 400
[00:38:56.733] Browser WARN    Inference API failed, using simulation 400
[00:38:56.733] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.07649624574023345
[00:38:56.733] Browser WARN    Inference API failed, using simulation 400
[00:38:56.733] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.14266147311215543
[00:38:56.733] Browser WARN    Inference API failed, using simulation 400
[00:38:56.733] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.271851093538514
[00:38:57.081] Server  ERROR   Backend returned 400
[00:38:57.117] Server  ERROR   Backend returned 400
[00:38:57.120] Server  ERROR   Backend returned 400
[00:38:57.229] Browser WARN    Inference API failed, using simulation 400
[00:38:57.229] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.15430739521344633
[00:38:57.229] Browser WARN    Inference API failed, using simulation 400
[00:38:57.229] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.08691026718751055
[00:38:57.229] Browser WARN    Inference API failed, using simulation 400
[00:38:57.229] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.38116233418542517
[00:38:57.582] Server  ERROR   Backend returned 400
[00:38:57.609] Server  ERROR   Backend returned 400
[00:38:57.613] Server  ERROR   Backend returned 400
[00:38:57.721] Browser WARN    Inference API failed, using simulation 400
[00:38:57.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.14585678379079808
[00:38:57.721] Browser WARN    Inference API failed, using simulation 400
[00:38:57.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2470017164623824
[00:38:57.721] Browser WARN    Inference API failed, using simulation 400
[00:38:57.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2652325833552486
[00:38:58.077] Server  ERROR   Backend returned 400
[00:38:58.102] Server  ERROR   Backend returned 400
[00:38:58.114] Server  ERROR   Backend returned 400
[00:38:58.218] Browser WARN    Inference API failed, using simulation 400
[00:38:58.218] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9772130219405656
[00:38:58.218] Browser WARN    Inference API failed, using simulation 400
[00:38:58.218] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.01838302540416481
[00:38:58.218] Browser WARN    Inference API failed, using simulation 400
[00:38:58.218] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.453049761383805
[00:38:58.578] Server  ERROR   Backend returned 400
[00:38:58.597] Server  ERROR   Backend returned 400
[00:38:58.612] Server  ERROR   Backend returned 400
[00:38:58.716] Browser WARN    Inference API failed, using simulation 400
[00:38:58.716] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7576513487572719
[00:38:58.716] Browser WARN    Inference API failed, using simulation 400
[00:38:58.716] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.06933602947349987
[00:38:58.716] Browser WARN    Inference API failed, using simulation 400
[00:38:58.716] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.23183200743036703
[00:38:59.107] Server  ERROR   Backend returned 400
[00:38:59.124] Server  ERROR   Backend returned 400
[00:38:59.143] Server  ERROR   Backend returned 400
[00:38:59.248] Browser WARN    Inference API failed, using simulation 400
[00:38:59.249] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7649735653182614
[00:38:59.249] Browser WARN    Inference API failed, using simulation 400
[00:38:59.249] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.31730549308455286
[00:38:59.249] Browser WARN    Inference API failed, using simulation 400
[00:38:59.249] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9686556023451423
[00:38:59.581] Server  ERROR   Backend returned 400
[00:38:59.599] Server  ERROR   Backend returned 400
[00:38:59.616] Server  ERROR   Backend returned 400
[00:38:59.723] Browser WARN    Inference API failed, using simulation 400
[00:38:59.723] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9866087548810913
[00:38:59.723] Browser WARN    Inference API failed, using simulation 400
[00:38:59.723] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9735759394775918
[00:38:59.723] Browser WARN    Inference API failed, using simulation 400
[00:38:59.723] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.41449324891811
[00:39:00.078] Server  ERROR   Backend returned 400
[00:39:00.104] Server  ERROR   Backend returned 400
[00:39:00.126] Server  ERROR   Backend returned 400
[00:39:00.229] Browser WARN    Inference API failed, using simulation 400
[00:39:00.229] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3892628038275238
[00:39:00.229] Browser WARN    Inference API failed, using simulation 400
[00:39:00.229] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2571927042782417
[00:39:00.229] Browser WARN    Inference API failed, using simulation 400
[00:39:00.229] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1545288649903479
[00:39:00.595] Server  ERROR   Backend returned 400
[00:39:00.613] Server  ERROR   Backend returned 400
[00:39:00.626] Server  ERROR   Backend returned 400
[00:39:00.729] Browser WARN    Inference API failed, using simulation 400
[00:39:00.729] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.757028792421171
[00:39:00.729] Browser WARN    Inference API failed, using simulation 400
[00:39:00.729] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.13428279213649336
[00:39:00.729] Browser WARN    Inference API failed, using simulation 400
[00:39:00.729] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.814371153999637
[00:39:01.085] Server  ERROR   Backend returned 400
[00:39:01.103] Server  ERROR   Backend returned 400
[00:39:01.116] Server  ERROR   Backend returned 400
[00:39:01.219] Browser WARN    Inference API failed, using simulation 400
[00:39:01.219] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.21597302507395089
[00:39:01.219] Browser WARN    Inference API failed, using simulation 400
[00:39:01.219] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.31169691662303706
[00:39:01.219] Browser WARN    Inference API failed, using simulation 400
[00:39:01.219] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7567877737198895
[00:39:01.580] Server  ERROR   Backend returned 400
[00:39:01.613] Server  ERROR   Backend returned 400
[00:39:01.619] Server  ERROR   Backend returned 400
[00:39:01.726] Browser WARN    Inference API failed, using simulation 400
[00:39:01.726] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7853484629107483
[00:39:01.726] Browser WARN    Inference API failed, using simulation 400
[00:39:01.726] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3014144397981574
[00:39:01.726] Browser WARN    Inference API failed, using simulation 400
[00:39:01.726] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7573766419647718
[00:39:02.078] Server  ERROR   Backend returned 400
[00:39:02.106] Server  ERROR   Backend returned 400
[00:39:02.108] Server  ERROR   Backend returned 400
[00:39:02.218] Browser WARN    Inference API failed, using simulation 400
[00:39:02.218] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.47142955712197376
[00:39:02.218] Browser WARN    Inference API failed, using simulation 400
[00:39:02.218] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.25265005724148654
[00:39:02.218] Browser WARN    Inference API failed, using simulation 400
[00:39:02.218] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.35215570225243853
[00:39:02.578] Server  ERROR   Backend returned 400
[00:39:02.601] Server  ERROR   Backend returned 400
[00:39:02.604] Server  ERROR   Backend returned 400
[00:39:02.713] Browser WARN    Inference API failed, using simulation 400
[00:39:02.713] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.08569951477040316
[00:39:02.713] Browser WARN    Inference API failed, using simulation 400
[00:39:02.713] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.23946914926141338
[00:39:02.713] Browser WARN    Inference API failed, using simulation 400
[00:39:02.713] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.76613632361237
[00:39:03.079] Server  ERROR   Backend returned 400
[00:39:03.096] Server  ERROR   Backend returned 400
[00:39:03.115] Server  ERROR   Backend returned 400
[00:39:03.223] Browser WARN    Inference API failed, using simulation 400
[00:39:03.223] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4624037912099832
[00:39:03.223] Browser WARN    Inference API failed, using simulation 400
[00:39:03.223] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8306491115822474
[00:39:03.223] Browser WARN    Inference API failed, using simulation 400
[00:39:03.223] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.004505834140789811
[00:39:03.582] Server  ERROR   Backend returned 400
[00:39:03.606] Server  ERROR   Backend returned 400
[00:39:03.621] Server  ERROR   Backend returned 400
[00:39:03.724] Browser WARN    Inference API failed, using simulation 400
[00:39:03.724] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8756776212596378
[00:39:03.724] Browser WARN    Inference API failed, using simulation 400
[00:39:03.724] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8249251245712674
[00:39:03.724] Browser WARN    Inference API failed, using simulation 400
[00:39:03.724] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2436141238154777
[00:39:04.078] Server  ERROR   Backend returned 400
[00:39:04.106] Server  ERROR   Backend returned 400
[00:39:04.119] Server  ERROR   Backend returned 400
[00:39:04.223] Browser WARN    Inference API failed, using simulation 400
[00:39:04.223] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.24994689513154905
[00:39:04.223] Browser WARN    Inference API failed, using simulation 400
[00:39:04.223] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7171720267500831
[00:39:04.223] Browser WARN    Inference API failed, using simulation 400
[00:39:04.223] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.37771180795537584
[00:39:04.579] Server  ERROR   Backend returned 400
[00:39:04.609] Server  ERROR   Backend returned 400
[00:39:04.613] Server  ERROR   Backend returned 400
[00:39:04.721] Browser WARN    Inference API failed, using simulation 400
[00:39:04.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.49598108322111706
[00:39:04.721] Browser WARN    Inference API failed, using simulation 400
[00:39:04.721] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9088768289652288
[00:39:04.721] Browser WARN    Inference API failed, using simulation 400
[00:39:04.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4795552365292321
[00:39:05.082] Server  ERROR   Backend returned 400
[00:39:05.101] Server  ERROR   Backend returned 400
[00:39:05.115] Server  ERROR   Backend returned 400
[00:39:05.218] Browser WARN    Inference API failed, using simulation 400
[00:39:05.218] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7140484287371328
[00:39:05.218] Browser WARN    Inference API failed, using simulation 400
[00:39:05.218] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.42102024036353136
[00:39:05.218] Browser WARN    Inference API failed, using simulation 400
[00:39:05.218] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.945207733860201
[00:39:05.622] Server  ERROR   Backend returned 400
[00:39:05.742] Server  ERROR   Backend returned 400
[00:39:05.745] Browser WARN    Inference API failed, using simulation 400
[00:39:05.745] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9991816788413397
[00:39:05.746] Server  ERROR   Backend returned 400
[00:39:05.855] Browser WARN    Inference API failed, using simulation 400
[00:39:05.855] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.13797264228105888
[00:39:05.855] Browser WARN    Inference API failed, using simulation 400
[00:39:05.855] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8263737847841588
[00:39:06.078] Server  ERROR   Backend returned 400
[00:39:06.097] Server  ERROR   Backend returned 400
[00:39:06.112] Server  ERROR   Backend returned 400
[00:39:06.217] Browser WARN    Inference API failed, using simulation 400
[00:39:06.217] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.47962770297321433
[00:39:06.217] Browser WARN    Inference API failed, using simulation 400
[00:39:06.217] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9729950876510114
[00:39:06.217] Browser WARN    Inference API failed, using simulation 400
[00:39:06.217] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.08395006288376045
[00:39:06.587] Server  ERROR   Backend returned 400
[00:39:06.609] Server  ERROR   Backend returned 400
[00:39:06.623] Server  ERROR   Backend returned 400
[00:39:06.728] Browser WARN    Inference API failed, using simulation 400
[00:39:06.728] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7522565616277528
[00:39:06.728] Browser WARN    Inference API failed, using simulation 400
[00:39:06.728] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9659463131765094
[00:39:06.728] Browser WARN    Inference API failed, using simulation 400
[00:39:06.728] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8302903261640876
[00:39:07.084] Server  ERROR   Backend returned 400
[00:39:07.102] Server  ERROR   Backend returned 400
[00:39:07.115] Server  ERROR   Backend returned 400
[00:39:07.218] Browser WARN    Inference API failed, using simulation 400
[00:39:07.218] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7963328679373682
[00:39:07.218] Browser WARN    Inference API failed, using simulation 400
[00:39:07.218] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.06678244367757757
[00:39:07.218] Browser WARN    Inference API failed, using simulation 400
[00:39:07.218] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.055195081316257166
[00:39:07.586] Server  ERROR   Backend returned 400
[00:39:07.605] Server  ERROR   Backend returned 400
[00:39:07.618] Server  ERROR   Backend returned 400
[00:39:07.723] Browser WARN    Inference API failed, using simulation 400
[00:39:07.723] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3262357768159363
[00:39:07.723] Browser WARN    Inference API failed, using simulation 400
[00:39:07.723] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.08740854452594354
[00:39:07.723] Browser WARN    Inference API failed, using simulation 400
[00:39:07.723] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.37060367703599684
[00:39:08.077] Server  ERROR   Backend returned 400
[00:39:08.116] Server  ERROR   Backend returned 400
[00:39:08.119] Server  ERROR   Backend returned 400
[00:39:08.228] Browser WARN    Inference API failed, using simulation 400
[00:39:08.228] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.723396118205325
[00:39:08.228] Browser WARN    Inference API failed, using simulation 400
[00:39:08.228] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.14157857173633065
[00:39:08.228] Browser WARN    Inference API failed, using simulation 400
[00:39:08.228] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.42788119351246623
[00:39:08.581] Server  ERROR   Backend returned 400
[00:39:08.611] Server  ERROR   Backend returned 400
[00:39:08.627] Server  ERROR   Backend returned 400
[00:39:08.732] Browser WARN    Inference API failed, using simulation 400
[00:39:08.732] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.947436110924269
[00:39:08.732] Browser WARN    Inference API failed, using simulation 400
[00:39:08.732] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.29329244654428327
[00:39:08.732] Browser WARN    Inference API failed, using simulation 400
[00:39:08.732] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4885122315101465
[00:39:09.092] Server  ERROR   Backend returned 400
[00:39:09.126] Server  ERROR   Backend returned 400
[00:39:09.149] Server  ERROR   Backend returned 400
[00:39:09.253] Browser WARN    Inference API failed, using simulation 400
[00:39:09.254] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.43453624352870585
[00:39:09.254] Browser WARN    Inference API failed, using simulation 400
[00:39:09.254] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3302927579359083
[00:39:09.254] Browser WARN    Inference API failed, using simulation 400
[00:39:09.254] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.10502902789920421
[00:39:09.585] Server  ERROR   Backend returned 400
[00:39:09.603] Server  ERROR   Backend returned 400
[00:39:09.616] Server  ERROR   Backend returned 400
[00:39:09.720] Browser WARN    Inference API failed, using simulation 400
[00:39:09.720] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3956350190966038
[00:39:09.720] Browser WARN    Inference API failed, using simulation 400
[00:39:09.720] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.17263125724562667
[00:39:09.720] Browser WARN    Inference API failed, using simulation 400
[00:39:09.720] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9906215361404656
[00:39:10.087] Server  ERROR   Backend returned 400
[00:39:10.116] Server  ERROR   Backend returned 400
[00:39:10.119] Server  ERROR   Backend returned 400
[00:39:10.228] Browser WARN    Inference API failed, using simulation 400
[00:39:10.228] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.03745242670339444
[00:39:10.228] Browser WARN    Inference API failed, using simulation 400
[00:39:10.228] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9882480732032182
[00:39:10.228] Browser WARN    Inference API failed, using simulation 400
[00:39:10.228] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9169453222865401
[00:39:10.581] Server  ERROR   Backend returned 400
[00:39:10.601] Server  ERROR   Backend returned 400
[00:39:10.620] Server  ERROR   Backend returned 400
[00:39:10.725] Browser WARN    Inference API failed, using simulation 400
[00:39:10.725] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7039852944208196
[00:39:10.725] Browser WARN    Inference API failed, using simulation 400
[00:39:10.725] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7150554892889385
[00:39:10.725] Browser WARN    Inference API failed, using simulation 400
[00:39:10.725] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9043988961430377
[00:39:11.075] Server  ERROR   Backend returned 400
[00:39:11.098] Server  ERROR   Backend returned 400
[00:39:11.122] Server  ERROR   Backend returned 400
[00:39:11.225] Browser WARN    Inference API failed, using simulation 400
[00:39:11.225] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.33362612800422903
[00:39:11.225] Browser WARN    Inference API failed, using simulation 400
[00:39:11.225] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9762104005359661
[00:39:11.225] Browser WARN    Inference API failed, using simulation 400
[00:39:11.225] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.03306216839797771
[00:39:11.582] Server  ERROR   Backend returned 400
[00:39:11.610] Server  ERROR   Backend returned 400
[00:39:11.621] Server  ERROR   Backend returned 400
[00:39:11.725] Browser WARN    Inference API failed, using simulation 400
[00:39:11.725] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4730717161521427
[00:39:11.725] Browser WARN    Inference API failed, using simulation 400
[00:39:11.725] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1184092568533186
[00:39:11.725] Browser WARN    Inference API failed, using simulation 400
[00:39:11.725] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.05977325744194212
[00:39:12.076] Server  ERROR   Backend returned 400
[00:39:12.102] Server  ERROR   Backend returned 400
[00:39:12.105] Server  ERROR   Backend returned 400
[00:39:12.213] Browser WARN    Inference API failed, using simulation 400
[00:39:12.213] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.25967064146313523
[00:39:12.213] Browser WARN    Inference API failed, using simulation 400
[00:39:12.213] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.43382957835412134
[00:39:12.213] Browser WARN    Inference API failed, using simulation 400
[00:39:12.213] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2990472888811699
[00:39:12.585] Server  ERROR   Backend returned 400
[00:39:12.607] Server  ERROR   Backend returned 400
[00:39:12.620] Server  ERROR   Backend returned 400
[00:39:12.723] Browser WARN    Inference API failed, using simulation 400
[00:39:12.723] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.34551384139556235
[00:39:12.723] Browser WARN    Inference API failed, using simulation 400
[00:39:12.723] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3084502894694805
[00:39:12.723] Browser WARN    Inference API failed, using simulation 400
[00:39:12.723] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2315935355947208
[00:39:13.078] Server  ERROR   Backend returned 400
[00:39:13.104] Server  ERROR   Backend returned 400
[00:39:13.115] Server  ERROR   Backend returned 400
[00:39:13.220] Browser WARN    Inference API failed, using simulation 400
[00:39:13.220] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2520252559153407
[00:39:13.220] Browser WARN    Inference API failed, using simulation 400
[00:39:13.220] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9765612299436208
[00:39:13.220] Browser WARN    Inference API failed, using simulation 400
[00:39:13.220] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.25046272597075253
[00:39:13.588] Server  ERROR   Backend returned 400
[00:39:13.610] Server  ERROR   Backend returned 400
[00:39:13.626] Server  ERROR   Backend returned 400
[00:39:13.731] Browser WARN    Inference API failed, using simulation 400
[00:39:13.731] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.06147629231067553
[00:39:13.731] Browser WARN    Inference API failed, using simulation 400
[00:39:13.731] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.30886854135264724
[00:39:13.731] Browser WARN    Inference API failed, using simulation 400
[00:39:13.731] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.46212914856807474
[00:39:14.088] Server  ERROR   Backend returned 400
[00:39:14.118] Server  ERROR   Backend returned 400
[00:39:14.120] Server  ERROR   Backend returned 400
[00:39:14.229] Browser WARN    Inference API failed, using simulation 400
[00:39:14.229] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.007974437826922198
[00:39:14.229] Browser WARN    Inference API failed, using simulation 400
[00:39:14.229] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9518227093886887
[00:39:14.229] Browser WARN    Inference API failed, using simulation 400
[00:39:14.229] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.27273357408088583
[00:39:14.579] Server  ERROR   Backend returned 400
[00:39:14.604] Server  ERROR   Backend returned 400
[00:39:14.627] Server  ERROR   Backend returned 400
[00:39:14.732] Browser WARN    Inference API failed, using simulation 400
[00:39:14.732] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9338073729936393
[00:39:14.732] Browser WARN    Inference API failed, using simulation 400
[00:39:14.732] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.09242677689592588
[00:39:14.732] Browser WARN    Inference API failed, using simulation 400
[00:39:14.732] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7678168472253966
[00:39:15.074] Server  ERROR   Backend returned 400
[00:39:15.103] Server  ERROR   Backend returned 400
[00:39:15.115] Server  ERROR   Backend returned 400
[00:39:15.220] Browser WARN    Inference API failed, using simulation 400
[00:39:15.220] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.10443918375851946
[00:39:15.220] Browser WARN    Inference API failed, using simulation 400
[00:39:15.220] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8656956268595313
[00:39:15.220] Browser WARN    Inference API failed, using simulation 400
[00:39:15.220] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7908189491066934
[00:39:15.579] Server  ERROR   Backend returned 400
[00:39:15.605] Server  ERROR   Backend returned 400
[00:39:15.608] Server  ERROR   Backend returned 400
[00:39:15.723] Browser WARN    Inference API failed, using simulation 400
[00:39:15.723] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.47241515866650513
[00:39:15.723] Browser WARN    Inference API failed, using simulation 400
[00:39:15.723] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.17840766244081707
[00:39:15.723] Browser WARN    Inference API failed, using simulation 400
[00:39:15.723] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8773871542591747
[00:39:16.079] Server  ERROR   Backend returned 400
[00:39:16.109] Server  ERROR   Backend returned 400
[00:39:16.124] Server  ERROR   Backend returned 400
[00:39:16.229] Browser WARN    Inference API failed, using simulation 400
[00:39:16.229] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.044641983855708334
[00:39:16.229] Browser WARN    Inference API failed, using simulation 400
[00:39:16.229] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8754369914552115
[00:39:16.229] Browser WARN    Inference API failed, using simulation 400
[00:39:16.229] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2737762726151072
[00:39:16.582] Server  ERROR   Backend returned 400
[00:39:16.601] Server  ERROR   Backend returned 400
[00:39:16.621] Server  ERROR   Backend returned 400
[00:39:16.725] Browser WARN    Inference API failed, using simulation 400
[00:39:16.725] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3661399248630997
[00:39:16.725] Browser WARN    Inference API failed, using simulation 400
[00:39:16.725] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3584186689581869
[00:39:16.725] Browser WARN    Inference API failed, using simulation 400
[00:39:16.725] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.10814007020020883
[00:39:17.079] Server  ERROR   Backend returned 400
[00:39:17.114] Server  ERROR   Backend returned 400
[00:39:17.117] Server  ERROR   Backend returned 400
[00:39:17.228] Browser WARN    Inference API failed, using simulation 400
[00:39:17.228] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7169488561820669
[00:39:17.228] Browser WARN    Inference API failed, using simulation 400
[00:39:17.228] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3887071688318813
[00:39:17.228] Browser WARN    Inference API failed, using simulation 400
[00:39:17.228] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.03729933144807318
[00:39:17.580] Server  ERROR   Backend returned 400
[00:39:17.619] Server  ERROR   Backend returned 400
[00:39:17.621] Server  ERROR   Backend returned 400
[00:39:17.739] Browser WARN    Inference API failed, using simulation 400
[00:39:17.739] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4999034295384057
[00:39:17.739] Browser WARN    Inference API failed, using simulation 400
[00:39:17.739] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8272181563211176
[00:39:17.739] Browser WARN    Inference API failed, using simulation 400
[00:39:17.739] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.28712782710588985
[00:39:18.081] Server  ERROR   Backend returned 400
[00:39:18.098] Server  ERROR   Backend returned 400
[00:39:18.111] Server  ERROR   Backend returned 400
[00:39:18.216] Browser WARN    Inference API failed, using simulation 400
[00:39:18.216] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2710165605086602
[00:39:18.216] Browser WARN    Inference API failed, using simulation 400
[00:39:18.216] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.23006391058248032
[00:39:18.216] Browser WARN    Inference API failed, using simulation 400
[00:39:18.216] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.48621599291229145
[00:39:18.581] Server  ERROR   Backend returned 400
[00:39:18.600] Server  ERROR   Backend returned 400
[00:39:18.617] Server  ERROR   Backend returned 400
[00:39:18.721] Browser WARN    Inference API failed, using simulation 400
[00:39:18.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.11153899844847459
[00:39:18.721] Browser WARN    Inference API failed, using simulation 400
[00:39:18.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.08123623304326105
[00:39:18.721] Browser WARN    Inference API failed, using simulation 400
[00:39:18.721] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7410758579023381
[00:39:19.078] Server  ERROR   Backend returned 400
[00:39:19.099] Server  ERROR   Backend returned 400
[00:39:19.119] Server  ERROR   Backend returned 400
[00:39:19.229] Browser WARN    Inference API failed, using simulation 400
[00:39:19.229] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7266549932069934
[00:39:19.229] Browser WARN    Inference API failed, using simulation 400
[00:39:19.229] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9566074038745539
[00:39:19.229] Browser WARN    Inference API failed, using simulation 400
[00:39:19.229] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.32886645797367176
[00:39:19.584] Server  ERROR   Backend returned 400
[00:39:19.603] Server  ERROR   Backend returned 400
[00:39:19.615] Server  ERROR   Backend returned 400
[00:39:19.721] Browser WARN    Inference API failed, using simulation 400
[00:39:19.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.14416809666179908
[00:39:19.721] Browser WARN    Inference API failed, using simulation 400
[00:39:19.721] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9067273848326334
[00:39:19.721] Browser WARN    Inference API failed, using simulation 400
[00:39:19.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4399014156291785
[00:39:20.078] Server  ERROR   Backend returned 400
[00:39:20.106] Server  ERROR   Backend returned 400
[00:39:20.109] Server  ERROR   Backend returned 400
[00:39:20.222] Browser WARN    Inference API failed, using simulation 400
[00:39:20.222] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.36087377001277415
[00:39:20.222] Browser WARN    Inference API failed, using simulation 400
[00:39:20.222] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2336443733783729
[00:39:20.222] Browser WARN    Inference API failed, using simulation 400
[00:39:20.222] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.45247722927387246
[00:39:20.579] Server  ERROR   Backend returned 400
[00:39:20.627] Server  ERROR   Backend returned 400
[00:39:20.632] Server  ERROR   Backend returned 400
[00:39:20.741] Browser WARN    Inference API failed, using simulation 400
[00:39:20.741] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8896251000742195
[00:39:20.741] Browser WARN    Inference API failed, using simulation 400
[00:39:20.741] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7038543970905347
[00:39:20.741] Browser WARN    Inference API failed, using simulation 400
[00:39:20.741] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.401928334262434
[00:39:21.085] Server  ERROR   Backend returned 400
[00:39:21.103] Server  ERROR   Backend returned 400
[00:39:21.116] Server  ERROR   Backend returned 400
[00:39:21.222] Browser WARN    Inference API failed, using simulation 400
[00:39:21.222] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.00680612944512532
[00:39:21.222] Browser WARN    Inference API failed, using simulation 400
[00:39:21.222] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1795515165133748
[00:39:21.222] Browser WARN    Inference API failed, using simulation 400
[00:39:21.222] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2046786399579984
[00:39:21.577] Server  ERROR   Backend returned 400
[00:39:21.603] Server  ERROR   Backend returned 400
[00:39:21.605] Server  ERROR   Backend returned 400
[00:39:21.717] Browser WARN    Inference API failed, using simulation 400
[00:39:21.717] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3314730244569211
[00:39:21.717] Browser WARN    Inference API failed, using simulation 400
[00:39:21.717] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.42414037191648274
[00:39:21.717] Browser WARN    Inference API failed, using simulation 400
[00:39:21.717] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3015499299286218
[00:39:22.080] Server  ERROR   Backend returned 400
[00:39:22.099] Server  ERROR   Backend returned 400
[00:39:22.114] Server  ERROR   Backend returned 400
[00:39:22.219] Browser WARN    Inference API failed, using simulation 400
[00:39:22.219] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.010725958138282976
[00:39:22.219] Browser WARN    Inference API failed, using simulation 400
[00:39:22.219] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8035423568264516
[00:39:22.219] Browser WARN    Inference API failed, using simulation 400
[00:39:22.219] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2504531706784498
[00:39:22.593] Server  ERROR   Backend returned 400
[00:39:22.615] Server  ERROR   Backend returned 400
[00:39:22.628] Server  ERROR   Backend returned 400
[00:39:22.732] Browser WARN    Inference API failed, using simulation 400
[00:39:22.732] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1720125161724218
[00:39:22.732] Browser WARN    Inference API failed, using simulation 400
[00:39:22.732] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.14071337553902502
[00:39:22.732] Browser WARN    Inference API failed, using simulation 400
[00:39:22.732] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.40401747563021473
[00:39:23.082] Server  ERROR   Backend returned 400
[00:39:23.103] Server  ERROR   Backend returned 400
[00:39:23.117] Server  ERROR   Backend returned 400
[00:39:23.222] Browser WARN    Inference API failed, using simulation 400
[00:39:23.222] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9635923270687773
[00:39:23.222] Browser WARN    Inference API failed, using simulation 400
[00:39:23.222] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.46898399576950256
[00:39:23.222] Browser WARN    Inference API failed, using simulation 400
[00:39:23.222] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.26181045454737645
[00:39:23.582] Server  ERROR   Backend returned 400
[00:39:23.600] Server  ERROR   Backend returned 400
[00:39:23.622] Server  ERROR   Backend returned 400
[00:39:23.739] Browser WARN    Inference API failed, using simulation 400
[00:39:23.739] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8144200708626732
[00:39:23.739] Browser WARN    Inference API failed, using simulation 400
[00:39:23.739] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7405102268586794
[00:39:23.739] Browser WARN    Inference API failed, using simulation 400
[00:39:23.739] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4661618353437267
[00:39:24.078] Server  ERROR   Backend returned 400
[00:39:24.108] Server  ERROR   Backend returned 400
[00:39:24.121] Server  ERROR   Backend returned 400
[00:39:24.224] Browser WARN    Inference API failed, using simulation 400
[00:39:24.224] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.21140958284664907
[00:39:24.224] Browser WARN    Inference API failed, using simulation 400
[00:39:24.224] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9898865251593241
[00:39:24.224] Browser WARN    Inference API failed, using simulation 400
[00:39:24.224] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.03467355461873339
[00:39:24.578] Server  ERROR   Backend returned 400
[00:39:24.598] Server  ERROR   Backend returned 400
[00:39:24.612] Server  ERROR   Backend returned 400
[00:39:24.716] Browser WARN    Inference API failed, using simulation 400
[00:39:24.716] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.41722107890833354
[00:39:24.716] Browser WARN    Inference API failed, using simulation 400
[00:39:24.717] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8088886381788117
[00:39:24.717] Browser WARN    Inference API failed, using simulation 400
[00:39:24.717] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3901721794941012
[00:39:25.089] Server  ERROR   Backend returned 400
[00:39:25.124] Server  ERROR   Backend returned 400
[00:39:25.139] Server  ERROR   Backend returned 400
[00:39:25.244] Browser WARN    Inference API failed, using simulation 400
[00:39:25.244] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.24931872318083836
[00:39:25.244] Browser WARN    Inference API failed, using simulation 400
[00:39:25.244] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2055860328558447
[00:39:25.244] Browser WARN    Inference API failed, using simulation 400
[00:39:25.244] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.48517803025028844
[00:39:25.584] Server  ERROR   Backend returned 400
[00:39:25.629] Server  ERROR   Backend returned 400
[00:39:25.633] Server  ERROR   Backend returned 400
[00:39:25.743] Browser WARN    Inference API failed, using simulation 400
[00:39:25.743] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7659928188817092
[00:39:25.743] Browser WARN    Inference API failed, using simulation 400
[00:39:25.743] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.24429724132271796
[00:39:25.743] Browser WARN    Inference API failed, using simulation 400
[00:39:25.743] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9181896480325794
[00:39:26.078] Server  ERROR   Backend returned 400
[00:39:26.102] Server  ERROR   Backend returned 400
[00:39:26.121] Server  ERROR   Backend returned 400
[00:39:26.224] Browser WARN    Inference API failed, using simulation 400
[00:39:26.224] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3063315227842977
[00:39:26.224] Browser WARN    Inference API failed, using simulation 400
[00:39:26.224] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.42181698397740675
[00:39:26.224] Browser WARN    Inference API failed, using simulation 400
[00:39:26.224] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.38501434208521695
[00:39:26.580] Server  ERROR   Backend returned 400
[00:39:26.596] Server  ERROR   Backend returned 400
[00:39:26.619] Server  ERROR   Backend returned 400
[00:39:26.722] Browser WARN    Inference API failed, using simulation 400
[00:39:26.722] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.27600304364769723
[00:39:26.722] Browser WARN    Inference API failed, using simulation 400
[00:39:26.722] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3037615849291936
[00:39:26.722] Browser WARN    Inference API failed, using simulation 400
[00:39:26.722] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.40610466298944525
[00:39:27.084] Server  ERROR   Backend returned 400
[00:39:27.104] Server  ERROR   Backend returned 400
[00:39:27.126] Server  ERROR   Backend returned 400
[00:39:27.232] Browser WARN    Inference API failed, using simulation 400
[00:39:27.232] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.42659856466722423
[00:39:27.232] Browser WARN    Inference API failed, using simulation 400
[00:39:27.232] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7678409722129713
[00:39:27.232] Browser WARN    Inference API failed, using simulation 400
[00:39:27.232] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8720292909413941
[00:39:27.630] Server  ERROR   Backend returned 400
[00:39:27.654] Server  ERROR   Backend returned 400
[00:39:27.674] Server  ERROR   Backend returned 400
[00:39:27.787] Browser WARN    Inference API failed, using simulation 400
[00:39:27.787] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.27005461215320775
[00:39:27.787] Browser WARN    Inference API failed, using simulation 400
[00:39:27.787] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8203348485867544
[00:39:27.787] Browser WARN    Inference API failed, using simulation 400
[00:39:27.787] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2404006789854239
[00:39:28.085] Server  ERROR   Backend returned 400
[00:39:28.108] Server  ERROR   Backend returned 400
[00:39:28.133] Server  ERROR   Backend returned 400
[00:39:28.238] Browser WARN    Inference API failed, using simulation 400
[00:39:28.238] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.25876536186250126
[00:39:28.238] Browser WARN    Inference API failed, using simulation 400
[00:39:28.238] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.15269381906412305
[00:39:28.238] Browser WARN    Inference API failed, using simulation 400
[00:39:28.238] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8162327270725893
[00:39:28.610] Server  ERROR   Backend returned 400
[00:39:28.624] Server  ERROR   Backend returned 400
[00:39:28.641] Server  ERROR   Backend returned 400
[00:39:28.754] Browser WARN    Inference API failed, using simulation 400
[00:39:28.754] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.15095927171890133
[00:39:28.754] Browser WARN    Inference API failed, using simulation 400
[00:39:28.754] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4635162843291225
[00:39:28.754] Browser WARN    Inference API failed, using simulation 400
[00:39:28.754] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1369234920363488
[00:39:29.099] Server  ERROR   Backend returned 400
[00:39:29.122] Server  ERROR   Backend returned 400
[00:39:29.128] Server  ERROR   Backend returned 400
[00:39:29.235] Browser WARN    Inference API failed, using simulation 400
[00:39:29.235] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.744577693566794
[00:39:29.235] Browser WARN    Inference API failed, using simulation 400
[00:39:29.235] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.10747691356737271
[00:39:29.235] Browser WARN    Inference API failed, using simulation 400
[00:39:29.235] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.34927364603520694
[00:39:29.600] Server  ERROR   Backend returned 400
[00:39:29.631] Server  ERROR   Backend returned 400
[00:39:29.634] Server  ERROR   Backend returned 400
[00:39:29.749] Browser WARN    Inference API failed, using simulation 400
[00:39:29.749] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.20711539177569555
[00:39:29.749] Browser WARN    Inference API failed, using simulation 400
[00:39:29.749] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9395361229262609
[00:39:29.749] Browser WARN    Inference API failed, using simulation 400
[00:39:29.749] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.04334097926058028
[00:39:30.081] Server  ERROR   Backend returned 400
[00:39:30.108] Server  ERROR   Backend returned 400
[00:39:30.131] Server  ERROR   Backend returned 400
[00:39:30.244] Browser WARN    Inference API failed, using simulation 400
[00:39:30.244] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3707480867759355
[00:39:30.244] Browser WARN    Inference API failed, using simulation 400
[00:39:30.244] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.06926598957972352
[00:39:30.244] Browser WARN    Inference API failed, using simulation 400
[00:39:30.244] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4990412927756965
[00:39:30.591] Server  ERROR   Backend returned 400
[00:39:30.615] Server  ERROR   Backend returned 400
[00:39:30.633] Server  ERROR   Backend returned 400
[00:39:30.742] Browser WARN    Inference API failed, using simulation 400
[00:39:30.742] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1897455806441642
[00:39:30.742] Browser WARN    Inference API failed, using simulation 400
[00:39:30.742] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8329639106502154
[00:39:30.742] Browser WARN    Inference API failed, using simulation 400
[00:39:30.742] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.49340019608236335
[00:39:31.079] Server  ERROR   Backend returned 400
[00:39:31.107] Server  ERROR   Backend returned 400
[00:39:31.119] Server  ERROR   Backend returned 400
[00:39:31.234] Browser WARN    Inference API failed, using simulation 400
[00:39:31.234] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.10398166969828343
[00:39:31.234] Browser WARN    Inference API failed, using simulation 400
[00:39:31.234] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2645299548506331
[00:39:31.234] Browser WARN    Inference API failed, using simulation 400
[00:39:31.234] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4673740044487373
[00:39:31.580] Server  ERROR   Backend returned 400
[00:39:31.608] Server  ERROR   Backend returned 400
[00:39:31.619] Server  ERROR   Backend returned 400
[00:39:31.730] Browser WARN    Inference API failed, using simulation 400
[00:39:31.730] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.06107254763509906
[00:39:31.730] Browser WARN    Inference API failed, using simulation 400
[00:39:31.730] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4881803061775314
[00:39:31.730] Browser WARN    Inference API failed, using simulation 400
[00:39:31.730] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.15113664522966075
[00:39:32.085] Server  ERROR   Backend returned 400
[00:39:32.107] Server  ERROR   Backend returned 400
[00:39:32.121] Server  ERROR   Backend returned 400
[00:39:32.229] Browser WARN    Inference API failed, using simulation 400
[00:39:32.229] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.16518185312910588
[00:39:32.229] Browser WARN    Inference API failed, using simulation 400
[00:39:32.229] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2398989714775539
[00:39:32.229] Browser WARN    Inference API failed, using simulation 400
[00:39:32.229] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.366888535360471
[00:39:32.587] Server  ERROR   Backend returned 400
[00:39:32.606] Server  ERROR   Backend returned 400
[00:39:32.640] Server  ERROR   Backend returned 400
[00:39:32.757] Browser WARN    Inference API failed, using simulation 400
[00:39:32.757] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.20629465198221597
[00:39:32.757] Browser WARN    Inference API failed, using simulation 400
[00:39:32.757] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.15167924611868555
[00:39:32.757] Browser WARN    Inference API failed, using simulation 400
[00:39:32.757] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8504092700613557
[00:39:33.091] Server  ERROR   Backend returned 400
[00:39:33.114] Server  ERROR   Backend returned 400
[00:39:33.143] Server  ERROR   Backend returned 400
[00:39:33.258] Browser WARN    Inference API failed, using simulation 400
[00:39:33.258] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8723206169843031
[00:39:33.258] Browser WARN    Inference API failed, using simulation 400
[00:39:33.258] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2872181111746673
[00:39:33.258] Browser WARN    Inference API failed, using simulation 400
[00:39:33.258] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7507315260687404
[00:39:33.605] Server  ERROR   Backend returned 400
[00:39:33.644] Server  ERROR   Backend returned 400
[00:39:33.650] Server  ERROR   Backend returned 400
[00:39:33.770] Browser WARN    Inference API failed, using simulation 400
[00:39:33.770] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.720274426122196
[00:39:33.770] Browser WARN    Inference API failed, using simulation 400
[00:39:33.770] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7698352388649619
[00:39:33.770] Browser WARN    Inference API failed, using simulation 400
[00:39:33.771] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9126371376824787
[00:39:34.103] Server  ERROR   Backend returned 400
[00:39:34.122] Server  ERROR   Backend returned 400
[00:39:34.136] Server  ERROR   Backend returned 400
[00:39:34.250] Browser WARN    Inference API failed, using simulation 400
[00:39:34.250] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.17566673130073185
[00:39:34.250] Browser WARN    Inference API failed, using simulation 400
[00:39:34.250] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.18567092386992762
[00:39:34.250] Browser WARN    Inference API failed, using simulation 400
[00:39:34.250] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.19900141324544263
[00:39:34.603] Server  ERROR   Backend returned 400
[00:39:34.621] Server  ERROR   Backend returned 400
[00:39:34.637] Server  ERROR   Backend returned 400
[00:39:34.749] Browser WARN    Inference API failed, using simulation 400
[00:39:34.749] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.47253964781975716
[00:39:34.749] Browser WARN    Inference API failed, using simulation 400
[00:39:34.749] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.06723519714796594
[00:39:34.749] Browser WARN    Inference API failed, using simulation 400
[00:39:34.749] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.842302866929828
[00:39:35.098] Server  ERROR   Backend returned 400
[00:39:35.123] Server  ERROR   Backend returned 400
[00:39:35.133] Server  ERROR   Backend returned 400
[00:39:35.246] Browser WARN    Inference API failed, using simulation 400
[00:39:35.246] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9104904074708345
[00:39:35.246] Browser WARN    Inference API failed, using simulation 400
[00:39:35.246] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8497099619186052
[00:39:35.246] Browser WARN    Inference API failed, using simulation 400
[00:39:35.246] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.863240485587635
[00:39:35.590] Server  ERROR   Backend returned 400
[00:39:35.612] Server  ERROR   Backend returned 400
[00:39:35.647] Server  ERROR   Backend returned 400
[00:39:35.756] Browser WARN    Inference API failed, using simulation 400
[00:39:35.756] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8279536586437948
[00:39:35.756] Browser WARN    Inference API failed, using simulation 400
[00:39:35.756] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.820804352367908
[00:39:35.756] Browser WARN    Inference API failed, using simulation 400
[00:39:35.756] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.015363991565382762
[00:39:36.096] Server  ERROR   Backend returned 400
[00:39:36.126] Server  ERROR   Backend returned 400
[00:39:36.129] Server  ERROR   Backend returned 400
[00:39:36.245] Browser WARN    Inference API failed, using simulation 400
[00:39:36.245] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.22668905962576918
[00:39:36.245] Browser WARN    Inference API failed, using simulation 400
[00:39:36.245] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2578499061725459
[00:39:36.245] Browser WARN    Inference API failed, using simulation 400
[00:39:36.245] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3105639002894667
[00:39:36.592] Server  ERROR   Backend returned 400
[00:39:36.608] Server  ERROR   Backend returned 400
[00:39:36.618] Server  ERROR   Backend returned 400
[00:39:36.727] Browser WARN    Inference API failed, using simulation 400
[00:39:36.727] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.31924813341259234
[00:39:36.727] Browser WARN    Inference API failed, using simulation 400
[00:39:36.727] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.10920994061775452
[00:39:36.727] Browser WARN    Inference API failed, using simulation 400
[00:39:36.727] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.46149829991371905
[00:39:37.085] Server  ERROR   Backend returned 400
[00:39:37.106] Server  ERROR   Backend returned 400
[00:39:37.133] Server  ERROR   Backend returned 400
[00:39:37.239] Browser WARN    Inference API failed, using simulation 400
[00:39:37.239] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3480183916422138
[00:39:37.239] Browser WARN    Inference API failed, using simulation 400
[00:39:37.239] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3914671223355701
[00:39:37.239] Browser WARN    Inference API failed, using simulation 400
[00:39:37.239] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8752134506634905
[00:39:37.607] Server  ERROR   Backend returned 400
[00:39:37.626] Server  ERROR   Backend returned 400
[00:39:37.638] Server  ERROR   Backend returned 400
[00:39:37.751] Browser WARN    Inference API failed, using simulation 400
[00:39:37.752] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7038727335639849
[00:39:37.752] Browser WARN    Inference API failed, using simulation 400
[00:39:37.752] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9905988997510304
[00:39:37.752] Browser WARN    Inference API failed, using simulation 400
[00:39:37.752] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.16580812316441024
[00:39:38.094] Server  ERROR   Backend returned 400
[00:39:38.126] Server  ERROR   Backend returned 400
[00:39:38.128] Server  ERROR   Backend returned 400
[00:39:38.250] Browser WARN    Inference API failed, using simulation 400
[00:39:38.250] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3101442102516434
[00:39:38.250] Browser WARN    Inference API failed, using simulation 400
[00:39:38.250] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.16757262599384426
[00:39:38.250] Browser WARN    Inference API failed, using simulation 400
[00:39:38.250] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4317247835475719
[00:39:38.604] Server  ERROR   Backend returned 400
[00:39:38.623] Server  ERROR   Backend returned 400
[00:39:38.633] Server  ERROR   Backend returned 400
[00:39:38.746] Browser WARN    Inference API failed, using simulation 400
[00:39:38.746] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.10260092888389127
[00:39:38.746] Browser WARN    Inference API failed, using simulation 400
[00:39:38.746] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7406441653695379
[00:39:38.746] Browser WARN    Inference API failed, using simulation 400
[00:39:38.746] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.18695134755299292
[00:39:39.084] Server  ERROR   Backend returned 400
[00:39:39.112] Server  ERROR   Backend returned 400
[00:39:39.114] Server  ERROR   Backend returned 400
[00:39:39.231] Browser WARN    Inference API failed, using simulation 400
[00:39:39.231] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9715761825010538
[00:39:39.231] Browser WARN    Inference API failed, using simulation 400
[00:39:39.231] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.40813847345518467
[00:39:39.231] Browser WARN    Inference API failed, using simulation 400
[00:39:39.231] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8215572495101022
[00:39:39.585] Server  ERROR   Backend returned 400
[00:39:39.606] Server  ERROR   Backend returned 400
[00:39:39.618] Server  ERROR   Backend returned 400
[00:39:39.731] Browser WARN    Inference API failed, using simulation 400
[00:39:39.731] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.09189839247659054
[00:39:39.731] Browser WARN    Inference API failed, using simulation 400
[00:39:39.731] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.0921943510476465
[00:39:39.731] Browser WARN    Inference API failed, using simulation 400
[00:39:39.731] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8686284500715716
[00:39:40.093] Server  ERROR   Backend returned 400
[00:39:40.119] Server  ERROR   Backend returned 400
[00:39:40.130] Server  ERROR   Backend returned 400
[00:39:40.241] Browser WARN    Inference API failed, using simulation 400
[00:39:40.241] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.06431354564810299
[00:39:40.242] Browser WARN    Inference API failed, using simulation 400
[00:39:40.242] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3929326412236448
[00:39:40.242] Browser WARN    Inference API failed, using simulation 400
[00:39:40.242] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.055195599637820725
[00:39:40.584] Server  ERROR   Backend returned 400
[00:39:40.623] Server  ERROR   Backend returned 400
[00:39:40.625] Server  ERROR   Backend returned 400
[00:39:40.738] Browser WARN    Inference API failed, using simulation 400
[00:39:40.738] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7388471914482428
[00:39:40.738] Browser WARN    Inference API failed, using simulation 400
[00:39:40.738] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.19348857821867954
[00:39:40.738] Browser WARN    Inference API failed, using simulation 400
[00:39:40.738] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8421984427654561
[00:39:41.083] Server  ERROR   Backend returned 400
[00:39:41.105] Server  ERROR   Backend returned 400
[00:39:41.143] Server  ERROR   Backend returned 400
[00:39:41.249] Browser WARN    Inference API failed, using simulation 400
[00:39:41.249] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.11764336814146853
[00:39:41.249] Browser WARN    Inference API failed, using simulation 400
[00:39:41.249] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.0846645748654275
[00:39:41.249] Browser WARN    Inference API failed, using simulation 400
[00:39:41.249] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.17407646743291544
[00:39:41.595] Server  ERROR   Backend returned 400
[00:39:41.649] Server  ERROR   Backend returned 400
[00:39:41.661] Server  ERROR   Backend returned 400
[00:39:41.778] Browser WARN    Inference API failed, using simulation 400
[00:39:41.778] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9252902802850438
[00:39:41.778] Browser WARN    Inference API failed, using simulation 400
[00:39:41.778] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.42668095644264614
[00:39:41.778] Browser WARN    Inference API failed, using simulation 400
[00:39:41.778] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2225328826156392
[00:39:42.085] Server  ERROR   Backend returned 400
[00:39:42.118] Server  ERROR   Backend returned 400
[00:39:42.132] Server  ERROR   Backend returned 400
[00:39:42.245] Browser WARN    Inference API failed, using simulation 400
[00:39:42.245] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.22765526551406895
[00:39:42.245] Browser WARN    Inference API failed, using simulation 400
[00:39:42.245] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.43195085799522703
[00:39:42.245] Browser WARN    Inference API failed, using simulation 400
[00:39:42.245] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.003557833453566084
[00:39:42.583] Server  ERROR   Backend returned 400
[00:39:42.612] Server  ERROR   Backend returned 400
[00:39:42.614] Server  ERROR   Backend returned 400
[00:39:42.730] Browser WARN    Inference API failed, using simulation 400
[00:39:42.730] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.968301099539717
[00:39:42.730] Browser WARN    Inference API failed, using simulation 400
[00:39:42.730] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3231429872016134
[00:39:42.730] Browser WARN    Inference API failed, using simulation 400
[00:39:42.730] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9706447353070249
[00:39:43.080] Server  ERROR   Backend returned 400
[00:39:43.116] Server  ERROR   Backend returned 400
[00:39:43.119] Server  ERROR   Backend returned 400
[00:39:43.231] Browser WARN    Inference API failed, using simulation 400
[00:39:43.231] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8403376636749028
[00:39:43.231] Browser WARN    Inference API failed, using simulation 400
[00:39:43.231] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8781811442827607
[00:39:43.231] Browser WARN    Inference API failed, using simulation 400
[00:39:43.231] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.11257135664934692
[00:39:43.584] Server  ERROR   Backend returned 400
[00:39:43.602] Server  ERROR   Backend returned 400
[00:39:43.617] Server  ERROR   Backend returned 400
[00:39:43.728] Browser WARN    Inference API failed, using simulation 400
[00:39:43.728] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7165906726531224
[00:39:43.728] Browser WARN    Inference API failed, using simulation 400
[00:39:43.728] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3758308998642977
[00:39:43.728] Browser WARN    Inference API failed, using simulation 400
[00:39:43.728] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1715296985843326
[00:39:44.099] Server  ERROR   Backend returned 400
[00:39:44.121] Server  ERROR   Backend returned 400
[00:39:44.160] Server  ERROR   Backend returned 400
[00:39:44.275] Browser WARN    Inference API failed, using simulation 400
[00:39:44.275] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.38295918446072885
[00:39:44.275] Browser WARN    Inference API failed, using simulation 400
[00:39:44.275] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9938655049410949
[00:39:44.275] Browser WARN    Inference API failed, using simulation 400
[00:39:44.275] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7873183332409692
[00:39:44.588] Server  ERROR   Backend returned 400
[00:39:44.606] Server  ERROR   Backend returned 400
[00:39:44.631] Server  ERROR   Backend returned 400
[00:39:44.743] Browser WARN    Inference API failed, using simulation 400
[00:39:44.743] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1668715013493925
[00:39:44.743] Browser WARN    Inference API failed, using simulation 400
[00:39:44.743] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4407005987883575
[00:39:44.743] Browser WARN    Inference API failed, using simulation 400
[00:39:44.743] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9371024308498144
[00:39:45.115] Server  ERROR   Backend returned 400
[00:39:45.141] Server  ERROR   Backend returned 400
[00:39:45.155] Server  ERROR   Backend returned 400
[00:39:45.272] Browser WARN    Inference API failed, using simulation 400
[00:39:45.272] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.10224462042973864
[00:39:45.272] Browser WARN    Inference API failed, using simulation 400
[00:39:45.272] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4460398957382936
[00:39:45.272] Browser WARN    Inference API failed, using simulation 400
[00:39:45.272] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7961198804268251
[00:39:45.601] Server  ERROR   Backend returned 400
[00:39:45.628] Server  ERROR   Backend returned 400
[00:39:45.639] Server  ERROR   Backend returned 400
[00:39:45.755] Browser WARN    Inference API failed, using simulation 400
[00:39:45.755] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.49169638784396297
[00:39:45.755] Browser WARN    Inference API failed, using simulation 400
[00:39:45.755] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.20534041475021325
[00:39:45.755] Browser WARN    Inference API failed, using simulation 400
[00:39:45.755] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.07069426386167388
[00:39:46.107] Server  ERROR   Backend returned 400
[00:39:46.120] Server  ERROR   Backend returned 400
[00:39:46.142] Server  ERROR   Backend returned 400
[00:39:46.250] Browser WARN    Inference API failed, using simulation 400
[00:39:46.250] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2180418311611642
[00:39:46.250] Browser WARN    Inference API failed, using simulation 400
[00:39:46.250] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2769115849555058
[00:39:46.250] Browser WARN    Inference API failed, using simulation 400
[00:39:46.250] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.10748491603691507
[00:39:46.582] Server  ERROR   Backend returned 400
[00:39:46.600] Server  ERROR   Backend returned 400
[00:39:46.620] Server  ERROR   Backend returned 400
[00:39:46.730] Browser WARN    Inference API failed, using simulation 400
[00:39:46.730] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9398000403181527
[00:39:46.730] Browser WARN    Inference API failed, using simulation 400
[00:39:46.730] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7170330886098739
[00:39:46.730] Browser WARN    Inference API failed, using simulation 400
[00:39:46.730] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7128401987135583
[00:39:47.096] Server  ERROR   Backend returned 400
[00:39:47.113] Server  ERROR   Backend returned 400
[00:39:47.151] Server  ERROR   Backend returned 400
[00:39:47.256] Browser WARN    Inference API failed, using simulation 400
[00:39:47.256] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.19895986097243806
[00:39:47.256] Browser WARN    Inference API failed, using simulation 400
[00:39:47.256] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7791457171091117
[00:39:47.256] Browser WARN    Inference API failed, using simulation 400
[00:39:47.256] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7211684807727925
[00:39:47.590] Server  ERROR   Backend returned 400
[00:39:47.613] Server  ERROR   Backend returned 400
[00:39:47.626] Server  ERROR   Backend returned 400
[00:39:47.740] Browser WARN    Inference API failed, using simulation 400
[00:39:47.740] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.06929794385861227
[00:39:47.740] Browser WARN    Inference API failed, using simulation 400
[00:39:47.740] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.06526952716101214
[00:39:47.740] Browser WARN    Inference API failed, using simulation 400
[00:39:47.740] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2833664252910145
[00:39:48.083] Server  ERROR   Backend returned 400
[00:39:48.103] Server  ERROR   Backend returned 400
[00:39:48.117] Server  ERROR   Backend returned 400
[00:39:48.224] Browser WARN    Inference API failed, using simulation 400
[00:39:48.224] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3416968173406697
[00:39:48.224] Browser WARN    Inference API failed, using simulation 400
[00:39:48.224] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.09079291058824246
[00:39:48.224] Browser WARN    Inference API failed, using simulation 400
[00:39:48.224] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.12945610473041608
[00:39:48.601] Server  ERROR   Backend returned 400
[00:39:48.628] Server  ERROR   Backend returned 400
[00:39:48.640] Server  ERROR   Backend returned 400
[00:39:48.753] Browser WARN    Inference API failed, using simulation 400
[00:39:48.753] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.09764768110031624
[00:39:48.753] Browser WARN    Inference API failed, using simulation 400
[00:39:48.753] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3679318904420388
[00:39:48.753] Browser WARN    Inference API failed, using simulation 400
[00:39:48.753] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4994963771482837
[00:39:49.097] Server  ERROR   Backend returned 400
[00:39:49.130] Server  ERROR   Backend returned 400
[00:39:49.136] Server  ERROR   Backend returned 400
[00:39:49.250] Browser WARN    Inference API failed, using simulation 400
[00:39:49.250] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8994530305745927
[00:39:49.250] Browser WARN    Inference API failed, using simulation 400
[00:39:49.250] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.35946988826503384
[00:39:49.250] Browser WARN    Inference API failed, using simulation 400
[00:39:49.250] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2453045765575418
[00:39:49.581] Server  ERROR   Backend returned 400
[00:39:49.601] Server  ERROR   Backend returned 400
[00:39:49.626] Server  ERROR   Backend returned 400
[00:39:49.732] Browser WARN    Inference API failed, using simulation 400
[00:39:49.732] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2697811290612413
[00:39:49.732] Browser WARN    Inference API failed, using simulation 400
[00:39:49.733] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.23721731110669947
[00:39:49.733] Browser WARN    Inference API failed, using simulation 400
[00:39:49.733] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.10447183190867954
[00:39:50.078] Server  ERROR   Backend returned 400
[00:39:50.109] Server  ERROR   Backend returned 400
[00:39:50.112] Server  ERROR   Backend returned 400
[00:39:50.229] Browser WARN    Inference API failed, using simulation 400
[00:39:50.229] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.15775515119417044
[00:39:50.229] Browser WARN    Inference API failed, using simulation 400
[00:39:50.229] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8598745085439604
[00:39:50.229] Browser WARN    Inference API failed, using simulation 400
[00:39:50.229] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.20313896481340393
[00:39:50.589] Server  ERROR   Backend returned 400
[00:39:50.614] Server  ERROR   Backend returned 400
[00:39:50.643] Server  ERROR   Backend returned 400
[00:39:50.756] Browser WARN    Inference API failed, using simulation 400
[00:39:50.757] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2753040828000878
[00:39:50.757] Browser WARN    Inference API failed, using simulation 400
[00:39:50.757] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.08570617174370065
[00:39:50.757] Browser WARN    Inference API failed, using simulation 400
[00:39:50.757] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4725995939070926
[00:39:51.093] Server  ERROR   Backend returned 400
[00:39:51.109] Server  ERROR   Backend returned 400
[00:39:51.129] Server  ERROR   Backend returned 400
[00:39:51.244] Browser WARN    Inference API failed, using simulation 400
[00:39:51.244] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.18134777430853988
[00:39:51.244] Browser WARN    Inference API failed, using simulation 400
[00:39:51.244] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9772339298883445
[00:39:51.244] Browser WARN    Inference API failed, using simulation 400
[00:39:51.244] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8568773034665885
[00:39:51.597] Server  ERROR   Backend returned 400
[00:39:51.617] Server  ERROR   Backend returned 400
[00:39:51.630] Server  ERROR   Backend returned 400
[00:39:51.741] Browser WARN    Inference API failed, using simulation 400
[00:39:51.741] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2179672847890204
[00:39:51.741] Browser WARN    Inference API failed, using simulation 400
[00:39:51.741] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1215576792835189
[00:39:51.741] Browser WARN    Inference API failed, using simulation 400
[00:39:51.741] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.009527030604430942
[00:39:52.104] Server  ERROR   Backend returned 400
[00:39:52.128] Server  ERROR   Backend returned 400
[00:39:52.145] Server  ERROR   Backend returned 400
[00:39:52.253] Browser WARN    Inference API failed, using simulation 400
[00:39:52.253] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.022593524791758524
[00:39:52.253] Browser WARN    Inference API failed, using simulation 400
[00:39:52.253] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3039261189810935
[00:39:52.253] Browser WARN    Inference API failed, using simulation 400
[00:39:52.253] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2801742238419586
[00:39:52.615] Server  ERROR   Backend returned 400
[00:39:52.649] Server  ERROR   Backend returned 400
[00:39:52.669] Server  ERROR   Backend returned 400
[00:39:52.785] Browser WARN    Inference API failed, using simulation 400
[00:39:52.785] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8707271998625923
[00:39:52.785] Browser WARN    Inference API failed, using simulation 400
[00:39:52.785] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9731448810228063
[00:39:52.785] Browser WARN    Inference API failed, using simulation 400
[00:39:52.785] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.30210043198721925
[00:39:53.091] Server  ERROR   Backend returned 400
[00:39:53.122] Server  ERROR   Backend returned 400
[00:39:53.145] Server  ERROR   Backend returned 400
[00:39:53.256] Browser WARN    Inference API failed, using simulation 400
[00:39:53.257] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.21167791091570742
[00:39:53.257] Browser WARN    Inference API failed, using simulation 400
[00:39:53.257] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4929010926790747
[00:39:53.257] Browser WARN    Inference API failed, using simulation 400
[00:39:53.257] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3729198705731761
[00:39:53.585] Server  ERROR   Backend returned 400
[00:39:53.609] Server  ERROR   Backend returned 400
[00:39:53.623] Server  ERROR   Backend returned 400
[00:39:53.737] Browser WARN    Inference API failed, using simulation 400
[00:39:53.737] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8417217720215401
[00:39:53.737] Browser WARN    Inference API failed, using simulation 400
[00:39:53.737] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.01956490218101753
[00:39:53.737] Browser WARN    Inference API failed, using simulation 400
[00:39:53.737] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3455466995233481
[00:39:54.083] Server  ERROR   Backend returned 400
[00:39:54.119] Server  ERROR   Backend returned 400
[00:39:54.121] Server  ERROR   Backend returned 400
[00:39:54.233] Browser WARN    Inference API failed, using simulation 400
[00:39:54.233] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.311342689081144
[00:39:54.233] Browser WARN    Inference API failed, using simulation 400
[00:39:54.233] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1314320328620574
[00:39:54.233] Browser WARN    Inference API failed, using simulation 400
[00:39:54.233] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.07713963363619886
[00:39:54.579] Server  ERROR   Backend returned 400
[00:39:54.601] Server  ERROR   Backend returned 400
[00:39:54.615] Server  ERROR   Backend returned 400
[00:39:54.731] Browser WARN    Inference API failed, using simulation 400
[00:39:54.731] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9394313473703554
[00:39:54.731] Browser WARN    Inference API failed, using simulation 400
[00:39:54.731] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.41539385416753405
[00:39:54.731] Browser WARN    Inference API failed, using simulation 400
[00:39:54.731] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2168264908476602
[00:39:55.092] Server  ERROR   Backend returned 400
[00:39:55.113] Server  ERROR   Backend returned 400
[00:39:55.128] Server  ERROR   Backend returned 400
[00:39:55.242] Browser WARN    Inference API failed, using simulation 400
[00:39:55.242] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.10676544367160329
[00:39:55.242] Browser WARN    Inference API failed, using simulation 400
[00:39:55.242] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.10500226629235943
[00:39:55.242] Browser WARN    Inference API failed, using simulation 400
[00:39:55.242] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8003715701648393
[00:39:55.592] Server  ERROR   Backend returned 400
[00:39:55.605] Server  ERROR   Backend returned 400
[00:39:55.638] Server  ERROR   Backend returned 400
[00:39:55.755] Browser WARN    Inference API failed, using simulation 400
[00:39:55.755] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.23983492701300269
[00:39:55.755] Browser WARN    Inference API failed, using simulation 400
[00:39:55.755] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4777733220585652
[00:39:55.755] Browser WARN    Inference API failed, using simulation 400
[00:39:55.755] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9234597501091937
[00:39:56.092] Server  ERROR   Backend returned 400
[00:39:56.127] Server  ERROR   Backend returned 400
[00:39:56.161] Server  ERROR   Backend returned 400
[00:39:56.273] Browser WARN    Inference API failed, using simulation 400
[00:39:56.273] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.74363488581094
[00:39:56.273] Browser WARN    Inference API failed, using simulation 400
[00:39:56.273] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3939489156590868
[00:39:56.273] Browser WARN    Inference API failed, using simulation 400
[00:39:56.273] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7151025923817532
[00:39:56.606] Server  ERROR   Backend returned 400
[00:39:56.632] Server  ERROR   Backend returned 400
[00:39:56.644] Server  ERROR   Backend returned 400
[00:39:56.754] Browser WARN    Inference API failed, using simulation 400
[00:39:56.754] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3119713146117936
[00:39:56.754] Browser WARN    Inference API failed, using simulation 400
[00:39:56.754] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.14870318431322088
[00:39:56.754] Browser WARN    Inference API failed, using simulation 400
[00:39:56.754] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.10301187823341762
[00:39:57.102] Server  ERROR   Backend returned 400
[00:39:57.132] Server  ERROR   Backend returned 400
[00:39:57.145] Server  ERROR   Backend returned 400
[00:39:57.252] Browser WARN    Inference API failed, using simulation 400
[00:39:57.252] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9742183374888111
[00:39:57.252] Browser WARN    Inference API failed, using simulation 400
[00:39:57.252] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.03356372696202803
[00:39:57.252] Browser WARN    Inference API failed, using simulation 400
[00:39:57.252] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.45604633986646365
[00:39:57.600] Server  ERROR   Backend returned 400
[00:39:57.626] Server  ERROR   Backend returned 400
[00:39:57.636] Server  ERROR   Backend returned 400
[00:39:57.751] Browser WARN    Inference API failed, using simulation 400
[00:39:57.751] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.27516006172430435
[00:39:57.751] Browser WARN    Inference API failed, using simulation 400
[00:39:57.751] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3225806662186642
[00:39:57.751] Browser WARN    Inference API failed, using simulation 400
[00:39:57.751] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.841365268184984
[00:39:58.097] Server  ERROR   Backend returned 400
[00:39:58.113] Server  ERROR   Backend returned 400
[00:39:58.123] Server  ERROR   Backend returned 400
[00:39:58.231] Browser WARN    Inference API failed, using simulation 400
[00:39:58.231] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4889426945871546
[00:39:58.231] Browser WARN    Inference API failed, using simulation 400
[00:39:58.231] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.03757890524496965
[00:39:58.231] Browser WARN    Inference API failed, using simulation 400
[00:39:58.231] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.08648771903937724
[00:39:58.596] Server  ERROR   Backend returned 400
[00:39:58.624] Server  ERROR   Backend returned 400
[00:39:58.645] Server  ERROR   Backend returned 400
[00:39:58.759] Browser WARN    Inference API failed, using simulation 400
[00:39:58.759] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9916026669514605
[00:39:58.759] Browser WARN    Inference API failed, using simulation 400
[00:39:58.759] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.78669744188536
[00:39:58.759] Browser WARN    Inference API failed, using simulation 400
[00:39:58.759] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.0339041314783608
[00:39:59.099] Server  ERROR   Backend returned 400
[00:39:59.118] Server  ERROR   Backend returned 400
[00:39:59.130] Server  ERROR   Backend returned 400
[00:39:59.241] Browser WARN    Inference API failed, using simulation 400
[00:39:59.241] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1697587695759234
[00:39:59.241] Browser WARN    Inference API failed, using simulation 400
[00:39:59.241] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.924329991028723
[00:39:59.241] Browser WARN    Inference API failed, using simulation 400
[00:39:59.241] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9356785538955064
[00:39:59.612] Server  ERROR   Backend returned 400
[00:39:59.633] Server  ERROR   Backend returned 400
[00:39:59.658] Server  ERROR   Backend returned 400
[00:39:59.769] Browser WARN    Inference API failed, using simulation 400
[00:39:59.769] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2587974746935053
[00:39:59.769] Browser WARN    Inference API failed, using simulation 400
[00:39:59.769] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.700383572128788
[00:39:59.769] Browser WARN    Inference API failed, using simulation 400
[00:39:59.769] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4928014463240662
[00:40:00.102] Server  ERROR   Backend returned 400
[00:40:00.142] Server  ERROR   Backend returned 400
[00:40:00.157] Server  ERROR   Backend returned 400
[00:40:00.266] Browser WARN    Inference API failed, using simulation 400
[00:40:00.266] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.04999535962105678
[00:40:00.266] Browser WARN    Inference API failed, using simulation 400
[00:40:00.266] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8486987411297698
[00:40:00.266] Browser WARN    Inference API failed, using simulation 400
[00:40:00.266] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1262617600788296
[00:40:00.673] Server  ERROR   Backend returned 400
[00:40:00.712] Server  ERROR   Backend returned 400
[00:40:00.741] Server  ERROR   Backend returned 400
[00:40:00.857] Browser WARN    Inference API failed, using simulation 400
[00:40:00.857] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9092479618973307
[00:40:00.857] Browser WARN    Inference API failed, using simulation 400
[00:40:00.857] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.37168200126974205
[00:40:00.857] Browser WARN    Inference API failed, using simulation 400
[00:40:00.857] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9092542363487297
[00:40:01.079] Server  ERROR   Backend returned 400
[00:40:01.099] Server  ERROR   Backend returned 400
[00:40:01.113] Server  ERROR   Backend returned 400
[00:40:01.229] Browser WARN    Inference API failed, using simulation 400
[00:40:01.229] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.11139410736695621
[00:40:01.229] Browser WARN    Inference API failed, using simulation 400
[00:40:01.229] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3329644989733149
[00:40:01.229] Browser WARN    Inference API failed, using simulation 400
[00:40:01.229] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.010677847581797517
[00:40:01.595] Server  ERROR   Backend returned 400
[00:40:01.614] Server  ERROR   Backend returned 400
[00:40:01.638] Server  ERROR   Backend returned 400
[00:40:01.755] Browser WARN    Inference API failed, using simulation 400
[00:40:01.755] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.26463671772538644
[00:40:01.755] Browser WARN    Inference API failed, using simulation 400
[00:40:01.755] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.035145144424655106
[00:40:01.755] Browser WARN    Inference API failed, using simulation 400
[00:40:01.755] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1673200202118844
[00:40:02.095] Server  ERROR   Backend returned 400
[00:40:02.135] Server  ERROR   Backend returned 400
[00:40:02.171] Server  ERROR   Backend returned 400
[00:40:02.285] Browser WARN    Inference API failed, using simulation 400
[00:40:02.285] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.24203361040962157
[00:40:02.285] Browser WARN    Inference API failed, using simulation 400
[00:40:02.285] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9566002801225453
[00:40:02.285] Browser WARN    Inference API failed, using simulation 400
[00:40:02.285] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3042321141099142
[00:40:02.621] Server  ERROR   Backend returned 400
[00:40:02.646] Server  ERROR   Backend returned 400
[00:40:02.661] Server  ERROR   Backend returned 400
[00:40:02.770] Browser WARN    Inference API failed, using simulation 400
[00:40:02.770] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3880972904131991
[00:40:02.770] Browser WARN    Inference API failed, using simulation 400
[00:40:02.770] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.051337799595390365
[00:40:02.770] Browser WARN    Inference API failed, using simulation 400
[00:40:02.770] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9715256376476986
[00:40:03.096] Server  ERROR   Backend returned 400
[00:40:03.117] Server  ERROR   Backend returned 400
[00:40:03.131] Server  ERROR   Backend returned 400
[00:40:03.238] Browser WARN    Inference API failed, using simulation 400
[00:40:03.238] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.12683970503422537
[00:40:03.238] Browser WARN    Inference API failed, using simulation 400
[00:40:03.238] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.33960715958039656
[00:40:03.238] Browser WARN    Inference API failed, using simulation 400
[00:40:03.238] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7200086117167603
[00:40:03.602] Server  ERROR   Backend returned 400
[00:40:03.618] Server  ERROR   Backend returned 400
[00:40:03.635] Server  ERROR   Backend returned 400
[00:40:03.752] Browser WARN    Inference API failed, using simulation 400
[00:40:03.752] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.11161779088316676
[00:40:03.752] Browser WARN    Inference API failed, using simulation 400
[00:40:03.752] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8907156590256237
[00:40:03.752] Browser WARN    Inference API failed, using simulation 400
[00:40:03.752] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.22243406939382399
[00:40:04.103] Server  ERROR   Backend returned 400
[00:40:04.120] Server  ERROR   Backend returned 400
[00:40:04.138] Server  ERROR   Backend returned 400
[00:40:04.250] Browser WARN    Inference API failed, using simulation 400
[00:40:04.250] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.18040226238039886
[00:40:04.250] Browser WARN    Inference API failed, using simulation 400
[00:40:04.250] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.18229244012193246
[00:40:04.250] Browser WARN    Inference API failed, using simulation 400
[00:40:04.250] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3121864814548997
[00:40:04.600] Server  ERROR   Backend returned 400
[00:40:04.616] Server  ERROR   Backend returned 400
[00:40:04.632] Server  ERROR   Backend returned 400
[00:40:04.749] Browser WARN    Inference API failed, using simulation 400
[00:40:04.749] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9841978593790277
[00:40:04.749] Browser WARN    Inference API failed, using simulation 400
[00:40:04.749] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.13326019980269443
[00:40:04.749] Browser WARN    Inference API failed, using simulation 400
[00:40:04.749] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4833088935883541
[00:40:05.084] Server  ERROR   Backend returned 400
[00:40:05.121] Server  ERROR   Backend returned 400
[00:40:05.134] Server  ERROR   Backend returned 400
[00:40:05.244] Browser WARN    Inference API failed, using simulation 400
[00:40:05.244] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1339241175746685
[00:40:05.244] Browser WARN    Inference API failed, using simulation 400
[00:40:05.244] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.21741080163366472
[00:40:05.244] Browser WARN    Inference API failed, using simulation 400
[00:40:05.244] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.02411010403054109
[00:40:05.607] Server  ERROR   Backend returned 400
[00:40:05.635] Server  ERROR   Backend returned 400
[00:40:05.658] Server  ERROR   Backend returned 400
[00:40:05.775] Browser WARN    Inference API failed, using simulation 400
[00:40:05.775] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.91428739864349
[00:40:05.775] Browser WARN    Inference API failed, using simulation 400
[00:40:05.775] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3417343864663497
[00:40:05.775] Browser WARN    Inference API failed, using simulation 400
[00:40:05.775] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8401453486227253
[00:40:06.088] Server  ERROR   Backend returned 400
[00:40:06.102] Server  ERROR   Backend returned 400
[00:40:06.125] Server  ERROR   Backend returned 400
[00:40:06.243] Browser WARN    Inference API failed, using simulation 400
[00:40:06.243] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.09289214940410828
[00:40:06.243] Browser WARN    Inference API failed, using simulation 400
[00:40:06.243] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.304002071189683
[00:40:06.243] Browser WARN    Inference API failed, using simulation 400
[00:40:06.243] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2805096286090687
[00:40:06.597] Server  ERROR   Backend returned 400
[00:40:06.607] Server  ERROR   Backend returned 400
[00:40:06.640] Server  ERROR   Backend returned 400
[00:40:06.755] Browser WARN    Inference API failed, using simulation 400
[00:40:06.755] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9787940164283554
[00:40:06.755] Browser WARN    Inference API failed, using simulation 400
[00:40:06.755] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3598984563705133
[00:40:06.755] Browser WARN    Inference API failed, using simulation 400
[00:40:06.755] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.39130280802934936
[00:40:07.091] Server  ERROR   Backend returned 400
[00:40:07.107] Server  ERROR   Backend returned 400
[00:40:07.119] Server  ERROR   Backend returned 400
[00:40:07.235] Browser WARN    Inference API failed, using simulation 400
[00:40:07.235] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.42263547654501904
[00:40:07.235] Browser WARN    Inference API failed, using simulation 400
[00:40:07.235] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.17813966166090361
[00:40:07.235] Browser WARN    Inference API failed, using simulation 400
[00:40:07.235] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.44809978188050226
[00:40:07.587] Server  ERROR   Backend returned 400
[00:40:07.607] Server  ERROR   Backend returned 400
[00:40:07.620] Server  ERROR   Backend returned 400
[00:40:07.885] Browser WARN    Inference API failed, using simulation 400
[00:40:07.885] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7338085031050309
[00:40:07.885] Browser WARN    Inference API failed, using simulation 400
[00:40:07.885] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.39170516820634427
[00:40:07.885] Browser WARN    Inference API failed, using simulation 400
[00:40:07.885] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9094224051393689
[00:40:08.101] Server  ERROR   Backend returned 400
[00:40:08.131] Server  ERROR   Backend returned 400
[00:40:08.158] Server  ERROR   Backend returned 400
[00:40:08.266] Browser WARN    Inference API failed, using simulation 400
[00:40:08.266] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8792453102842079
[00:40:08.266] Browser WARN    Inference API failed, using simulation 400
[00:40:08.266] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9399375006121835
[00:40:08.266] Browser WARN    Inference API failed, using simulation 400
[00:40:08.266] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2806197936460353
[00:40:08.601] Server  ERROR   Backend returned 400
[00:40:08.618] Server  ERROR   Backend returned 400
[00:40:08.629] Server  ERROR   Backend returned 400
[00:40:08.746] Browser WARN    Inference API failed, using simulation 400
[00:40:08.746] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.27884446795618884
[00:40:08.746] Browser WARN    Inference API failed, using simulation 400
[00:40:08.746] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.799301983392966
[00:40:08.746] Browser WARN    Inference API failed, using simulation 400
[00:40:08.746] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.06909837918882433
[00:40:09.092] Server  ERROR   Backend returned 400
[00:40:09.109] Server  ERROR   Backend returned 400
[00:40:09.120] Server  ERROR   Backend returned 400
[00:40:09.229] Browser WARN    Inference API failed, using simulation 400
[00:40:09.229] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.41410675029698374
[00:40:09.229] Browser WARN    Inference API failed, using simulation 400
[00:40:09.229] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.40591893836438
[00:40:09.229] Browser WARN    Inference API failed, using simulation 400
[00:40:09.229] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.10904214313220945
[00:40:09.596] Server  ERROR   Backend returned 400
[00:40:09.641] Server  ERROR   Backend returned 400
[00:40:09.643] Server  ERROR   Backend returned 400
[00:40:09.757] Browser WARN    Inference API failed, using simulation 400
[00:40:09.757] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9047558180860653
[00:40:09.757] Browser WARN    Inference API failed, using simulation 400
[00:40:09.757] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7204935674570213
[00:40:09.757] Browser WARN    Inference API failed, using simulation 400
[00:40:09.757] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8346398908107214
[00:40:10.087] Server  ERROR   Backend returned 400
[00:40:10.104] Server  ERROR   Backend returned 400
[00:40:10.130] Server  ERROR   Backend returned 400
[00:40:10.240] Browser WARN    Inference API failed, using simulation 400
[00:40:10.240] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.30550659805778774
[00:40:10.240] Browser WARN    Inference API failed, using simulation 400
[00:40:10.240] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9568272998266609
[00:40:10.240] Browser WARN    Inference API failed, using simulation 400
[00:40:10.240] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2994852429131019
[00:40:10.616] Server  ERROR   Backend returned 400
[00:40:10.636] Server  ERROR   Backend returned 400
[00:40:10.649] Server  ERROR   Backend returned 400
[00:40:10.767] Browser WARN    Inference API failed, using simulation 400
[00:40:10.767] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.07383285036802373
[00:40:10.767] Browser WARN    Inference API failed, using simulation 400
[00:40:10.767] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.944121538580911
[00:40:10.767] Browser WARN    Inference API failed, using simulation 400
[00:40:10.767] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.989961156665135
[00:40:11.095] Server  ERROR   Backend returned 400
[00:40:11.121] Server  ERROR   Backend returned 400
[00:40:11.140] Server  ERROR   Backend returned 400
[00:40:11.248] Browser WARN    Inference API failed, using simulation 400
[00:40:11.248] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.17684499546794902
[00:40:11.248] Browser WARN    Inference API failed, using simulation 400
[00:40:11.248] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7037976942311877
[00:40:11.248] Browser WARN    Inference API failed, using simulation 400
[00:40:11.248] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4578073281091852
[00:40:11.580] Server  ERROR   Backend returned 400
[00:40:11.608] Server  ERROR   Backend returned 400
[00:40:11.622] Server  ERROR   Backend returned 400
[00:40:11.729] Browser WARN    Inference API failed, using simulation 400
[00:40:11.729] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.11867974853378216
[00:40:11.729] Browser WARN    Inference API failed, using simulation 400
[00:40:11.729] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7591531329846573
[00:40:11.729] Browser WARN    Inference API failed, using simulation 400
[00:40:11.729] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8135406034622057
[00:40:12.080] Server  ERROR   Backend returned 400
[00:40:12.098] Server  ERROR   Backend returned 400
[00:40:12.114] Server  ERROR   Backend returned 400
[00:40:12.218] Browser WARN    Inference API failed, using simulation 400
[00:40:12.218] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.004161676463316388
[00:40:12.218] Browser WARN    Inference API failed, using simulation 400
[00:40:12.218] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.421873122510718
[00:40:12.218] Browser WARN    Inference API failed, using simulation 400
[00:40:12.218] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.47574283365596537
[00:40:12.589] Server  ERROR   Backend returned 400
[00:40:12.618] Server  ERROR   Backend returned 400
[00:40:12.622] Server  ERROR   Backend returned 400
[00:40:12.730] Browser WARN    Inference API failed, using simulation 400
[00:40:12.730] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.15039906278533377
[00:40:12.730] Browser WARN    Inference API failed, using simulation 400
[00:40:12.730] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.24362148226687685
[00:40:12.730] Browser WARN    Inference API failed, using simulation 400
[00:40:12.730] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1688522202026118
[00:40:13.087] Server  ERROR   Backend returned 400
[00:40:13.109] Server  ERROR   Backend returned 400
[00:40:13.124] Server  ERROR   Backend returned 400
[00:40:13.230] Browser WARN    Inference API failed, using simulation 400
[00:40:13.230] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1159005184586423
[00:40:13.230] Browser WARN    Inference API failed, using simulation 400
[00:40:13.230] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.018884718983310456
[00:40:13.230] Browser WARN    Inference API failed, using simulation 400
[00:40:13.230] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7650487855460434
[00:40:13.581] Server  ERROR   Backend returned 400
[00:40:13.600] Server  ERROR   Backend returned 400
[00:40:13.614] Server  ERROR   Backend returned 400
[00:40:13.728] Browser WARN    Inference API failed, using simulation 400
[00:40:13.728] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.11788237446802702
[00:40:13.728] Browser WARN    Inference API failed, using simulation 400
[00:40:13.728] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.35559968987313845
[00:40:13.728] Browser WARN    Inference API failed, using simulation 400
[00:40:13.728] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3867671548337468
[00:40:14.081] Server  ERROR   Backend returned 400
[00:40:14.100] Server  ERROR   Backend returned 400
[00:40:14.114] Server  ERROR   Backend returned 400
[00:40:14.218] Browser WARN    Inference API failed, using simulation 400
[00:40:14.218] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.004289341065862373
[00:40:14.218] Browser WARN    Inference API failed, using simulation 400
[00:40:14.218] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8546042503172321
[00:40:14.218] Browser WARN    Inference API failed, using simulation 400
[00:40:14.218] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.770060064798566
[00:40:14.644] Server  ERROR   Backend returned 400
[00:40:14.664] Server  ERROR   Backend returned 400
[00:40:14.681] Server  ERROR   Backend returned 400
[00:40:14.787] Browser WARN    Inference API failed, using simulation 400
[00:40:14.787] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.27546320151496495
[00:40:14.787] Browser WARN    Inference API failed, using simulation 400
[00:40:14.787] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.14653667363711348
[00:40:14.787] Browser WARN    Inference API failed, using simulation 400
[00:40:14.787] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8781359389126934
[00:40:15.085] Server  ERROR   Backend returned 400
[00:40:15.106] Server  ERROR   Backend returned 400
[00:40:15.124] Server  ERROR   Backend returned 400
[00:40:15.235] Browser WARN    Inference API failed, using simulation 400
[00:40:15.235] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.10649600559454403
[00:40:15.235] Browser WARN    Inference API failed, using simulation 400
[00:40:15.235] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.24114991437453243
[00:40:15.235] Browser WARN    Inference API failed, using simulation 400
[00:40:15.235] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4857240514406446
[00:40:15.596] Server  ERROR   Backend returned 400
[00:40:15.621] Server  ERROR   Backend returned 400
[00:40:15.635] Server  ERROR   Backend returned 400
[00:40:15.742] Browser WARN    Inference API failed, using simulation 400
[00:40:15.742] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.18406405477102833
[00:40:15.742] Browser WARN    Inference API failed, using simulation 400
[00:40:15.742] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.26454832649080257
[00:40:15.742] Browser WARN    Inference API failed, using simulation 400
[00:40:15.742] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.06413121750646167
[00:40:16.094] Server  ERROR   Backend returned 400
[00:40:16.113] Server  ERROR   Backend returned 400
[00:40:16.130] Server  ERROR   Backend returned 400
[00:40:16.242] Browser WARN    Inference API failed, using simulation 400
[00:40:16.242] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1769244782976599
[00:40:16.242] Browser WARN    Inference API failed, using simulation 400
[00:40:16.242] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7043414554331048
[00:40:16.242] Browser WARN    Inference API failed, using simulation 400
[00:40:16.242] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2815460839874685
[00:40:16.591] Server  ERROR   Backend returned 400
[00:40:16.621] Server  ERROR   Backend returned 400
[00:40:16.642] Server  ERROR   Backend returned 400
[00:40:16.758] Browser WARN    Inference API failed, using simulation 400
[00:40:16.758] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.03860231343393428
[00:40:16.758] Browser WARN    Inference API failed, using simulation 400
[00:40:16.758] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.33190979464592474
[00:40:16.758] Browser WARN    Inference API failed, using simulation 400
[00:40:16.758] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4291144484633312
[00:40:17.097] Server  ERROR   Backend returned 400
[00:40:17.118] Server  ERROR   Backend returned 400
[00:40:17.131] Server  ERROR   Backend returned 400
[00:40:17.241] Browser WARN    Inference API failed, using simulation 400
[00:40:17.242] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.27280603516774743
[00:40:17.242] Browser WARN    Inference API failed, using simulation 400
[00:40:17.242] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4853314044244108
[00:40:17.242] Browser WARN    Inference API failed, using simulation 400
[00:40:17.242] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.34225873545919683
[00:40:17.596] Server  ERROR   Backend returned 400
[00:40:17.624] Server  ERROR   Backend returned 400
[00:40:17.641] Server  ERROR   Backend returned 400
[00:40:17.758] Browser WARN    Inference API failed, using simulation 400
[00:40:17.758] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3086503475699146
[00:40:17.758] Browser WARN    Inference API failed, using simulation 400
[00:40:17.758] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.15171480646504043
[00:40:17.758] Browser WARN    Inference API failed, using simulation 400
[00:40:17.758] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.27805895758613086
[00:40:18.097] Server  ERROR   Backend returned 400
[00:40:18.118] Server  ERROR   Backend returned 400
[00:40:18.132] Server  ERROR   Backend returned 400
[00:40:18.235] Browser WARN    Inference API failed, using simulation 400
[00:40:18.235] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7445126708268636
[00:40:18.235] Browser WARN    Inference API failed, using simulation 400
[00:40:18.235] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8563004137579887
[00:40:18.235] Browser WARN    Inference API failed, using simulation 400
[00:40:18.235] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4283732464513839
[00:40:18.583] Server  ERROR   Backend returned 400
[00:40:18.609] Server  ERROR   Backend returned 400
[00:40:18.621] Server  ERROR   Backend returned 400
[00:40:18.725] Browser WARN    Inference API failed, using simulation 400
[00:40:18.726] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.20216331836971674
[00:40:18.726] Browser WARN    Inference API failed, using simulation 400
[00:40:18.726] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.34436245090372986
[00:40:18.726] Browser WARN    Inference API failed, using simulation 400
[00:40:18.726] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.22760705507506468
[00:40:19.078] Server  ERROR   Backend returned 400
[00:40:19.091] Server  ERROR   Backend returned 400
[00:40:19.113] Server  ERROR   Backend returned 400
[00:40:19.218] Browser WARN    Inference API failed, using simulation 400
[00:40:19.218] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1101056256948057
[00:40:19.218] Browser WARN    Inference API failed, using simulation 400
[00:40:19.218] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.05933899486883515
[00:40:19.218] Browser WARN    Inference API failed, using simulation 400
[00:40:19.218] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.41187875616112557
[00:40:19.585] Server  ERROR   Backend returned 400
[00:40:19.600] Server  ERROR   Backend returned 400
[00:40:19.627] Server  ERROR   Backend returned 400
[00:40:19.731] Browser WARN    Inference API failed, using simulation 400
[00:40:19.731] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.987441261986274
[00:40:19.731] Browser WARN    Inference API failed, using simulation 400
[00:40:19.731] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8407168334142211
[00:40:19.731] Browser WARN    Inference API failed, using simulation 400
[00:40:19.731] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.29797776767274076
[00:40:20.087] Server  ERROR   Backend returned 400
[00:40:20.105] Server  ERROR   Backend returned 400
[00:40:20.127] Server  ERROR   Backend returned 400
[00:40:20.231] Browser WARN    Inference API failed, using simulation 400
[00:40:20.231] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7465530943059074
[00:40:20.231] Browser WARN    Inference API failed, using simulation 400
[00:40:20.231] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8039724160771514
[00:40:20.231] Browser WARN    Inference API failed, using simulation 400
[00:40:20.231] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.856265060362681
[00:40:20.584] Server  ERROR   Backend returned 400
[00:40:20.604] Server  ERROR   Backend returned 400
[00:40:20.615] Server  ERROR   Backend returned 400
[00:40:20.721] Browser WARN    Inference API failed, using simulation 400
[00:40:20.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.036733912228019416
[00:40:20.721] Browser WARN    Inference API failed, using simulation 400
[00:40:20.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.45504919549597506
[00:40:20.721] Browser WARN    Inference API failed, using simulation 400
[00:40:20.721] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8525186670541796
[00:40:21.084] Server  ERROR   Backend returned 400
[00:40:21.125] Server  ERROR   Backend returned 400
[00:40:21.136] Server  ERROR   Backend returned 400
[00:40:21.247] Browser WARN    Inference API failed, using simulation 400
[00:40:21.247] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.42542854655540563
[00:40:21.247] Browser WARN    Inference API failed, using simulation 400
[00:40:21.247] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4115292193446202
[00:40:21.247] Browser WARN    Inference API failed, using simulation 400
[00:40:21.247] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2216880354755032
[00:40:21.578] Server  ERROR   Backend returned 400
[00:40:21.595] Server  ERROR   Backend returned 400
[00:40:21.607] Server  ERROR   Backend returned 400
[00:40:21.714] Browser WARN    Inference API failed, using simulation 400
[00:40:21.714] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8718670772087289
[00:40:21.714] Browser WARN    Inference API failed, using simulation 400
[00:40:21.714] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8077798739155609
[00:40:21.714] Browser WARN    Inference API failed, using simulation 400
[00:40:21.714] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.053762721883884035
[00:40:22.084] Server  ERROR   Backend returned 400
[00:40:22.102] Server  ERROR   Backend returned 400
[00:40:22.115] Server  ERROR   Backend returned 400
[00:40:22.220] Browser WARN    Inference API failed, using simulation 400
[00:40:22.220] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8581440071626372
[00:40:22.220] Browser WARN    Inference API failed, using simulation 400
[00:40:22.220] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2765838664936293
[00:40:22.220] Browser WARN    Inference API failed, using simulation 400
[00:40:22.220] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.28813042468375943
[00:40:22.578] Server  ERROR   Backend returned 400
[00:40:22.613] Server  ERROR   Backend returned 400
[00:40:22.615] Server  ERROR   Backend returned 400
[00:40:22.728] Browser WARN    Inference API failed, using simulation 400
[00:40:22.728] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4103970341957128
[00:40:22.728] Browser WARN    Inference API failed, using simulation 400
[00:40:22.728] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.136111880892347
[00:40:22.728] Browser WARN    Inference API failed, using simulation 400
[00:40:22.728] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.21363938483688422
[00:40:23.081] Server  ERROR   Backend returned 400
[00:40:23.108] Server  ERROR   Backend returned 400
[00:40:23.121] Server  ERROR   Backend returned 400
[00:40:23.228] Browser WARN    Inference API failed, using simulation 400
[00:40:23.228] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.06309608460718125
[00:40:23.228] Browser WARN    Inference API failed, using simulation 400
[00:40:23.228] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.24393873729997712
[00:40:23.228] Browser WARN    Inference API failed, using simulation 400
[00:40:23.228] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.08421233597847971
[00:40:23.578] Server  ERROR   Backend returned 400
[00:40:23.596] Server  ERROR   Backend returned 400
[00:40:23.614] Server  ERROR   Backend returned 400
[00:40:23.721] Browser WARN    Inference API failed, using simulation 400
[00:40:23.721] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4347197573026277
[00:40:23.721] Browser WARN    Inference API failed, using simulation 400
[00:40:23.721] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.10342487664973249
[00:40:23.721] Browser WARN    Inference API failed, using simulation 400
[00:40:23.721] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4491095503563738
[00:40:24.078] Server  ERROR   Backend returned 400
[00:40:24.113] Server  ERROR   Backend returned 400
[00:40:24.115] Server  ERROR   Backend returned 400
[00:40:24.226] Browser WARN    Inference API failed, using simulation 400
[00:40:24.226] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7003808288053949
[00:40:24.226] Browser WARN    Inference API failed, using simulation 400
[00:40:24.226] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.23058127754579877
[00:40:24.226] Browser WARN    Inference API failed, using simulation 400
[00:40:24.226] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8797604222759291
[00:40:24.583] Server  ERROR   Backend returned 400
[00:40:24.624] Server  ERROR   Backend returned 400
[00:40:24.628] Server  ERROR   Backend returned 400
[00:40:24.737] Browser WARN    Inference API failed, using simulation 400
[00:40:24.737] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.11496880020133182
[00:40:24.737] Browser WARN    Inference API failed, using simulation 400
[00:40:24.737] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7303831845225679
[00:40:24.737] Browser WARN    Inference API failed, using simulation 400
[00:40:24.737] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.49976389746574135
[00:40:25.078] Server  ERROR   Backend returned 400
[00:40:25.112] Server  ERROR   Backend returned 400
[00:40:25.118] Server  ERROR   Backend returned 400
[00:40:25.228] Browser WARN    Inference API failed, using simulation 400
[00:40:25.228] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8717329104346875
[00:40:25.228] Browser WARN    Inference API failed, using simulation 400
[00:40:25.228] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8967684062846086
[00:40:25.228] Browser WARN    Inference API failed, using simulation 400
[00:40:25.228] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9468703497396012
[00:40:26.212] Server  ERROR   Backend returned 400
[00:40:26.260] Server  ERROR   Backend returned 400
[00:40:26.280] Server  ERROR   Backend returned 400
[00:40:27.229] Browser WARN    Inference API failed, using simulation 400
[00:40:27.229] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.256634718316251
[00:40:27.229] Browser WARN    Inference API failed, using simulation 400
[00:40:27.229] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8729657262455033
[00:40:27.229] Browser WARN    Inference API failed, using simulation 400
[00:40:27.229] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3096181930348887
[00:40:27.325] Server  ERROR   Backend returned 400
[00:40:27.348] Server  ERROR   Backend returned 400
[00:40:27.362] Server  ERROR   Backend returned 400
[00:40:28.167] Browser WARN    Inference API failed, using simulation 400
[00:40:28.167] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.39947975046304557
[00:40:28.167] Browser WARN    Inference API failed, using simulation 400
[00:40:28.167] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.09705926185181901
[00:40:28.167] Browser WARN    Inference API failed, using simulation 400
[00:40:28.167] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.27820575640575995
[00:40:28.213] Server  ERROR   Backend returned 400
[00:40:28.248] Server  ERROR   Backend returned 400
[00:40:28.253] Server  ERROR   Backend returned 400
[00:40:29.165] Browser WARN    Inference API failed, using simulation 400
[00:40:29.165] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.35345715956772217
[00:40:29.165] Browser WARN    Inference API failed, using simulation 400
[00:40:29.165] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.14608263021097806
[00:40:29.165] Browser WARN    Inference API failed, using simulation 400
[00:40:29.165] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4206574765248613
[00:40:29.218] Server  ERROR   Backend returned 400
[00:40:29.244] Server  ERROR   Backend returned 400
[00:40:29.246] Server  ERROR   Backend returned 400
[00:40:30.161] Browser WARN    Inference API failed, using simulation 400
[00:40:30.161] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.25889797788719937
[00:40:30.161] Browser WARN    Inference API failed, using simulation 400
[00:40:30.161] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7632635942997582
[00:40:30.161] Browser WARN    Inference API failed, using simulation 400
[00:40:30.161] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7132180481033945
[00:40:30.224] Server  ERROR   Backend returned 400
[00:40:30.245] Server  ERROR   Backend returned 400
[00:40:30.248] Server  ERROR   Backend returned 400
[00:40:31.173] Browser WARN    Inference API failed, using simulation 400
[00:40:31.173] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.251017909814389
[00:40:31.173] Browser WARN    Inference API failed, using simulation 400
[00:40:31.173] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1832068896285456
[00:40:31.173] Browser WARN    Inference API failed, using simulation 400
[00:40:31.173] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.029751740627391998
[00:40:31.213] Server  ERROR   Backend returned 400
[00:40:31.273] Server  ERROR   Backend returned 400
[00:40:31.288] Server  ERROR   Backend returned 400
[00:40:32.167] Browser WARN    Inference API failed, using simulation 400
[00:40:32.167] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.18233490042956652
[00:40:32.167] Browser WARN    Inference API failed, using simulation 400
[00:40:32.167] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7581833301589397
[00:40:32.167] Browser WARN    Inference API failed, using simulation 400
[00:40:32.167] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.83726638493941
[00:40:32.226] Server  ERROR   Backend returned 400
[00:40:32.247] Server  ERROR   Backend returned 400
[00:40:32.259] Server  ERROR   Backend returned 400
[00:40:33.189] Browser WARN    Inference API failed, using simulation 400
[00:40:33.189] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.37739142040650686
[00:40:33.189] Browser WARN    Inference API failed, using simulation 400
[00:40:33.189] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2595064745034198
[00:40:33.189] Browser WARN    Inference API failed, using simulation 400
[00:40:33.189] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.22611806657453998
[00:40:33.220] Server  ERROR   Backend returned 400
[00:40:33.243] Server  ERROR   Backend returned 400
[00:40:33.265] Server  ERROR   Backend returned 400
[00:40:34.174] Browser WARN    Inference API failed, using simulation 400
[00:40:34.174] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.936519072680252
[00:40:34.174] Browser WARN    Inference API failed, using simulation 400
[00:40:34.174] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.258172866595031
[00:40:34.174] Browser WARN    Inference API failed, using simulation 400
[00:40:34.174] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.144565312771126
[00:40:34.226] Server  ERROR   Backend returned 400
[00:40:34.244] Server  ERROR   Backend returned 400
[00:40:34.257] Server  ERROR   Backend returned 400
[00:40:35.173] Browser WARN    Inference API failed, using simulation 400
[00:40:35.174] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.44658991127985365
[00:40:35.174] Browser WARN    Inference API failed, using simulation 400
[00:40:35.174] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7500093043194184
[00:40:35.174] Browser WARN    Inference API failed, using simulation 400
[00:40:35.174] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4237662373537181
[00:40:35.269] Server  ERROR   Backend returned 400
[00:40:35.288] Server  ERROR   Backend returned 400
[00:40:35.305] Server  ERROR   Backend returned 400
[00:40:36.174] Browser WARN    Inference API failed, using simulation 400
[00:40:36.174] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2961611226266422
[00:40:36.174] Browser WARN    Inference API failed, using simulation 400
[00:40:36.174] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.24653823290257815
[00:40:36.174] Browser WARN    Inference API failed, using simulation 400
[00:40:36.174] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.11419723791086156
[00:40:37.250] Server  ERROR   Backend returned 400
[00:40:37.257] Server  ERROR   Backend returned 400
[00:40:37.259] Server  ERROR   Backend returned 400
[00:40:38.173] Browser WARN    Inference API failed, using simulation 400
[00:40:38.173] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.776588861026951
[00:40:38.173] Browser WARN    Inference API failed, using simulation 400
[00:40:38.173] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.0008017299952330359
[00:40:38.173] Browser WARN    Inference API failed, using simulation 400
[00:40:38.173] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.47105377702127493
[00:40:38.222] Server  ERROR   Backend returned 400
[00:40:38.244] Server  ERROR   Backend returned 400
[00:40:38.246] Server  ERROR   Backend returned 400
[00:40:39.167] Browser WARN    Inference API failed, using simulation 400
[00:40:39.167] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8453630244329474
[00:40:39.167] Browser WARN    Inference API failed, using simulation 400
[00:40:39.167] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.34883745210753125
[00:40:39.167] Browser WARN    Inference API failed, using simulation 400
[00:40:39.167] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8967104528407276
[00:40:42.438] Server  ERROR   Backend returned 400
[00:40:42.902] Server  ERROR   Backend returned 400
[00:40:42.948] Server  ERROR   Backend returned 400
[00:40:42.957] Server  ERROR   Backend returned 400
[00:40:42.960] Server  ERROR   Backend returned 400
[00:40:42.967] Server  ERROR   Backend returned 400
[00:40:42.976] Server  ERROR   Backend returned 400
[00:40:43.031] Server  ERROR   Backend returned 400
[00:40:43.063] Server  ERROR   Backend returned 400
[00:40:43.108] Server  ERROR   Backend returned 400
[00:40:43.158] Server  ERROR   Backend returned 400
[00:40:43.161] Server  ERROR   Backend returned 400
[00:40:43.355] Server  ERROR   Backend returned 400
[00:40:43.366] Server  ERROR   Backend returned 400
[00:40:43.369] Server  ERROR   Backend returned 400
[00:40:44.174] Browser WARN    Inference API failed, using simulation 400
[00:40:44.174] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.828245681131422
[00:40:44.175] Browser WARN    Inference API failed, using simulation 400
[00:40:44.175] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.84271307000206
[00:40:44.175] Browser WARN    Inference API failed, using simulation 400
[00:40:44.175] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3915203076048549
[00:40:44.175] Browser WARN    Inference API failed, using simulation 400
[00:40:44.175] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.12428543345376103
[00:40:44.175] Browser WARN    Inference API failed, using simulation 400
[00:40:44.175] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.08592041407135814
[00:40:44.175] Browser WARN    Inference API failed, using simulation 400
[00:40:44.175] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3900280127238301
[00:40:44.175] Browser WARN    Inference API failed, using simulation 400
[00:40:44.175] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8278466978967346
[00:40:44.175] Browser WARN    Inference API failed, using simulation 400
[00:40:44.175] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8516598007877517
[00:40:44.175] Browser WARN    Inference API failed, using simulation 400
[00:40:44.175] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.16482304193700792
[00:40:44.175] Browser WARN    Inference API failed, using simulation 400
[00:40:44.175] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.40910388033465417
[00:40:44.175] Browser WARN    Inference API failed, using simulation 400
[00:40:44.175] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7590496201435813
[00:40:44.175] Browser WARN    Inference API failed, using simulation 400
[00:40:44.175] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4810182888449383
[00:40:44.175] Browser WARN    Inference API failed, using simulation 400
[00:40:44.175] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3603167153663149
[00:40:44.175] Browser WARN    Inference API failed, using simulation 400
[00:40:44.175] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9041252398539898
[00:40:44.175] Browser WARN    Inference API failed, using simulation 400
[00:40:44.175] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9634312540302674
[00:40:44.231] Server  ERROR   Backend returned 400
[00:40:44.241] Server  ERROR   Backend returned 400
[00:40:44.255] Server  ERROR   Backend returned 400
[00:40:45.171] Browser WARN    Inference API failed, using simulation 400
[00:40:45.171] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.746906329242609
[00:40:45.171] Browser WARN    Inference API failed, using simulation 400
[00:40:45.171] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8314052881845948
[00:40:45.171] Browser WARN    Inference API failed, using simulation 400
[00:40:45.171] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.13667541181101206
[00:40:45.229] Server  ERROR   Backend returned 400
[00:40:45.231] Server  ERROR   Backend returned 400
[00:40:45.247] Server  ERROR   Backend returned 400
[00:40:46.171] Browser WARN    Inference API failed, using simulation 400
[00:40:46.171] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7306931924254821
[00:40:46.171] Browser WARN    Inference API failed, using simulation 400
[00:40:46.171] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7687270865814886
[00:40:46.171] Browser WARN    Inference API failed, using simulation 400
[00:40:46.171] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.0286888645744946
[00:40:46.225] Server  ERROR   Backend returned 400
[00:40:46.243] Server  ERROR   Backend returned 400
[00:40:46.246] Server  ERROR   Backend returned 400
[00:40:47.170] Browser WARN    Inference API failed, using simulation 400
[00:40:47.170] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9441230443163845
[00:40:47.170] Browser WARN    Inference API failed, using simulation 400
[00:40:47.170] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.11169280661457948
[00:40:47.170] Browser WARN    Inference API failed, using simulation 400
[00:40:47.170] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.032595863048753115
[00:40:47.232] Server  ERROR   Backend returned 400
[00:40:47.246] Server  ERROR   Backend returned 400
[00:40:47.260] Server  ERROR   Backend returned 400
[00:40:48.167] Browser WARN    Inference API failed, using simulation 400
[00:40:48.167] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4471891972819021
[00:40:48.167] Browser WARN    Inference API failed, using simulation 400
[00:40:48.167] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4989348287840724
[00:40:48.167] Browser WARN    Inference API failed, using simulation 400
[00:40:48.167] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8156292860959379
[00:40:48.503] Server  ERROR   Backend returned 400
[00:40:48.511] Server  ERROR   Backend returned 400
[00:40:48.526] Server  ERROR   Backend returned 400
[00:40:49.189] Browser WARN    Inference API failed, using simulation 400
[00:40:49.189] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3682210612964248
[00:40:49.189] Browser WARN    Inference API failed, using simulation 400
[00:40:49.189] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.055412732670410525
[00:40:49.189] Browser WARN    Inference API failed, using simulation 400
[00:40:49.189] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1262780714204892
[00:40:49.224] Server  ERROR   Backend returned 400
[00:40:49.233] Server  ERROR   Backend returned 400
[00:40:49.468] Server  ERROR   Backend returned 400
[00:40:50.186] Browser WARN    Inference API failed, using simulation 400
[00:40:50.186] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.43066228573174103
[00:40:50.186] Browser WARN    Inference API failed, using simulation 400
[00:40:50.186] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3816618881523423
[00:40:50.186] Browser WARN    Inference API failed, using simulation 400
[00:40:50.186] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3252687722063727
[00:40:50.213] Server  ERROR   Backend returned 400
[00:40:50.230] Server  ERROR   Backend returned 400
[00:40:50.241] Server  ERROR   Backend returned 400
[00:40:51.166] Browser WARN    Inference API failed, using simulation 400
[00:40:51.166] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2921883780603711
[00:40:51.166] Browser WARN    Inference API failed, using simulation 400
[00:40:51.166] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.948334372347989
[00:40:51.166] Browser WARN    Inference API failed, using simulation 400
[00:40:51.166] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.11543825667876273
[00:40:51.244] Server  ERROR   Backend returned 400
[00:40:51.246] Server  ERROR   Backend returned 400
[00:40:51.261] Server  ERROR   Backend returned 400
[00:40:52.167] Browser WARN    Inference API failed, using simulation 400
[00:40:52.167] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8782631715865177
[00:40:52.167] Browser WARN    Inference API failed, using simulation 400
[00:40:52.167] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.0693146841329873
[00:40:52.167] Browser WARN    Inference API failed, using simulation 400
[00:40:52.167] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.09623107016732324
[00:40:52.680] Server  ERROR   Backend returned 400
[00:40:52.682] Server  ERROR   Backend returned 400
[00:40:52.690] Server  ERROR   Backend returned 400
[00:40:53.182] Browser WARN    Inference API failed, using simulation 400
[00:40:53.182] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.014485493049511522
[00:40:53.182] Browser WARN    Inference API failed, using simulation 400
[00:40:53.182] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2343491481703936
[00:40:53.182] Browser WARN    Inference API failed, using simulation 400
[00:40:53.182] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.23980213220792168
[00:40:53.207] Server  ERROR   Backend returned 400
[00:40:53.230] Server  ERROR   Backend returned 400
[00:40:53.245] Server  ERROR   Backend returned 400
[00:40:54.161] Browser WARN    Inference API failed, using simulation 400
[00:40:54.161] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.27768359066368614
[00:40:54.161] Browser WARN    Inference API failed, using simulation 400
[00:40:54.161] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.22433503452524828
[00:40:54.161] Browser WARN    Inference API failed, using simulation 400
[00:40:54.161] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.24971377133845152
[00:40:54.213] Server  ERROR   Backend returned 400
[00:40:54.230] Server  ERROR   Backend returned 400
[00:40:54.243] Server  ERROR   Backend returned 400
[00:40:55.171] Browser WARN    Inference API failed, using simulation 400
[00:40:55.171] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.07156116503659915
[00:40:55.171] Browser WARN    Inference API failed, using simulation 400
[00:40:55.171] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.10212442392667731
[00:40:55.171] Browser WARN    Inference API failed, using simulation 400
[00:40:55.171] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.17459341716317894
[00:40:55.237] Server  ERROR   Backend returned 400
[00:40:55.255] Server  ERROR   Backend returned 400
[00:40:55.258] Server  ERROR   Backend returned 400
[00:40:56.170] Browser WARN    Inference API failed, using simulation 400
[00:40:56.170] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9998554062845807
[00:40:56.170] Browser WARN    Inference API failed, using simulation 400
[00:40:56.170] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.12259744615029161
[00:40:56.170] Browser WARN    Inference API failed, using simulation 400
[00:40:56.170] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.034718970614947
[00:40:56.236] Server  ERROR   Backend returned 400
[00:40:56.239] Server  ERROR   Backend returned 400
[00:40:56.258] Server  ERROR   Backend returned 400
[00:40:57.164] Browser WARN    Inference API failed, using simulation 400
[00:40:57.165] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9159400882617652
[00:40:57.165] Browser WARN    Inference API failed, using simulation 400
[00:40:57.165] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.32958871290180286
[00:40:57.165] Browser WARN    Inference API failed, using simulation 400
[00:40:57.165] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.15926127849512534
[00:40:57.211] Server  ERROR   Backend returned 400
[00:40:57.230] Server  ERROR   Backend returned 400
[00:40:57.241] Server  ERROR   Backend returned 400
[00:40:58.162] Browser WARN    Inference API failed, using simulation 400
[00:40:58.162] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1353881587894652
[00:40:58.162] Browser WARN    Inference API failed, using simulation 400
[00:40:58.162] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9011543888431873
[00:40:58.162] Browser WARN    Inference API failed, using simulation 400
[00:40:58.162] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.32579217686397716
[00:40:58.225] Server  ERROR   Backend returned 400
[00:40:58.235] Server  ERROR   Backend returned 400
[00:40:58.251] Server  ERROR   Backend returned 400
[00:40:59.176] Browser WARN    Inference API failed, using simulation 400
[00:40:59.176] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8514091280710405
[00:40:59.176] Browser WARN    Inference API failed, using simulation 400
[00:40:59.176] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.19392387502190545
[00:40:59.176] Browser WARN    Inference API failed, using simulation 400
[00:40:59.176] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4057275419301765
[00:40:59.231] Server  ERROR   Backend returned 400
[00:40:59.253] Server  ERROR   Backend returned 400
[00:40:59.256] Server  ERROR   Backend returned 400
[00:41:00.162] Browser WARN    Inference API failed, using simulation 400
[00:41:00.162] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.042567958840013076
[00:41:00.162] Browser WARN    Inference API failed, using simulation 400
[00:41:00.162] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.284318900705771
[00:41:00.162] Browser WARN    Inference API failed, using simulation 400
[00:41:00.162] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2474920319896327
[00:41:00.226] Server  ERROR   Backend returned 400
[00:41:00.229] Server  ERROR   Backend returned 400
[00:41:00.245] Server  ERROR   Backend returned 400
[00:41:01.172] Browser WARN    Inference API failed, using simulation 400
[00:41:01.172] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.978790637178778
[00:41:01.172] Browser WARN    Inference API failed, using simulation 400
[00:41:01.172] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.0372443715205778
[00:41:01.172] Browser WARN    Inference API failed, using simulation 400
[00:41:01.172] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.45783506827222664
[00:41:01.231] Server  ERROR   Backend returned 400
[00:41:01.256] Server  ERROR   Backend returned 400
[00:41:01.259] Server  ERROR   Backend returned 400
[00:41:02.175] Browser WARN    Inference API failed, using simulation 400
[00:41:02.175] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3213916942124564
[00:41:02.175] Browser WARN    Inference API failed, using simulation 400
[00:41:02.175] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.174888283209923
[00:41:02.175] Browser WARN    Inference API failed, using simulation 400
[00:41:02.175] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.24206778749595675
[00:41:02.224] Server  ERROR   Backend returned 400
[00:41:02.248] Server  ERROR   Backend returned 400
[00:41:02.252] Server  ERROR   Backend returned 400
[00:41:03.165] Browser WARN    Inference API failed, using simulation 400
[00:41:03.165] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9769326879671358
[00:41:03.165] Browser WARN    Inference API failed, using simulation 400
[00:41:03.165] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4307579400241007
[00:41:03.165] Browser WARN    Inference API failed, using simulation 400
[00:41:03.165] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9030867446222897
[00:41:03.223] Server  ERROR   Backend returned 400
[00:41:03.247] Server  ERROR   Backend returned 400
[00:41:03.260] Server  ERROR   Backend returned 400
[00:41:04.168] Browser WARN    Inference API failed, using simulation 400
[00:41:04.168] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7748449825439029
[00:41:04.168] Browser WARN    Inference API failed, using simulation 400
[00:41:04.168] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7970029874903927
[00:41:04.168] Browser WARN    Inference API failed, using simulation 400
[00:41:04.168] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.01651265454052042
[00:41:04.225] Server  ERROR   Backend returned 400
[00:41:04.227] Server  ERROR   Backend returned 400
[00:41:04.242] Server  ERROR   Backend returned 400
[00:41:05.168] Browser WARN    Inference API failed, using simulation 400
[00:41:05.168] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.48222131663816875
[00:41:05.168] Browser WARN    Inference API failed, using simulation 400
[00:41:05.168] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8755337935913196
[00:41:05.168] Browser WARN    Inference API failed, using simulation 400
[00:41:05.168] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8897828727980988
[00:41:05.236] Server  ERROR   Backend returned 400
[00:41:05.252] Server  ERROR   Backend returned 400
[00:41:05.263] Server  ERROR   Backend returned 400
[00:41:06.166] Browser WARN    Inference API failed, using simulation 400
[00:41:06.166] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.03789413752690951
[00:41:06.166] Browser WARN    Inference API failed, using simulation 400
[00:41:06.166] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2537477076814629
[00:41:06.166] Browser WARN    Inference API failed, using simulation 400
[00:41:06.166] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.19273030789894519
[00:41:06.260] Server  ERROR   Backend returned 400
[00:41:06.292] Server  ERROR   Backend returned 400
[00:41:06.294] Server  ERROR   Backend returned 400
[00:41:07.162] Browser WARN    Inference API failed, using simulation 400
[00:41:07.162] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4725563190609615
[00:41:07.162] Browser WARN    Inference API failed, using simulation 400
[00:41:07.162] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2139996043654579
[00:41:07.162] Browser WARN    Inference API failed, using simulation 400
[00:41:07.162] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1408616354994761
[00:41:07.205] Server  ERROR   Backend returned 400
[00:41:07.223] Server  ERROR   Backend returned 400
[00:41:07.241] Server  ERROR   Backend returned 400
[00:41:08.172] Browser WARN    Inference API failed, using simulation 400
[00:41:08.172] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7126724461278982
[00:41:08.172] Browser WARN    Inference API failed, using simulation 400
[00:41:08.172] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.30422254539710536
[00:41:08.172] Browser WARN    Inference API failed, using simulation 400
[00:41:08.172] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.946948841249435
[00:41:08.239] Server  ERROR   Backend returned 400
[00:41:08.242] Server  ERROR   Backend returned 400
[00:41:08.258] Server  ERROR   Backend returned 400
[00:41:09.165] Browser WARN    Inference API failed, using simulation 400
[00:41:09.165] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.18965659821362035
[00:41:09.165] Browser WARN    Inference API failed, using simulation 400
[00:41:09.165] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.12427703839371534
[00:41:09.165] Browser WARN    Inference API failed, using simulation 400
[00:41:09.165] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2614223489758427
[00:41:09.218] Server  ERROR   Backend returned 400
[00:41:09.243] Server  ERROR   Backend returned 400
[00:41:09.245] Server  ERROR   Backend returned 400
[00:41:10.161] Browser WARN    Inference API failed, using simulation 400
[00:41:10.161] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7478225366030099
[00:41:10.161] Browser WARN    Inference API failed, using simulation 400
[00:41:10.161] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2957292781110276
[00:41:10.161] Browser WARN    Inference API failed, using simulation 400
[00:41:10.161] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.18126569535705078
[00:41:10.226] Server  ERROR   Backend returned 400
[00:41:10.244] Server  ERROR   Backend returned 400
[00:41:10.265] Server  ERROR   Backend returned 400
[00:41:11.170] Browser WARN    Inference API failed, using simulation 400
[00:41:11.170] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.05181666005880531
[00:41:11.170] Browser WARN    Inference API failed, using simulation 400
[00:41:11.170] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.42292490931573296
[00:41:11.170] Browser WARN    Inference API failed, using simulation 400
[00:41:11.170] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.19062613700721626
[00:41:11.240] Server  ERROR   Backend returned 400
[00:41:11.251] Server  ERROR   Backend returned 400
[00:41:11.257] Server  ERROR   Backend returned 400
[00:41:12.163] Browser WARN    Inference API failed, using simulation 400
[00:41:12.163] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3463940688041803
[00:41:12.163] Browser WARN    Inference API failed, using simulation 400
[00:41:12.163] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.17080258948811472
[00:41:12.163] Browser WARN    Inference API failed, using simulation 400
[00:41:12.163] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.08577825243891118
[00:41:12.209] Server  ERROR   Backend returned 400
[00:41:12.243] Server  ERROR   Backend returned 400
[00:41:12.260] Server  ERROR   Backend returned 400
[00:41:13.174] Browser WARN    Inference API failed, using simulation 400
[00:41:13.174] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.18942524889720314
[00:41:13.174] Browser WARN    Inference API failed, using simulation 400
[00:41:13.174] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.12693882437084436
[00:41:13.174] Browser WARN    Inference API failed, using simulation 400
[00:41:13.174] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.19766393668791604
[00:41:13.234] Server  ERROR   Backend returned 400
[00:41:13.236] Server  ERROR   Backend returned 400
[00:41:13.255] Server  ERROR   Backend returned 400
[00:41:14.168] Browser WARN    Inference API failed, using simulation 400
[00:41:14.168] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4433984482948419
[00:41:14.168] Browser WARN    Inference API failed, using simulation 400
[00:41:14.168] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.39518760789067453
[00:41:14.168] Browser WARN    Inference API failed, using simulation 400
[00:41:14.168] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8672937969575242
[00:41:14.206] Server  ERROR   Backend returned 400
[00:41:14.224] Server  ERROR   Backend returned 400
[00:41:14.235] Server  ERROR   Backend returned 400
[00:41:15.163] Browser WARN    Inference API failed, using simulation 400
[00:41:15.163] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9247340155712971
[00:41:15.163] Browser WARN    Inference API failed, using simulation 400
[00:41:15.163] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3642568257212017
[00:41:15.163] Browser WARN    Inference API failed, using simulation 400
[00:41:15.163] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.261229457341241
[00:41:15.223] Server  ERROR   Backend returned 400
[00:41:15.237] Server  ERROR   Backend returned 400
[00:41:15.250] Server  ERROR   Backend returned 400
[00:41:16.176] Browser WARN    Inference API failed, using simulation 400
[00:41:16.176] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8243590652762521
[00:41:16.176] Browser WARN    Inference API failed, using simulation 400
[00:41:16.176] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.45023444611992286
[00:41:16.176] Browser WARN    Inference API failed, using simulation 400
[00:41:16.176] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9056017324762018
[00:41:16.229] Server  ERROR   Backend returned 400
[00:41:16.249] Server  ERROR   Backend returned 400
[00:41:16.252] Server  ERROR   Backend returned 400
[00:41:17.166] Browser WARN    Inference API failed, using simulation 400
[00:41:17.166] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.46352859714153105
[00:41:17.166] Browser WARN    Inference API failed, using simulation 400
[00:41:17.166] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.280974249492318
[00:41:17.166] Browser WARN    Inference API failed, using simulation 400
[00:41:17.166] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.7962022090162814
[00:41:17.240] Server  ERROR   Backend returned 400
[00:41:17.245] Server  ERROR   Backend returned 400
[00:41:17.261] Server  ERROR   Backend returned 400
[00:41:18.165] Browser WARN    Inference API failed, using simulation 400
[00:41:18.165] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.25945750836729303
[00:41:18.165] Browser WARN    Inference API failed, using simulation 400
[00:41:18.165] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.16581026603105448
[00:41:18.165] Browser WARN    Inference API failed, using simulation 400
[00:41:18.165] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.020021864902486086
[00:41:18.202] Server  ERROR   Backend returned 400
[00:41:18.222] Server  ERROR   Backend returned 400
[00:41:18.234] Server  ERROR   Backend returned 400
[00:41:19.175] Browser WARN    Inference API failed, using simulation 400
[00:41:19.175] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.013431677779889795
[00:41:19.175] Browser WARN    Inference API failed, using simulation 400
[00:41:19.175] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1746928247405009
[00:41:19.175] Browser WARN    Inference API failed, using simulation 400
[00:41:19.175] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9887796563360126
[00:41:19.228] Server  ERROR   Backend returned 400
[00:41:19.248] Server  ERROR   Backend returned 400
[00:41:19.261] Server  ERROR   Backend returned 400
[00:41:20.172] Browser WARN    Inference API failed, using simulation 400
[00:41:20.172] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.40061537304775363
[00:41:20.172] Browser WARN    Inference API failed, using simulation 400
[00:41:20.172] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8046580725945258
[00:41:20.172] Browser WARN    Inference API failed, using simulation 400
[00:41:20.172] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.12278252660403904
[00:41:20.223] Server  ERROR   Backend returned 400
[00:41:20.227] Server  ERROR   Backend returned 400
[00:41:20.241] Server  ERROR   Backend returned 400
[00:41:21.170] Browser WARN    Inference API failed, using simulation 400
[00:41:21.170] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.41345331611111685
[00:41:21.170] Browser WARN    Inference API failed, using simulation 400
[00:41:21.170] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7779774960389096
[00:41:21.170] Browser WARN    Inference API failed, using simulation 400
[00:41:21.170] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.0958564471519186
[00:41:21.246] Server  ERROR   Backend returned 400
[00:41:21.248] Server  ERROR   Backend returned 400
[00:41:21.315] Server  ERROR   Backend returned 400
[00:41:22.172] Browser WARN    Inference API failed, using simulation 400
[00:41:22.173] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.011769092661351177
[00:41:22.173] Browser WARN    Inference API failed, using simulation 400
[00:41:22.173] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8731061407835462
[00:41:22.173] Browser WARN    Inference API failed, using simulation 400
[00:41:22.173] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.11152615338761618
[00:41:22.215] Server  ERROR   Backend returned 400
[00:41:22.234] Server  ERROR   Backend returned 400
[00:41:22.250] Server  ERROR   Backend returned 400
[00:41:23.174] Browser WARN    Inference API failed, using simulation 400
[00:41:23.174] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.34727812821033704
[00:41:23.174] Browser WARN    Inference API failed, using simulation 400
[00:41:23.174] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.22170918704596076
[00:41:23.174] Browser WARN    Inference API failed, using simulation 400
[00:41:23.174] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.28277625501411485
[00:41:23.253] Server  ERROR   Backend returned 400
[00:41:23.287] Server  ERROR   Backend returned 400
[00:41:23.299] Server  ERROR   Backend returned 400
[00:41:24.164] Browser WARN    Inference API failed, using simulation 400
[00:41:24.164] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3645117308261376
[00:41:24.164] Browser WARN    Inference API failed, using simulation 400
[00:41:24.164] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3497656619354307
[00:41:24.164] Browser WARN    Inference API failed, using simulation 400
[00:41:24.164] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9504114793376847
[00:41:24.246] Server  ERROR   Backend returned 400
[00:41:24.270] Server  ERROR   Backend returned 400
[00:41:24.281] Server  ERROR   Backend returned 400
[00:41:25.174] Browser WARN    Inference API failed, using simulation 400
[00:41:25.174] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8670717187503412
[00:41:25.174] Browser WARN    Inference API failed, using simulation 400
[00:41:25.174] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.16821703425341006
[00:41:25.174] Browser WARN    Inference API failed, using simulation 400
[00:41:25.174] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.48397175082934796
[00:41:25.249] Server  ERROR   Backend returned 400
[00:41:25.268] Server  ERROR   Backend returned 400
[00:41:25.271] Server  ERROR   Backend returned 400
[00:41:26.167] Browser WARN    Inference API failed, using simulation 400
[00:41:26.167] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.15718020063739835
[00:41:26.167] Browser WARN    Inference API failed, using simulation 400
[00:41:26.167] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7513904243417424
[00:41:26.167] Browser WARN    Inference API failed, using simulation 400
[00:41:26.167] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2346629370028443
[00:41:26.264] Server  ERROR   Backend returned 400
[00:41:26.277] Server  ERROR   Backend returned 400
[00:41:26.287] Server  ERROR   Backend returned 400
[00:41:27.166] Browser WARN    Inference API failed, using simulation 400
[00:41:27.166] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9510027209841986
[00:41:27.166] Browser WARN    Inference API failed, using simulation 400
[00:41:27.166] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2964183397855204
[00:41:27.166] Browser WARN    Inference API failed, using simulation 400
[00:41:27.166] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.060214550289613034
[00:41:33.469] Server  ERROR   Backend returned 400
[00:41:33.490] Server  ERROR   Backend returned 400
[00:41:33.518] Server  ERROR   Backend returned 400
[00:41:33.592] Server  ERROR   Backend returned 400
[00:41:33.609] Server  ERROR   Backend returned 400
[00:41:33.627] Server  ERROR   Backend returned 400
[00:41:33.733] Browser WARN    Inference API failed, using simulation 400
[00:41:33.733] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.24391301655413783
[00:41:33.733] Browser WARN    Inference API failed, using simulation 400
[00:41:33.733] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.25817668413144096
[00:41:33.733] Browser WARN    Inference API failed, using simulation 400
[00:41:33.733] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.20960127394816847
[00:41:33.733] Browser WARN    Inference API failed, using simulation 400
[00:41:33.733] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8579287487637276
[00:41:33.733] Browser WARN    Inference API failed, using simulation 400
[00:41:33.733] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9491424564926865
[00:41:33.733] Browser WARN    Inference API failed, using simulation 400
[00:41:33.733] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.06733830186962719
[00:41:34.089] Server  ERROR   Backend returned 400
[00:41:34.132] Server  ERROR   Backend returned 400
[00:41:34.135] Server  ERROR   Backend returned 400
[00:41:34.248] Browser WARN    Inference API failed, using simulation 400
[00:41:34.248] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8369291750944594
[00:41:34.248] Browser WARN    Inference API failed, using simulation 400
[00:41:34.248] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4716182064108542
[00:41:34.248] Browser WARN    Inference API failed, using simulation 400
[00:41:34.248] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.09059457868626053
[00:41:34.674] Server  ERROR   Backend returned 400
[00:41:34.738] Server  ERROR   Backend returned 400
[00:41:34.746] Server  ERROR   Backend returned 400
[00:41:34.866] Browser WARN    Inference API failed, using simulation 400
[00:41:34.866] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.237335521502196
[00:41:34.866] Browser WARN    Inference API failed, using simulation 400
[00:41:34.866] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.010451316699087498
[00:41:34.866] Browser WARN    Inference API failed, using simulation 400
[00:41:34.866] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.031058650639006424
[00:41:35.087] Server  ERROR   Backend returned 400
[00:41:35.099] Server  ERROR   Backend returned 400
[00:41:35.133] Server  ERROR   Backend returned 400
[00:41:35.238] Browser WARN    Inference API failed, using simulation 400
[00:41:35.238] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3533725007409001
[00:41:35.238] Browser WARN    Inference API failed, using simulation 400
[00:41:35.238] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7552069789013515
[00:41:35.238] Browser WARN    Inference API failed, using simulation 400
[00:41:35.238] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1114049978933036
[00:41:36.214] Server  ERROR   Backend returned 400
[00:41:36.229] Server  ERROR   Backend returned 400
[00:41:36.240] Server  ERROR   Backend returned 400
[00:41:37.161] Browser WARN    Inference API failed, using simulation 400
[00:41:37.161] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.17307393942181215
[00:41:37.161] Browser WARN    Inference API failed, using simulation 400
[00:41:37.161] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3692939986955958
[00:41:37.161] Browser WARN    Inference API failed, using simulation 400
[00:41:37.161] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.45645977817849565
[00:41:37.207] Server  ERROR   Backend returned 400
[00:41:37.223] Server  ERROR   Backend returned 400
[00:41:37.239] Server  ERROR   Backend returned 400
[00:41:38.162] Browser WARN    Inference API failed, using simulation 400
[00:41:38.162] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.37720767727973964
[00:41:38.162] Browser WARN    Inference API failed, using simulation 400
[00:41:38.162] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.11627269919817285
[00:41:38.162] Browser WARN    Inference API failed, using simulation 400
[00:41:38.162] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.17481466538051416
[00:41:38.209] Server  ERROR   Backend returned 400
[00:41:38.236] Server  ERROR   Backend returned 400
[00:41:38.242] Server  ERROR   Backend returned 400
[00:41:39.162] Browser WARN    Inference API failed, using simulation 400
[00:41:39.162] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.21839815149266573
[00:41:39.162] Browser WARN    Inference API failed, using simulation 400
[00:41:39.162] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2217080620439666
[00:41:39.162] Browser WARN    Inference API failed, using simulation 400
[00:41:39.162] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.25055315959775815
[00:41:39.204] Server  ERROR   Backend returned 400
[00:41:39.222] Server  ERROR   Backend returned 400
[00:41:39.234] Server  ERROR   Backend returned 400
[00:41:40.162] Browser WARN    Inference API failed, using simulation 400
[00:41:40.162] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3856599977519616
[00:41:40.162] Browser WARN    Inference API failed, using simulation 400
[00:41:40.162] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7672586037822529
[00:41:40.162] Browser WARN    Inference API failed, using simulation 400
[00:41:40.162] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9366742424850603
[00:41:40.205] Server  ERROR   Backend returned 400
[00:41:40.222] Server  ERROR   Backend returned 400
[00:41:40.238] Server  ERROR   Backend returned 400
[00:41:41.161] Browser WARN    Inference API failed, using simulation 400
[00:41:41.161] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.39941037975798294
[00:41:41.161] Browser WARN    Inference API failed, using simulation 400
[00:41:41.161] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8583532514714006
[00:41:41.161] Browser WARN    Inference API failed, using simulation 400
[00:41:41.161] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.35657218474226887
[00:41:41.202] Server  ERROR   Backend returned 400
[00:41:41.224] Server  ERROR   Backend returned 400
[00:41:41.236] Server  ERROR   Backend returned 400
[00:41:42.164] Browser WARN    Inference API failed, using simulation 400
[00:41:42.164] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4193565343159822
[00:41:42.164] Browser WARN    Inference API failed, using simulation 400
[00:41:42.164] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9247408480171072
[00:41:42.164] Browser WARN    Inference API failed, using simulation 400
[00:41:42.164] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4532214671717959
[00:41:42.204] Server  ERROR   Backend returned 400
[00:41:42.221] Server  ERROR   Backend returned 400
[00:41:42.231] Server  ERROR   Backend returned 400
[00:41:43.162] Browser WARN    Inference API failed, using simulation 400
[00:41:43.162] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.44828344971385264
[00:41:43.162] Browser WARN    Inference API failed, using simulation 400
[00:41:43.162] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7387739735736117
[00:41:43.162] Browser WARN    Inference API failed, using simulation 400
[00:41:43.162] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.813967210070177
[00:41:43.203] Server  ERROR   Backend returned 400
[00:41:43.228] Server  ERROR   Backend returned 400
[00:41:43.234] Server  ERROR   Backend returned 400
[00:41:44.162] Browser WARN    Inference API failed, using simulation 400
[00:41:44.162] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7975475030372984
[00:41:44.162] Browser WARN    Inference API failed, using simulation 400
[00:41:44.162] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.483632635718188
[00:41:44.162] Browser WARN    Inference API failed, using simulation 400
[00:41:44.162] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9540279502251723
[00:41:44.207] Server  ERROR   Backend returned 400
[00:41:44.223] Server  ERROR   Backend returned 400
[00:41:44.240] Server  ERROR   Backend returned 400
[00:41:45.162] Browser WARN    Inference API failed, using simulation 400
[00:41:45.162] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2731208075175921
[00:41:45.162] Browser WARN    Inference API failed, using simulation 400
[00:41:45.162] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3952074960635433
[00:41:45.162] Browser WARN    Inference API failed, using simulation 400
[00:41:45.162] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.802038522936576
[00:41:45.219] Server  ERROR   Backend returned 400
[00:41:45.230] Server  ERROR   Backend returned 400
[00:41:45.244] Server  ERROR   Backend returned 400
[00:41:46.161] Browser WARN    Inference API failed, using simulation 400
[00:41:46.161] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9014937674687393
[00:41:46.161] Browser WARN    Inference API failed, using simulation 400
[00:41:46.161] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9843877748110843
[00:41:46.161] Browser WARN    Inference API failed, using simulation 400
[00:41:46.161] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3614500616619063
[00:41:47.214] Server  ERROR   Backend returned 400
[00:41:47.233] Server  ERROR   Backend returned 400
[00:41:47.236] Server  ERROR   Backend returned 400
[00:41:48.161] Browser WARN    Inference API failed, using simulation 400
[00:41:48.161] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3252760705525722
[00:41:48.161] Browser WARN    Inference API failed, using simulation 400
[00:41:48.161] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.18609882538385475
[00:41:48.161] Browser WARN    Inference API failed, using simulation 400
[00:41:48.161] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9731910349135566
[00:41:48.213] Server  ERROR   Backend returned 400
[00:41:48.215] Server  ERROR   Backend returned 400
[00:41:48.228] Server  ERROR   Backend returned 400
[00:41:49.162] Browser WARN    Inference API failed, using simulation 400
[00:41:49.162] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9660440593034711
[00:41:49.162] Browser WARN    Inference API failed, using simulation 400
[00:41:49.162] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.2294121139591226
[00:41:49.162] Browser WARN    Inference API failed, using simulation 400
[00:41:49.162] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.29234730035444007
[00:41:49.205] Server  ERROR   Backend returned 400
[00:41:49.223] Server  ERROR   Backend returned 400
[00:41:49.233] Server  ERROR   Backend returned 400
[00:41:50.162] Browser WARN    Inference API failed, using simulation 400
[00:41:50.162] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.11808780314340095
[00:41:50.162] Browser WARN    Inference API failed, using simulation 400
[00:41:50.162] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.38526072638324094
[00:41:50.162] Browser WARN    Inference API failed, using simulation 400
[00:41:50.162] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8026966420611077
[00:41:50.257] Server  ERROR   Backend returned 400
[00:41:50.274] Server  ERROR   Backend returned 400
[00:41:50.290] Server  ERROR   Backend returned 400
[00:41:51.162] Browser WARN    Inference API failed, using simulation 400
[00:41:51.162] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.4838788277473478
[00:41:51.162] Browser WARN    Inference API failed, using simulation 400
[00:41:51.162] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.35868313437973864
[00:41:51.162] Browser WARN    Inference API failed, using simulation 400
[00:41:51.162] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4245847957115365
[00:41:51.235] Server  ERROR   Backend returned 400
[00:41:51.239] Server  ERROR   Backend returned 400
[00:41:51.265] Server  ERROR   Backend returned 400
[00:41:52.173] Browser WARN    Inference API failed, using simulation 400
[00:41:52.173] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.07316682643681538
[00:41:52.173] Browser WARN    Inference API failed, using simulation 400
[00:41:52.173] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.046410174300016316
[00:41:52.173] Browser WARN    Inference API failed, using simulation 400
[00:41:52.173] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.40437699029694857
[00:41:52.243] Server  ERROR   Backend returned 400
[00:41:52.246] Server  ERROR   Backend returned 400
[00:41:52.258] Server  ERROR   Backend returned 400
[00:41:53.162] Browser WARN    Inference API failed, using simulation 400
[00:41:53.162] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.21551994211993875
[00:41:53.162] Browser WARN    Inference API failed, using simulation 400
[00:41:53.162] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8435224551522978
[00:41:53.162] Browser WARN    Inference API failed, using simulation 400
[00:41:53.162] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.08853136482519869
[00:41:53.202] Server  ERROR   Backend returned 400
[00:41:53.220] Server  ERROR   Backend returned 400
[00:41:53.235] Server  ERROR   Backend returned 400
[00:41:54.195] Server  ERROR   Backend returned 400
[00:41:54.213] Server  ERROR   Backend returned 400
[00:41:55.196] Server  ERROR   Backend returned 400
[00:41:56.217] Server  ERROR   Backend returned 400
[00:41:56.223] Server  ERROR   Backend returned 400
[00:41:57.208] Server  ERROR   Backend returned 400
[00:41:58.224] Server  ERROR   Backend returned 400
[00:41:58.227] Server  ERROR   Backend returned 400
[00:41:59.207] Server  ERROR   Backend returned 400
[00:42:00.205] Server  ERROR   Backend returned 400
[00:42:01.174] Browser WARN    Inference API failed, using simulation 400
[00:42:01.174] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9459880629175661
[00:42:01.174] Browser WARN    Inference API failed, using simulation 400
[00:42:01.174] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.39611011980668176
[00:42:01.174] Browser WARN    Inference API failed, using simulation 400
[00:42:01.174] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.13779848379986753
[00:42:01.174] Browser WARN    Inference API failed, using simulation 400
[00:42:01.174] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.12974083556334276
[00:42:01.174] Browser WARN    Inference API failed, using simulation 400
[00:42:01.174] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.03062431135144883
[00:42:01.174] Browser WARN    Inference API failed, using simulation 400
[00:42:01.174] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.1014305323218308
[00:42:01.174] Browser WARN    Inference API failed, using simulation 400
[00:42:01.174] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.3093309994889739
[00:42:01.174] Browser WARN    Inference API failed, using simulation 400
[00:42:01.174] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.27726202665155925
[00:42:01.174] Browser WARN    Inference API failed, using simulation 400
[00:42:01.174] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3850781987448129
[00:42:01.174] Browser WARN    Inference API failed, using simulation 400
[00:42:01.174] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7840194350895439
[00:42:01.174] Browser WARN    Inference API failed, using simulation 400
[00:42:01.174] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.37263711525381904
[00:42:01.174] Browser WARN    Inference API failed, using simulation 400
[00:42:01.174] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.34421223931653105
[00:42:01.174] Browser WARN    Inference API failed, using simulation 400
[00:42:01.174] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9249596517364139
[00:42:01.211] Server  ERROR   Backend returned 400
[00:42:01.224] Server  ERROR   Backend returned 400
[00:42:02.197] Server  ERROR   Backend returned 400
[00:42:02.215] Server  ERROR   Backend returned 400
[00:42:03.218] Server  ERROR   Backend returned 400
[00:42:04.212] Server  ERROR   Backend returned 400
[00:42:04.232] Server  ERROR   Backend returned 400
[00:42:05.212] Server  ERROR   Backend returned 400
[00:42:06.210] Server  ERROR   Backend returned 400
[00:42:07.197] Server  ERROR   Backend returned 400
[00:42:08.179] Browser WARN    Inference API failed, using simulation 400
[00:42:08.180] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3441231142017525
[00:42:08.180] Browser WARN    Inference API failed, using simulation 400
[00:42:08.180] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4750344349465806
[00:42:08.180] Browser WARN    Inference API failed, using simulation 400
[00:42:08.180] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.23326554843082115
[00:42:08.180] Browser WARN    Inference API failed, using simulation 400
[00:42:08.180] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.22679416153963655
[00:42:08.180] Browser WARN    Inference API failed, using simulation 400
[00:42:08.180] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3977807615294271
[00:42:08.180] Browser WARN    Inference API failed, using simulation 400
[00:42:08.180] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.855708473705612
[00:42:08.180] Browser WARN    Inference API failed, using simulation 400
[00:42:08.180] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.28205941820738833
[00:42:08.180] Browser WARN    Inference API failed, using simulation 400
[00:42:08.180] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9127285444909127
[00:42:08.180] Browser WARN    Inference API failed, using simulation 400
[00:42:08.180] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7747627655408057
[00:42:08.180] Browser WARN    Inference API failed, using simulation 400
[00:42:08.180] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.7254733386812302
[00:42:08.216] Server  ERROR   Backend returned 400
[00:42:08.222] Server  ERROR   Backend returned 400
[00:42:09.203] Server  ERROR   Backend returned 400
[00:42:09.217] Server  ERROR   Backend returned 400
[00:42:10.194] Server  ERROR   Backend returned 400
[00:42:11.180] Browser WARN    Inference API failed, using simulation 400
[00:42:11.180] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3584482460130826
[00:42:11.180] Browser WARN    Inference API failed, using simulation 400
[00:42:11.180] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.02488257825342416
[00:42:11.180] Browser WARN    Inference API failed, using simulation 400
[00:42:11.180] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.17209642400391656
[00:42:11.180] Browser WARN    Inference API failed, using simulation 400
[00:42:11.180] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1911227103365682
[00:42:11.180] Browser WARN    Inference API failed, using simulation 400
[00:42:11.180] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.4811045034276319
[00:42:11.230] Server  ERROR   Backend returned 400
[00:42:11.232] Server  ERROR   Backend returned 400
[00:42:12.177] Browser WARN    Inference API failed, using simulation 400
[00:42:12.177] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.08747790539476197
[00:42:12.177] Browser WARN    Inference API failed, using simulation 400
[00:42:12.177] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3993289959917211
[00:42:12.201] Server  ERROR   Backend returned 400
[00:42:12.214] Server  ERROR   Backend returned 400
[00:42:13.174] Browser WARN    Inference API failed, using simulation 400
[00:42:13.174] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.06158953449246335
[00:42:13.174] Browser WARN    Inference API failed, using simulation 400
[00:42:13.174] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.8138909073692908
[00:42:13.217] Server  ERROR   Backend returned 400
[00:42:13.228] Server  ERROR   Backend returned 400
[00:42:14.207] Server  ERROR   Backend returned 400
[00:42:14.219] Server  ERROR   Backend returned 400
[00:42:15.182] Browser WARN    Inference API failed, using simulation 400
[00:42:15.182] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.4684097038817267
[00:42:15.182] Browser WARN    Inference API failed, using simulation 400
[00:42:15.182] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.11591746887027088
[00:42:15.182] Browser WARN    Inference API failed, using simulation 400
[00:42:15.182] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.11572826443054923
[00:42:15.182] Browser WARN    Inference API failed, using simulation 400
[00:42:15.182] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.2876192874731622
[00:42:15.200] Server  ERROR   Backend returned 400
[00:42:16.179] Browser WARN    Inference API failed, using simulation 400
[00:42:16.179] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.14725041585816512
[00:42:16.217] Server  ERROR   Backend returned 400
[00:42:16.222] Server  ERROR   Backend returned 400
[00:42:17.218] Server  ERROR   Backend returned 400
[00:42:17.221] Server  ERROR   Backend returned 400
[00:42:18.228] Server  ERROR   Backend returned 400
[00:42:19.176] Browser WARN    Inference API failed, using simulation 400
[00:42:19.176] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9622660427016433
[00:42:19.176] Browser WARN    Inference API failed, using simulation 400
[00:42:19.176] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.49344092849305315
[00:42:19.176] Browser WARN    Inference API failed, using simulation 400
[00:42:19.176] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.9036500203847448
[00:42:19.176] Browser WARN    Inference API failed, using simulation 400
[00:42:19.176] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7206518066669005
[00:42:19.176] Browser WARN    Inference API failed, using simulation 400
[00:42:19.176] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.38874548029400796
[00:42:19.232] Server  ERROR   Backend returned 400
[00:42:20.164] Browser WARN    Inference API failed, using simulation 400
[00:42:20.164] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.859210760309256
[00:42:20.213] Server  ERROR   Backend returned 400
[00:42:20.216] Server  ERROR   Backend returned 400
[00:42:20.233] Server  ERROR   Backend returned 400
[00:42:21.203] Server  ERROR   Backend returned 400
[00:42:22.178] Browser WARN    Inference API failed, using simulation 400
[00:42:22.178] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.25810069083930665
[00:42:22.178] Browser WARN    Inference API failed, using simulation 400
[00:42:22.178] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.2735635915701299
[00:42:22.178] Browser WARN    Inference API failed, using simulation 400
[00:42:22.178] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.07340899161772396
[00:42:22.178] Browser WARN    Inference API failed, using simulation 400
[00:42:22.178] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7264656849089756
[00:42:22.214] Server  ERROR   Backend returned 400
[00:42:22.216] Server  ERROR   Backend returned 400
[00:42:23.200] Server  ERROR   Backend returned 400
[00:42:23.213] Server  ERROR   Backend returned 400
[00:42:24.195] Server  ERROR   Backend returned 400
[00:42:25.169] Browser WARN    Inference API failed, using simulation 400
[00:42:25.169] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.47647190148116303
[00:42:25.169] Browser WARN    Inference API failed, using simulation 400
[00:42:25.169] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9221059923324084
[00:42:25.169] Browser WARN    Inference API failed, using simulation 400
[00:42:25.169] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9886154644187755
[00:42:25.169] Browser WARN    Inference API failed, using simulation 400
[00:42:25.169] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.3724734775998458
[00:42:25.169] Browser WARN    Inference API failed, using simulation 400
[00:42:25.169] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.22007594808665276
[00:42:25.196] Server  ERROR   Backend returned 400
[00:42:25.214] Server  ERROR   Backend returned 400
[00:42:26.204] Server  ERROR   Backend returned 400
[00:42:26.228] Server  ERROR   Backend returned 400
[00:42:27.198] Server  ERROR   Backend returned 400
[00:42:28.198] Server  ERROR   Backend returned 400
[00:42:29.169] Browser WARN    Inference API failed, using simulation 400
[00:42:29.169] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.0646725046420627
[00:42:29.169] Browser WARN    Inference API failed, using simulation 400
[00:42:29.169] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.005416374452067163
[00:42:29.169] Browser WARN    Inference API failed, using simulation 400
[00:42:29.169] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8961374247854488
[00:42:29.169] Browser WARN    Inference API failed, using simulation 400
[00:42:29.169] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.26454543048528983
[00:42:29.169] Browser WARN    Inference API failed, using simulation 400
[00:42:29.169] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1366709348719679
[00:42:29.169] Browser WARN    Inference API failed, using simulation 400
[00:42:29.169] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.7342906153556586
[00:42:29.202] Server  ERROR   Backend returned 400
[00:42:29.218] Server  ERROR   Backend returned 400
[00:42:30.204] Server  ERROR   Backend returned 400
[00:42:30.215] Server  ERROR   Backend returned 400
[00:42:31.195] Server  ERROR   Backend returned 400
[00:42:32.183] Browser WARN    Inference API failed, using simulation 400
[00:42:32.183] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.3365618364344848
[00:42:32.183] Browser WARN    Inference API failed, using simulation 400
[00:42:32.183] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.26401328945830854
[00:42:32.183] Browser WARN    Inference API failed, using simulation 400
[00:42:32.183] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.916970539461621
[00:42:32.183] Browser WARN    Inference API failed, using simulation 400
[00:42:32.183] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.1855403450563477
[00:42:32.183] Browser WARN    Inference API failed, using simulation 400
[00:42:32.183] Browser LOG     Simulating inference for video 3 - Panic: false Confidence: 0.29611914850352555
[00:42:32.234] Server  ERROR   Backend returned 400
[00:42:32.236] Server  ERROR   Backend returned 400
[00:42:33.207] Server  ERROR   Backend returned 400
[00:42:33.219] Server  ERROR   Backend returned 400
[00:42:34.207] Server  ERROR   Backend returned 400
[00:42:35.169] Browser WARN    Inference API failed, using simulation 400
[00:42:35.169] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9627390643103574
[00:42:35.169] Browser WARN    Inference API failed, using simulation 400
[00:42:35.169] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.25145544262379715
[00:42:35.169] Browser WARN    Inference API failed, using simulation 400
[00:42:35.169] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.8477254436344994
[00:42:35.169] Browser WARN    Inference API failed, using simulation 400
[00:42:35.169] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.1695278036170012
[00:42:35.169] Browser WARN    Inference API failed, using simulation 400
[00:42:35.169] Browser LOG     Simulating inference for video 2 - Panic: false Confidence: 0.210762329969714
[00:42:35.197] Server  ERROR   Backend returned 400
[00:42:35.211] Server  ERROR   Backend returned 400
[00:42:36.174] Browser WARN    Inference API failed, using simulation 400
[00:42:36.174] Browser LOG     Simulating inference for video 1 - Panic: false Confidence: 0.16566486889177162
[00:42:36.174] Browser WARN    Inference API failed, using simulation 400
[00:42:36.174] Browser LOG     Simulating inference for video 3 - Panic: true Confidence: 0.9141090684027655
[00:42:36.198] Server  ERROR   Backend returned 400
[00:42:36.211] Server  ERROR   Backend returned 400
[00:42:37.169] Browser WARN    Inference API failed, using simulation 400
[00:42:37.169] Browser LOG     Simulating inference for video 2 - Panic: true Confidence: 0.8210144977061611
[00:42:37.169] Browser WARN    Inference API failed, using simulation 400
[00:42:37.169] Browser LOG     Simulating inference for video 1 - Panic: true Confidence: 0.9368385849582859
